{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----********************-----\n",
    "\n",
    "# Created Time: 2025/07/13\n",
    "\n",
    "# Last updated: 2025/07/21\n",
    "\n",
    "# Author: Yiyi He\n",
    "\n",
    "### Use Case\n",
    "\n",
    "# This notebook explores the application of Granger Causality and autoregressive models\n",
    "# 1.\n",
    "\n",
    "# -----********************-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Stats\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "from statsmodels.tsa.api import ARDL\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "from statsmodels.tsa.ardl import ardl_select_order\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "# Geo\n",
    "from shapely.geometry import Point, Polygon\n",
    "# import geopandas as gpd\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.options.display.max_rows = 1000\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# Processing\n",
    "from tqdm import tqdm\n",
    "import functools as ft\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stationarity Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 519/519 [1:12:38<00:00,  8.40s/it]\n"
     ]
    }
   ],
   "source": [
    "# Hourly\n",
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/' # Office Mac\n",
    "home_dir = '/Users/yiyihe/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology/Research/Energy_resilience/' # Macbook Pro\n",
    "\n",
    "hourly_df = pd.read_csv(home_dir + \"01_data/processed/csv/hourly_519station_3weather.csv\",\n",
    "                        index_col=0)\n",
    "\n",
    "# Generate a list of station ids\n",
    "station_lst = list(set(hourly_df.station_id.unique()))\n",
    "\n",
    "adf_res = {}\n",
    "for s_id in tqdm(station_lst):\n",
    "        # Extracting the dataframe for a/one station\n",
    "        station_df = hourly_df[hourly_df['station_id'] == s_id].sort_values(by='datetime')\n",
    "        # Extact time series\n",
    "        pct_blackout_ts = station_df.pct_blackout.values # Target\n",
    "        t2m_ts = station_df.t2m.values # Predictor\n",
    "        tp_ts = station_df.tp.values # Predictor\n",
    "        wind_speed_ts = station_df.wind_speed.values # Predictor\n",
    "\n",
    "        adf_res[s_id] = [\n",
    "                        adfuller(pct_blackout_ts)[1], # p-value\n",
    "                        adfuller(t2m_ts)[1],          # p-value\n",
    "                        adfuller(tp_ts)[1],           # p-value\n",
    "                        adfuller(wind_speed_ts)[1]    # p-value\n",
    "        ]\n",
    "adf_res_df = pd.DataFrame.from_dict(adf_res, orient='index').reset_index()\n",
    "adf_res_df.rename(columns={\n",
    "    'index': 'station_id',\n",
    "    0: 'pct_blackout_pvalue',\n",
    "    1: 't2m_pvalue',\n",
    "    2: 'tp_pvalue',\n",
    "    3: 'wind_speed_pvalue'\n",
    "    }, inplace=True)\n",
    "adf_res_df.to_csv(home_dir + '01_data/processed/csv/ADF/hourly_ADF_test_pvalues.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pct_blackout_pvalue</th>\n",
       "      <th>t2m_pvalue</th>\n",
       "      <th>tp_pvalue</th>\n",
       "      <th>wind_speed_pvalue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.007</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.140</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     pct_blackout_pvalue t2m_pvalue tp_pvalue wind_speed_pvalue\n",
       "mean               0.001      0.229     0.006             0.007\n",
       "std                0.007      0.263     0.063             0.043\n",
       "min                0.000      0.000     0.000             0.000\n",
       "25%                0.000      0.010     0.000             0.000\n",
       "50%                0.000      0.130     0.000             0.000\n",
       "75%                0.000      0.358     0.000             0.000\n",
       "max                0.140      0.951     0.903             0.571"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary statistics\n",
    "hourly_adf_res = pd.read_csv(home_dir + '01_data/processed/csv/Stationarity_test/hourly_ADF_test_pvalues.csv',\n",
    "                             index_col=0)\n",
    "\n",
    "# summarize number of stationary time series\n",
    "# (hourly_adf_res.iloc[:, 1:]<0.05).sum()\n",
    "# Summary statistics for p values\n",
    "(hourly_adf_res.iloc[:, 1:]).describe().iloc[1:].applymap(lambda x: f\"{x:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 519/519 [05:09<00:00,  1.67it/s]\n"
     ]
    }
   ],
   "source": [
    "# Daily\n",
    "# Set home directory\n",
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/' # Office Mac\n",
    "home_dir = '/Users/yiyihe/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology/Research/Energy_resilience/' # Macbook Pro\n",
    "\n",
    "# Load station daily data\n",
    "daily_df = pd.read_csv(home_dir + \"01_data/processed/csv/daily_519station_13weather.csv\", index_col=0)\n",
    "\n",
    "# Generate a list of station ids\n",
    "station_lst = list(set(daily_df.station_id.unique()))\n",
    "\n",
    "# Predictors\n",
    "weather_variables = ['t2m', 'wind_speed', 'tp']\n",
    "aggregations = ['median', 'mean', 'max']\n",
    "predictor_lst = ['daily_blackout_minutes'] + [w + \"_\" + a for w in weather_variables for a in aggregations]\n",
    "\n",
    "adf_res = {}\n",
    "for s_id in tqdm(station_lst):\n",
    "        # Extracting the dataframe for a/one station\n",
    "        station_df = daily_df[daily_df['station_id'] == s_id].sort_values(by='date')\n",
    "        adf_res_lst = []\n",
    "        for var in predictor_lst:\n",
    "                # Extact time series\n",
    "                ts = station_df[var].values\n",
    "                adf_res_lst.append(adfuller(ts)[1]) # append pvalue\n",
    "\n",
    "        adf_res[s_id] = adf_res_lst\n",
    "\n",
    "adf_res_df = pd.DataFrame.from_dict(adf_res, orient='index').reset_index()\n",
    "# Dictionary to update column names\n",
    "rename_dict = {'index': 'station_id'}\n",
    "rename_dict.update({i: var_name + '_pvalue' for i, var_name in enumerate(predictor_lst)})\n",
    "# Rename columns\n",
    "adf_res_df.rename(columns=rename_dict, inplace=True)\n",
    "adf_res_df.to_csv(home_dir + '01_data/processed/csv/ADF/daily_ADF_test_pvalues.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>daily_blackout_minutes_pvalue</th>\n",
       "      <th>t2m_median_pvalue</th>\n",
       "      <th>t2m_mean_pvalue</th>\n",
       "      <th>t2m_max_pvalue</th>\n",
       "      <th>wind_speed_median_pvalue</th>\n",
       "      <th>wind_speed_mean_pvalue</th>\n",
       "      <th>wind_speed_max_pvalue</th>\n",
       "      <th>tp_median_pvalue</th>\n",
       "      <th>tp_mean_pvalue</th>\n",
       "      <th>tp_max_pvalue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.030</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.121</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.682</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.948</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     daily_blackout_minutes_pvalue t2m_median_pvalue t2m_mean_pvalue  \\\n",
       "mean                         0.030             0.432           0.440   \n",
       "std                          0.121             0.317           0.316   \n",
       "min                          0.000             0.000           0.000   \n",
       "25%                          0.000             0.134           0.154   \n",
       "50%                          0.000             0.410           0.424   \n",
       "75%                          0.000             0.680           0.682   \n",
       "max                          0.948             1.000           1.000   \n",
       "\n",
       "     t2m_max_pvalue wind_speed_median_pvalue wind_speed_mean_pvalue  \\\n",
       "mean          0.329                    0.044                  0.049   \n",
       "std           0.281                    0.133                  0.143   \n",
       "min           0.000                    0.000                  0.000   \n",
       "25%           0.078                    0.000                  0.000   \n",
       "50%           0.261                    0.000                  0.000   \n",
       "75%           0.526                    0.017                  0.025   \n",
       "max           0.999                    0.979                  0.997   \n",
       "\n",
       "     wind_speed_max_pvalue tp_median_pvalue tp_mean_pvalue tp_max_pvalue  \n",
       "mean                 0.093            0.077          0.072         0.083  \n",
       "std                  0.165            0.171          0.163         0.184  \n",
       "min                  0.000            0.000          0.000         0.000  \n",
       "25%                  0.000            0.000          0.000         0.000  \n",
       "50%                  0.006            0.002          0.001         0.003  \n",
       "75%                  0.116            0.064          0.056         0.057  \n",
       "max                  0.999            1.000          0.999         1.000  "
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary statistics\n",
    "daily_adf_res = pd.read_csv(home_dir + '01_data/processed/csv/Stationarity_test/daily_ADF_test_pvalues.csv',\n",
    "                             index_col=0)\n",
    "\n",
    "# summarize number of stationary time series\n",
    "(daily_adf_res.iloc[:, 1:]<0.05).sum()\n",
    "\n",
    "# Summary statistics for p values\n",
    "(daily_adf_res.iloc[:, 1:]).describe().iloc[1:].applymap(lambda x: f\"{x:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KPSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 519/519 [00:30<00:00, 17.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# Hourly\n",
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/' # Office Mac\n",
    "home_dir = '/Users/yiyihe/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology/Research/Energy_resilience/' # Macbook Pro\n",
    "\n",
    "hourly_df = pd.read_csv(home_dir + \"01_data/processed/csv/hourly_519station_3weather.csv\",\n",
    "                        index_col=0)\n",
    "\n",
    "# Generate a list of station ids\n",
    "station_lst = list(set(hourly_df.station_id.unique()))\n",
    "\n",
    "kpss_res = {}\n",
    "for s_id in tqdm(station_lst):\n",
    "        # Extracting the dataframe for a/one station\n",
    "        station_df = hourly_df[hourly_df['station_id'] == s_id].sort_values(by='datetime')\n",
    "        # Extact time series\n",
    "        pct_blackout_ts = station_df.pct_blackout.values # Target\n",
    "        t2m_ts = station_df.t2m.values # Predictor\n",
    "        tp_ts = station_df.tp.values # Predictor\n",
    "        wind_speed_ts = station_df.wind_speed.values # Predictor\n",
    "\n",
    "        kpss_res[s_id] = [\n",
    "                        kpss(pct_blackout_ts, regression=\"c\", nlags=\"auto\")[1], # p-value\n",
    "                        kpss(t2m_ts, regression=\"c\", nlags=\"auto\")[1],          # p-value\n",
    "                        kpss(tp_ts, regression=\"c\", nlags=\"auto\")[1],           # p-value\n",
    "                        kpss(wind_speed_ts, regression=\"c\", nlags=\"auto\")[1]    # p-value\n",
    "        ]\n",
    "kpss_res_df = pd.DataFrame.from_dict(kpss_res, orient='index').reset_index()\n",
    "kpss_res_df.rename(columns={\n",
    "    'index': 'station_id',\n",
    "    0: 'pct_blackout_pvalue',\n",
    "    1: 't2m_pvalue',\n",
    "    2: 'tp_pvalue',\n",
    "    3: 'wind_speed_pvalue'\n",
    "    }, inplace=True)\n",
    "kpss_res_df.to_csv(home_dir + '01_data/processed/csv/Stationarity_test/hourly_KPSS_test_pvalues.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 519/519 [00:01<00:00, 306.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# Daily\n",
    "# Set home directory\n",
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/' # Office Mac\n",
    "home_dir = '/Users/yiyihe/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology/Research/Energy_resilience/' # Macbook Pro\n",
    "\n",
    "# Load station daily data\n",
    "daily_df = pd.read_csv(home_dir + \"01_data/processed/csv/daily_519station_13weather.csv\", index_col=0)\n",
    "\n",
    "# Generate a list of station ids\n",
    "station_lst = list(set(daily_df.station_id.unique()))\n",
    "\n",
    "# Predictors\n",
    "weather_variables = ['t2m', 'wind_speed', 'tp']\n",
    "aggregations = ['median', 'mean', 'max']\n",
    "predictor_lst = ['daily_blackout_minutes'] + [w + \"_\" + a for w in weather_variables for a in aggregations]\n",
    "\n",
    "kpss_res = {}\n",
    "for s_id in tqdm(station_lst):\n",
    "        # Extracting the dataframe for a/one station\n",
    "        station_df = daily_df[daily_df['station_id'] == s_id].sort_values(by='date')\n",
    "        kpss_res_lst = []\n",
    "        for var in predictor_lst:\n",
    "                # Extact time series\n",
    "                ts = station_df[var].values\n",
    "                kpss_res_lst.append(kpss(ts, regression=\"c\", nlags=\"auto\")[1]) # append pvalue\n",
    "\n",
    "        kpss_res[s_id] = kpss_res_lst\n",
    "        # break\n",
    "kpss_res_df = pd.DataFrame.from_dict(kpss_res, orient='index').reset_index()\n",
    "# Dictionary to update column names\n",
    "rename_dict = {'index': 'station_id'}\n",
    "rename_dict.update({i: var_name + '_pvalue' for i, var_name in enumerate(predictor_lst)})\n",
    "# Rename columns\n",
    "kpss_res_df.rename(columns=rename_dict, inplace=True)\n",
    "kpss_res_df.to_csv(home_dir + '01_data/processed/csv/Stationarity_test/daily_KPSS_test_pvalues.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differencing Timeseries (based on ADF test results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference_series(series, order=1):\n",
    "    \"\"\"\n",
    "    Perform differencing on a pandas Series.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    series : pd.Series\n",
    "        The time series to difference\n",
    "    order : int\n",
    "        Order of differencing (1 for first difference, 2 for second, etc.)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series\n",
    "        Differenced series\n",
    "    \"\"\"\n",
    "    differenced = series.copy()\n",
    "    \n",
    "    for i in range(order):\n",
    "        differenced = differenced.diff().dropna()\n",
    "    \n",
    "    return differenced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process differecing order=1 for hourly data (that is not stationary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hourly\n",
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/' # Office Mac\n",
    "home_dir = '/Users/yiyihe/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology/Research/Energy_resilience/' # Macbook Pro\n",
    "\n",
    "hourly_df = pd.read_csv(home_dir + \"01_data/processed/csv/hourly_519station_3weather.csv\",\n",
    "                        index_col=0)\n",
    "\n",
    "# Generate a list of station ids\n",
    "station_lst = list(set(hourly_df.station_id.unique()))\n",
    "\n",
    "adf_res_df = pd.read_csv(home_dir + '01_data/processed/csv/ADF/hourly_ADF_test_pvalues.csv', index_col=0)\n",
    "\n",
    "var_names = ['pct_blackout', 't2m', 'tp', 'wind_speed']\n",
    "\n",
    "for s_id in station_lst:\n",
    "    # check ADF test result for station\n",
    "    ADF_result = adf_res_df[adf_res_df.station_id == s_id].values[0][1:]<0.05 # array of four boolean values\n",
    "    if ADF_result.sum() == 4: # all variables are stationary, no action needed\n",
    "        continue\n",
    "    else:\n",
    "        print(f'I am working on station id = {s_id}')\n",
    "        if ADF_result[0] == True: # The target timeseries is stationary, therefore we only need to drop the first observation\n",
    "            var_to_process = [var for var, keep in zip(var_names, ADF_result<0.05) if keep]\n",
    "            # original station df\n",
    "            station_df = hourly_df[hourly_df['station_id'] == s_id].sort_values(by='datetime')\n",
    "\n",
    "            station_pct_blackout_new = station_df.pct_blackout.values[1:] # new array of pct blackout values (eliminated the first entry)\n",
    "            station_new = pd.DataFrame(station_pct_blackout_new, columns=['pct_blackout_shift1'])\n",
    "            for var in var_to_process:\n",
    "                var_diff = difference_series(station_df[var]) # differencing order=1\n",
    "                if adfuller(var_diff)[1]>0.05: # is order 1 differencing does not work\n",
    "                    print(s_id, var)\n",
    "                else:\n",
    "                    # add var_diff as another column with correct name\n",
    "                    station_new[f'{var}_diff1'] = var_diff.values\n",
    "        else: # the target timeseries is not stationary, we need to difference it first\n",
    "            station_pct_blackout_diff = difference_series(station_df['pct_blackout']) # difference the pct blackout\n",
    "            station_new = pd.DataFrame(station_pct_blackout_diff.values, columns=['pct_blackout_diff1'])\n",
    "            if adfuller(station_pct_blackout_diff)[1]>0.05: # is order 1 differencing does not work\n",
    "                print(s_id, 'diff1 does not work for this target variable')\n",
    "            else:\n",
    "                # process predictor variables\n",
    "                var_to_process = [var for var, keep in zip(var_names, ADF_result<0.05) if keep][1:] # take away pct blackout\n",
    "                for var in var_to_process:\n",
    "                    var_diff = difference_series(station_df[var]) # differencing order=1\n",
    "                    if adfuller(var_diff)[1]>0.05: # is order 1 differencing does not work\n",
    "                        print(s_id, var)\n",
    "                    else:\n",
    "                    # add var_diff as another column with correct name\n",
    "                        station_new[f'{var}_diff1'] = var_diff.values\n",
    "\n",
    "            \n",
    "    station_new.to_csv(home_dir + f\"01_data/processed/csv/hourly_diff1/station_{s_id}_hourly_diff1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the results of differencing works or not, in other words, if the time series are indeed stationary now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/' # Office Mac\n",
    "home_dir = '/Users/yiyihe/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology/Research/Energy_resilience/' # Macbook Pro\n",
    "\n",
    "# Check if all the time series in the differenced dataset are stationary\n",
    "diff_dir = home_dir + \"01_data/processed/csv/hourly_diff1/\"\n",
    "for item in os.listdir(diff_dir):\n",
    "    if item[-3:] == 'csv':\n",
    "        df = pd.read_csv(os.path.join(diff_dir, item), index_col=0)\n",
    "        # Perform ADF test on all columns\n",
    "        for col in df.columns:\n",
    "            col_ts = df[col].values\n",
    "            adf_res = adfuller(col_ts)[1]\n",
    "            if adf_res >= 0.05:\n",
    "                # Print filename and column name if time series is not stationary\n",
    "                print(item, col) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After differencing (order=1), all the time series are now stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process differecing order=1 for daily data (that is not stationary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 519/519 [01:09<00:00,  7.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# Hourly\n",
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/' # Office Mac\n",
    "home_dir = '/Users/yiyihe/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology/Research/Energy_resilience/' # Macbook Pro\n",
    "\n",
    "daily_df = pd.read_csv(home_dir + \"01_data/processed/csv/daily_519station_13weather.csv\",\n",
    "                        index_col=0)\n",
    "\n",
    "# Generate a list of station ids\n",
    "station_lst = list(set(daily_df.station_id.unique()))\n",
    "\n",
    "adf_res_df = pd.read_csv(home_dir + '01_data/processed/csv/Stationarity_test/daily_ADF_test_pvalues.csv', index_col=0)\n",
    "\n",
    "# Variable names\n",
    "weather_variables = ['t2m', 'wind_speed', 'tp']\n",
    "aggregations = ['median', 'mean', 'max']\n",
    "var_names = ['daily_blackout_minutes'] + [w + \"_\" + a for w in weather_variables for a in aggregations]\n",
    "\n",
    "for s_id in tqdm(station_lst):\n",
    "    # check ADF test result for station\n",
    "    ADF_result = adf_res_df[adf_res_df.station_id == s_id].values[0][1:]<0.05 # array of four boolean values indicating stationarity for variables\n",
    "\n",
    "    if ADF_result.sum() == len(var_names): # all variables are stationary, no action needed\n",
    "        continue\n",
    "    else:\n",
    "        # print(f'I am working on station id = {s_id}')\n",
    "        if ADF_result[0] == True: # The target timeseries is stationary, therefore we only need to drop the first observation\n",
    "            var_to_process = [var for var, keep in zip(var_names, ADF_result<0.05) if keep] # list of variables that are not stationary\n",
    "            # original station df\n",
    "            station_df = daily_df[daily_df['station_id'] == s_id].sort_values(by='date')\n",
    "            \n",
    "            station_blackout_minutes_new = station_df.daily_blackout_minutes.values[1:] # new array of pct blackout values (eliminated the first entry)\n",
    "            station_new = pd.DataFrame(station_blackout_minutes_new, columns=['blackout_minutes_shift1'])\n",
    "\n",
    "            flag = 1\n",
    "            # First check if any var needs second order differencing\n",
    "            for var in var_to_process:\n",
    "                var_diff = difference_series(station_df[var]) # differencing order=1\n",
    "                if adfuller(var_diff)[1]>=0.05:\n",
    "                    flag = 2\n",
    "\n",
    "            if flag == 1:\n",
    "            # order 1 differencing is enough for this station\n",
    "                for var in var_to_process:\n",
    "                    var_diff = difference_series(station_df[var]) # differencing order=1\n",
    "                    # add var_diff as another column with correct name\n",
    "                    station_new[f'{var}_diff1'] = var_diff.values\n",
    "            elif flag == 2:\n",
    "                # need to shift blackout minutes by 2\n",
    "                station_blackout_minutes_new = station_df.daily_blackout_minutes.values[2:] # new array of pct blackout values (eliminated the first 2 entries)\n",
    "                station_new = pd.DataFrame(station_blackout_minutes_new, columns=['blackout_minutes_shift2']) # redefine station_new dataframe\n",
    "                for var in var_to_process:\n",
    "                    var_diff = difference_series(station_df[var]) # differencing order=1\n",
    "                    # check if this variable needs second differencing\n",
    "                    if adfuller(var_diff)[1]>=0.05:\n",
    "                        var_diff2 = difference_series(var_diff) # second diff\n",
    "                        station_new[f'{var}_diff2'] = var_diff2.values\n",
    "                    else: # this variable does not need second differencing\n",
    "                        var_diff = var_diff.values[1:]\n",
    "                        station_new[f'{var}_diff1'] = var_diff\n",
    "\n",
    "        else: # the target timeseries is not stationary, we need to difference it first\n",
    "            station_blackout_minutes_diff = difference_series(station_df['daily_blackout_minutes']) # difference the daily blackout minutes\n",
    "            station_new = pd.DataFrame(station_blackout_minutes_diff.values, columns=['daily_blackout_minutes_diff1'])\n",
    "            if adfuller(station_blackout_minutes_diff)[1]>=0.05: # is order 1 differencing does not work for blackout minutes\n",
    "                print(s_id, 'diff1 does not work for this target variable')\n",
    "            else:\n",
    "                # process predictor variables\n",
    "                var_to_process = [var for var, keep in zip(var_names, ADF_result<0.05) if keep][1:] # take away blackout minutes\n",
    "\n",
    "                flag = 1\n",
    "                # First check if any var needs second order differencing\n",
    "                for var in var_to_process:\n",
    "                    var_diff = difference_series(station_df[var]) # differencing order=1\n",
    "                    if adfuller(var_diff)[1]>=0.05:\n",
    "                        flag = 2\n",
    "\n",
    "                if flag == 1:\n",
    "                # order 1 differencing is enough for this station\n",
    "                    for var in var_to_process:\n",
    "                        var_diff = difference_series(station_df[var]) # differencing order=1\n",
    "                        # add var_diff as another column with correct name\n",
    "                        station_new[f'{var}_diff1'] = var_diff.values\n",
    "\n",
    "                elif flag == 2:\n",
    "                    # need to shift blackout minutes by 2\n",
    "                    station_blackout_minutes_new = station_df.daily_blackout_minutes.values[2:] # new array of pct blackout values (eliminated the first 2 entries)\n",
    "                    station_new = pd.DataFrame(station_blackout_minutes_new, columns=['blackout_minutes_shift2']) # redefine station_new dataframe\n",
    "                    for var in var_to_process:\n",
    "                        var_diff = difference_series(station_df[var]) # differencing order=1\n",
    "                        # check if this variable needs second differencing\n",
    "                        if adfuller(var_diff)[1]>=0.05:\n",
    "                            var_diff2 = difference_series(var_diff) # second diff\n",
    "                            station_new[f'{var}_diff2'] = var_diff2.values\n",
    "                        else: # this variable does not need second differencing\n",
    "                            var_diff = var_diff.values[1:]\n",
    "                            station_new[f'{var}_diff1'] = var_diff\n",
    "\n",
    "    station_new.to_csv(home_dir + f\"01_data/processed/csv/daily_diff1/station_{s_id}_daily_diff{flag}.csv\")\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the differencing works for all stations and all variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/' # Office Mac\n",
    "home_dir = '/Users/yiyihe/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology/Research/Energy_resilience/' # Macbook Pro\n",
    "\n",
    "# Check if all the time series in the differenced dataset are stationary\n",
    "diff_dir = home_dir + \"01_data/processed/csv/daily_diff1/\"\n",
    "for item in os.listdir(diff_dir):\n",
    "    if item[-3:] == 'csv':\n",
    "        df = pd.read_csv(os.path.join(diff_dir, item), index_col=0)\n",
    "        # Perform ADF test on all columns\n",
    "        for col in df.columns:\n",
    "            col_ts = df[col].values\n",
    "            adf_res = adfuller(col_ts)[1]\n",
    "            if adf_res >= 0.05:\n",
    "                # Print filename and column name if time series is not stationary\n",
    "                print(item, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After stationarity check, the following six stations will be removed from the daily analysis:\n",
    "- station id: 183, 423, 559, 315, 103, 62"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Granger Causality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GC with hourly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/'\n",
    "\n",
    "hourly_df = pd.read_csv(home_dir + \"01_data/processed/csv/hourly_519station_3weather.csv\",\n",
    "                        index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function that runs the Granger Causality test on hourly station data. \\\n",
    "For each station, run the Granger Causality test with the following set up:\\\n",
    " \\\n",
    "**Target variable**: \\\n",
    "'pct_blackout' \\\n",
    "**Predictor variables**: \n",
    "1. 't2m' (temperature) \n",
    "2. 'tp' (precipitation) \n",
    "3. 'wind_speed' (wind speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gc_hourly(maxlag, target, predictor, station_lst, hourly_df):\n",
    "    # Initiate an empty dictionary for storing test results\n",
    "    gc_dic = {}\n",
    "\n",
    "    # Iterate through all stations\n",
    "    for s_id in tqdm(station_lst):\n",
    "        # Extracting the dataframe for a/one station\n",
    "        station_df = hourly_df[hourly_df['station_id'] == s_id].sort_values(by='datetime')\n",
    "        try:\n",
    "            # Check if the number of observations available at the station is sufficient for the gc model\n",
    "            if len(station_df) <= maxlag + 2 + 1: # maxlag + num_variables (target, predictor) + 1\n",
    "                print(f\"Skipping Station {s_id} due to insufficient observations for maxlag={maxlag}\")\n",
    "                continue\n",
    "            else:\n",
    "                # Run granger causality test on station hourly data\n",
    "                test_result = grangercausalitytests(\n",
    "                    station_df[[target, predictor]], maxlag=maxlag, addconst=True, verbose=False)\n",
    "                # Save test values\n",
    "                F_test_p_values = [round(test_result[i+1][0]['ssr_ftest'][1],4) for i in range(maxlag)]\n",
    "                Chi_squared_p_values = [round(test_result[i+1][0]['ssr_chi2test'][1],4) for i in range(maxlag)]\n",
    "                p_values_min = np.min(F_test_p_values+Chi_squared_p_values)\n",
    "                # Key: station id, Value: list of 1. minimum F/Chi p values 2. F-test p values for all lags 3. Chi-square test p-values for all lags\n",
    "                gc_dic[s_id] = [p_values_min, F_test_p_values, Chi_squared_p_values]\n",
    "        except ValueError:\n",
    "            print(f\"Skipping Station {s_id} due to insufficient observations for maxlag={maxlag}\")\n",
    "    # Convert to dataframe        \n",
    "    gc_df = pd.DataFrame.from_dict(gc_dic, orient='index').reset_index()\n",
    "    gc_df.rename(columns={\n",
    "    'index':'station_id',\n",
    "    0:f'{predictor}_p-value_min',\n",
    "    1:f'{predictor}_f_p-value',\n",
    "    2:f'{predictor}_Chi_p-value'\n",
    "    }, inplace=True)\n",
    "    return gc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 519/519 [00:22<00:00, 22.99it/s]\n",
      "100%|██████████| 519/519 [00:22<00:00, 22.99it/s]\n",
      "100%|██████████| 519/519 [00:23<00:00, 22.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# Set home directory\n",
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/'\n",
    "# Read hourly data\n",
    "hourly_df = pd.read_csv(home_dir + \"01_data/processed/csv/hourly_519station_3weather.csv\",\n",
    "                        index_col=0)\n",
    "# Set target variable\n",
    "target = 'pct_blackout'\n",
    "# Set max lag for all variables\n",
    "maxlag = 3\n",
    "# Generate a list of station ids\n",
    "station_lst = list(set(hourly_df.station_id.unique()))\n",
    "# List of predictor variables\n",
    "predictor_lst = ['t2m', 'tp', 'wind_speed']\n",
    "\n",
    "# Initiate empty dictionary to store test output dataframes\n",
    "res_dic = {}\n",
    "# Loop through the list of predictors\n",
    "for predictor in predictor_lst:\n",
    "    # Run gc test\n",
    "    df_res = run_gc_hourly(maxlag, target, predictor, station_lst, hourly_df)\n",
    "    # Save output dataframe in dictionary key: predictor, value: dataframe with test values\n",
    "    res_dic[predictor] = df_res\n",
    "\n",
    "# Join resulting dataframes for all predictors together\n",
    "dfs = [res_dic[p] for p in predictor_lst]\n",
    "df_joined= ft.reduce(lambda left, right: pd.merge(left, right, on='station_id'), dfs)\n",
    "# Save output as new csv file\n",
    "df_joined.to_csv(home_dir + f\"01_data/processed/csv/GC/granger_hourly_max{maxlag}_pvalue.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GC on hourly data after first-order differencing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 329/329 [12:59<00:00,  2.37s/it]\n"
     ]
    }
   ],
   "source": [
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/' # Office Mac\n",
    "# home_dir = '/Users/yiyihe/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology/Research/Energy_resilience/' # Macbook Pro\n",
    "# Folder that contains the differenced data\n",
    "diff_dir = \"01_data/processed/csv/hourly_diff1\"\n",
    "# Set max lag\n",
    "maxlag = 3\n",
    "# Iterate through station files (first order differencing applied and all time series are stationary)\n",
    "for file in tqdm(os.listdir(home_dir + diff_dir)):\n",
    "    if file[-3:] == 'csv':\n",
    "        # Extract station id\n",
    "        station_id = int(file.split('_')[1])\n",
    "        df = pd.read_csv(os.path.join(home_dir + diff_dir, file), index_col=0)\n",
    "        # Initiate empty dictionary to store results\n",
    "        gc_res = {}\n",
    "        # Target time series: percent blackout\n",
    "        target_ts = df.iloc[:, 0].values\n",
    "        # Go though rest of the columns and perform GC test. Note, stations have different column numbers, depending on how many of the original are not stationary\n",
    "        try:\n",
    "            # Check if the number of observations available at the station is sufficient for the gc model\n",
    "            if len(df) <= maxlag + 2 + 1: # maxlag + num_variables (target, predictor) + 1\n",
    "                print(f\"Skipping Station {station_id} due to insufficient observations for maxlag={maxlag}\")\n",
    "                continue\n",
    "            else:\n",
    "                for col in list(df.columns)[1:]:\n",
    "                    col_name = col.split('_')[0]\n",
    "                    test_result = grangercausalitytests(\n",
    "                                                        df[[list(df.columns)[0], col]],\n",
    "                                                        maxlag=maxlag,\n",
    "                                                        addconst=True,\n",
    "                                                        verbose=False)\n",
    "                    F_test_p_values = [round(test_result[i+1][0]['ssr_ftest'][1],4) for i in range(maxlag)]\n",
    "                    Chi_squared_p_values = [round(test_result[i+1][0]['ssr_chi2test'][1],4) for i in range(maxlag)]\n",
    "                    p_values_min = np.min(F_test_p_values+Chi_squared_p_values)\n",
    "                    gc_res[col + '_p-value_min'] = p_values_min\n",
    "                    gc_res[col + '_f_p-value'] = F_test_p_values\n",
    "                    gc_res[col + '_Chi_p-value'] = Chi_squared_p_values\n",
    "                # Store results in a dataframe\n",
    "                gc_res_df = pd.DataFrame.from_dict(gc_res)\n",
    "                # Save station results to new csv\n",
    "                gc_res_df.to_csv(home_dir + f\"01_data/processed/csv/GC/hourly_station_diff1/hourly_station_maxlag_{maxlag}/station_{station_id}.csv\")\n",
    "        except ValueError:\n",
    "            print(f\"Skipping Station {station_id} due to insufficient observations for maxlag={maxlag}\")\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update original GC hourly results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/' # Office Mac\n",
    "# home_dir = '/Users/yiyihe/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology/Research/Energy_resilience/' # Macbook Pro\n",
    "\n",
    "maxlag = 3\n",
    "diff_res_dir = f\"01_data/processed/csv/GC/hourly_station_diff1/hourly_station_maxlag_{maxlag}\"\n",
    "\n",
    "# Load original gc result dataframe\n",
    "original_gc_hourly = pd.read_csv(home_dir+f'01_data/processed/csv/GC/hourly/granger_hourly_max{maxlag}_pvalue.csv',\n",
    "                                 index_col=0)\n",
    "\n",
    "original_gc_hourly_stations = original_gc_hourly.station_id.values\n",
    "\n",
    "for file in os.listdir(os.path.join(home_dir, diff_res_dir)):\n",
    "    # Extract station id\n",
    "    station_id = int(file.split('_')[1][:-4])\n",
    "    if station_id in original_gc_hourly_stations:\n",
    "        if file[-3:] == 'csv':\n",
    "            # Read dataframe\n",
    "            station_diff_gc = pd.read_csv(os.path.join(home_dir, diff_res_dir, file), index_col=0)\n",
    "            # Now we are ready to extract the outputs from this dataframe\n",
    "            var_list = list(set([i.split(\"_diff1\")[0] for i in station_diff_gc.columns.values]))\n",
    "            for var in var_list:\n",
    "                var_pvalue_min = station_diff_gc[f'{var}_diff1_p-value_min'].values[0]\n",
    "                var_f_pvalues = list(station_diff_gc[f'{var}_diff1_f_p-value'].values)\n",
    "                var_chi_pvalues = list(station_diff_gc[f'{var}_diff1_Chi_p-value'].values)\n",
    "                # Update original dataframe            \n",
    "                row_idx  = original_gc_hourly[original_gc_hourly['station_id'] == station_id].index\n",
    "                original_gc_hourly.at[row_idx.item(), f'{var}_p-value_min'] = var_pvalue_min.item()\n",
    "                original_gc_hourly.at[row_idx.item(), f'{var}_f_p-value'] = [float(x) for x in var_f_pvalues]\n",
    "                original_gc_hourly.at[row_idx.item(), f'{var}_Chi_p-value'] = [float(x) for x in var_chi_pvalues]\n",
    "    else:\n",
    "        continue\n",
    "original_gc_hourly.to_csv(home_dir + f'01_data/processed/csv/GC/hourly/granger_hourly_max{maxlag}_pvalue_diff_update.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>t2m_p-value_min</th>\n",
       "      <th>t2m_f_p-value</th>\n",
       "      <th>t2m_Chi_p-value</th>\n",
       "      <th>tp_p-value_min</th>\n",
       "      <th>tp_f_p-value</th>\n",
       "      <th>tp_Chi_p-value</th>\n",
       "      <th>wind_speed_p-value_min</th>\n",
       "      <th>wind_speed_f_p-value</th>\n",
       "      <th>wind_speed_Chi_p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0271</td>\n",
       "      <td>[0.0272, 0.0746, 0.1287, 0.1963, 0.3036, 0.380...</td>\n",
       "      <td>[0.0271, 0.0742, 0.1279, 0.195, 0.3017, 0.378,...</td>\n",
       "      <td>0.0985</td>\n",
       "      <td>[0.786, 0.1352, 0.0992, 0.1542, 0.2243, 0.2233...</td>\n",
       "      <td>[0.7859, 0.1347, 0.0985, 0.153, 0.2224, 0.221,...</td>\n",
       "      <td>0.2261</td>\n",
       "      <td>[0.3995, 0.3685, 0.5548, 0.5856, 0.5533, 0.294...</td>\n",
       "      <td>[0.3993, 0.3679, 0.5539, 0.5844, 0.5515, 0.292...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.0453</td>\n",
       "      <td>[0.9078, 0.4611, 0.6229, 0.4407, 0.162, 0.0653...</td>\n",
       "      <td>[0.9078, 0.461, 0.6227, 0.4404, 0.1616, 0.065,...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[0.0004, 0.0005, 0.0003, 0.0007, 0.0014, 0.000...</td>\n",
       "      <td>[0.0004, 0.0004, 0.0003, 0.0007, 0.0014, 0.000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[0.0737, 0.4865, 0.2713, 0.0608, 0.0, 0.0, 0.0...</td>\n",
       "      <td>[0.0736, 0.4862, 0.2707, 0.0604, 0.0, 0.0, 0.0...</td>\n",
       "      <td>0.3917</td>\n",
       "      <td>[0.8354, 0.9068, 0.644, 0.527, 0.55, 0.6509, 0...</td>\n",
       "      <td>[0.8353, 0.9067, 0.6436, 0.5262, 0.5489, 0.649...</td>\n",
       "      <td>0.0236</td>\n",
       "      <td>[0.0628, 0.2473, 0.4359, 0.0495, 0.065, 0.1052...</td>\n",
       "      <td>[0.0627, 0.2469, 0.4353, 0.0491, 0.0644, 0.104...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   station_id  t2m_p-value_min  \\\n",
       "0           1           0.0271   \n",
       "1           2           0.0000   \n",
       "2           3           0.0000   \n",
       "\n",
       "                                       t2m_f_p-value  \\\n",
       "0  [0.0272, 0.0746, 0.1287, 0.1963, 0.3036, 0.380...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  [0.0737, 0.4865, 0.2713, 0.0608, 0.0, 0.0, 0.0...   \n",
       "\n",
       "                                     t2m_Chi_p-value  tp_p-value_min  \\\n",
       "0  [0.0271, 0.0742, 0.1279, 0.195, 0.3017, 0.378,...          0.0985   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...          0.0453   \n",
       "2  [0.0736, 0.4862, 0.2707, 0.0604, 0.0, 0.0, 0.0...          0.3917   \n",
       "\n",
       "                                        tp_f_p-value  \\\n",
       "0  [0.786, 0.1352, 0.0992, 0.1542, 0.2243, 0.2233...   \n",
       "1  [0.9078, 0.4611, 0.6229, 0.4407, 0.162, 0.0653...   \n",
       "2  [0.8354, 0.9068, 0.644, 0.527, 0.55, 0.6509, 0...   \n",
       "\n",
       "                                      tp_Chi_p-value  wind_speed_p-value_min  \\\n",
       "0  [0.7859, 0.1347, 0.0985, 0.153, 0.2224, 0.221,...                  0.2261   \n",
       "1  [0.9078, 0.461, 0.6227, 0.4404, 0.1616, 0.065,...                  0.0000   \n",
       "2  [0.8353, 0.9067, 0.6436, 0.5262, 0.5489, 0.649...                  0.0236   \n",
       "\n",
       "                                wind_speed_f_p-value  \\\n",
       "0  [0.3995, 0.3685, 0.5548, 0.5856, 0.5533, 0.294...   \n",
       "1  [0.0004, 0.0005, 0.0003, 0.0007, 0.0014, 0.000...   \n",
       "2  [0.0628, 0.2473, 0.4359, 0.0495, 0.065, 0.1052...   \n",
       "\n",
       "                              wind_speed_Chi_p-value  \n",
       "0  [0.3993, 0.3679, 0.5539, 0.5844, 0.5515, 0.292...  \n",
       "1  [0.0004, 0.0004, 0.0003, 0.0007, 0.0014, 0.000...  \n",
       "2  [0.0627, 0.2469, 0.4353, 0.0491, 0.0644, 0.104...  "
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/' # Office Mac\n",
    "home_dir = '/Users/yiyihe/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology/Research/Energy_resilience/' # Macbook Pro\n",
    "\n",
    "maxlag = 24 # hourly lags: 24, 72, 120\n",
    "gc_hourly_original = pd.read_csv(home_dir + f\"01_data/processed/csv/GC/granger_hourly_max{maxlag}_pvalue.csv\",\n",
    "                                 index_col=0)\n",
    "gc_hourly_original.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GC with daily data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gc_daily(maxlag, target, predictor, station_lst, daily_df):\n",
    "    # Initiate an empty dictionary for storing test results\n",
    "    gc_dic = {}\n",
    "\n",
    "    # Iterate through all stations\n",
    "    for s_id in tqdm(station_lst):\n",
    "        # Extracting the dataframe for a/one station\n",
    "        station_df = daily_df[daily_df['station_id'] == s_id].sort_values(by='date') # Sort by date\n",
    "        try:\n",
    "            # Check if the number of observations available at the station is sufficient for the gc model\n",
    "            if len(station_df) <= maxlag + 2 + 1: # maxlag + num_variables (target, predictor) + 1\n",
    "                print(f\"Skipping Station {s_id} due to insufficient observations for maxlag={maxlag}\")\n",
    "                continue\n",
    "            else:\n",
    "                # Run granger causality test on station daily data\n",
    "                test_result = grangercausalitytests(\n",
    "                    station_df[[target, predictor]], maxlag=maxlag, addconst=True, verbose=False)\n",
    "                # Save test values\n",
    "                F_test_p_values = [round(test_result[i+1][0]['ssr_ftest'][1],4) for i in range(maxlag)]\n",
    "                Chi_squared_p_values = [round(test_result[i+1][0]['ssr_chi2test'][1],4) for i in range(maxlag)]\n",
    "                p_values_min = np.min(F_test_p_values+Chi_squared_p_values)\n",
    "                # Key: station id, Value: list of 1. minimum F/Chi p values 2. F-test p values for all lags 3. Chi-square test p-values for all lags\n",
    "                gc_dic[s_id] = [p_values_min, F_test_p_values, Chi_squared_p_values]\n",
    "        except ValueError:\n",
    "            print(f\"Skipping Station {s_id} due to insufficient observations for maxlag={maxlag}\")\n",
    "    # Convert to dataframe        \n",
    "    gc_df = pd.DataFrame.from_dict(gc_dic, orient='index').reset_index()\n",
    "    gc_df.rename(columns={\n",
    "    'index':'station_id',\n",
    "    0:f'{predictor}_p-value_min',\n",
    "    1:f'{predictor}_f_p-value',\n",
    "    2:f'{predictor}_Chi_p-value'\n",
    "    }, inplace=True)\n",
    "    return gc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set home directory\n",
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/'\n",
    "# Load station daily data\n",
    "daily_df = pd.read_csv(home_dir + \"01_data/processed/csv/daily_519station_13weather.csv\", index_col=0)\n",
    "\n",
    "# Target\n",
    "target = \"daily_blackout_minutes\"\n",
    "# Predictors\n",
    "weather_variables = ['t2m', 'wind_speed', 'tp']\n",
    "aggregations = ['median', 'mean', 'max']\n",
    "predictor_lst = [w + \"_\" + a for w in weather_variables for a in aggregations]\n",
    "\n",
    "# Create a list of unique station ids\n",
    "station_id_lst = list(set(daily_df.station_id.unique()))\n",
    "\n",
    "# Loop through multiple max lag values\n",
    "for maxlag in [3, 5, 7, 15, 30, 45, 60]:\n",
    "    # Initiate empty dictionary to store test output dataframes\n",
    "    res_dic = {}\n",
    "    # Loop through the list of predictors\n",
    "    for predictor in predictor_lst:\n",
    "        # Run gc test\n",
    "        df_res = run_gc_daily(maxlag, target, predictor, station_lst, daily_df)\n",
    "        # Save output dataframe in dictionary key: predictor, value: dataframe with test values\n",
    "        res_dic[predictor] = df_res\n",
    "\n",
    "    # Join resulting dataframes for all predictors together\n",
    "    dfs = [res_dic[p] for p in predictor_lst]\n",
    "    df_joined= ft.reduce(lambda left, right: pd.merge(left, right, on='station_id'), dfs)\n",
    "    # Save output as new csv file\n",
    "    df_joined.to_csv(home_dir + f\"01_data/processed/csv/granger_daily_max{maxlag}_pvalue.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GC on daily data after first-order differencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/' # Office Mac\n",
    "home_dir = '/Users/yiyihe/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology/Research/Energy_resilience/' # Macbook Pro\n",
    "# Folder that contains the differenced data\n",
    "diff_dir = \"01_data/processed/csv/daily_diff1\"\n",
    "# Set max lag\n",
    "maxlag = 60   # 3, 5, 7, 15, 30, 45, 60\n",
    "# Iterate through station files (first order differencing applied and all time series are stationary)\n",
    "for file in tqdm(os.listdir(home_dir + diff_dir)):\n",
    "    if file[-3:] == 'csv':\n",
    "        # Extract station id\n",
    "        station_id = int(file.split('_')[1])\n",
    "        df = pd.read_csv(os.path.join(home_dir + diff_dir, file), index_col=0)\n",
    "        # Initiate empty dictionary to store results\n",
    "        gc_res = {}\n",
    "        # Target time series: percent blackout\n",
    "        target_ts = df.iloc[:, 0].values\n",
    "        # Go though rest of the columns and perform GC test. Note, stations have different column numbers, depending on how many of the original are not stationary\n",
    "        try:\n",
    "            # Check if the number of observations available at the station is sufficient for the gc model\n",
    "            if len(df) <= maxlag + 2 + 1: # maxlag + num_variables (target, predictor) + 1\n",
    "                print(f\"Skipping Station {station_id} due to insufficient observations for maxlag={maxlag}\")\n",
    "                continue\n",
    "            else:\n",
    "                for col in list(df.columns)[1:]:\n",
    "                    col_name = col.split('_diff')[0]\n",
    "                    test_result = grangercausalitytests(\n",
    "                                                        df[[list(df.columns)[0], col]],\n",
    "                                                        maxlag=maxlag,\n",
    "                                                        addconst=True,\n",
    "                                                        verbose=False\n",
    "                                                        )\n",
    "                    F_test_p_values = [round(test_result[i+1][0]['ssr_ftest'][1],4) for i in range(maxlag)] # this collects the p values for all lags\n",
    "                    Chi_squared_p_values = [round(test_result[i+1][0]['ssr_chi2test'][1],4) for i in range(maxlag)] # similar here\n",
    "                    p_values_min = np.min(F_test_p_values+Chi_squared_p_values)\n",
    "                    gc_res[col + '_p-value_min'] = p_values_min\n",
    "                    gc_res[col + '_f_p-value'] = F_test_p_values\n",
    "                    gc_res[col + '_Chi_p-value'] = Chi_squared_p_values\n",
    "                # Store results in a dataframe\n",
    "                gc_res_df = pd.DataFrame.from_dict(gc_res)\n",
    "                # Save station results to new csv\n",
    "                gc_res_df.to_csv(home_dir + f\"01_data/processed/csv/GC/daily_station_diff1/daily_station_maxlag_{maxlag}/station_{station_id}.csv\")\n",
    "        except ValueError:\n",
    "            print(f\"Skipping Station {station_id} due to insufficient observations for maxlag={maxlag}\")\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update original GC daily results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:02<00:00, 141.66it/s]\n"
     ]
    }
   ],
   "source": [
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/' # Office Mac\n",
    "home_dir = '/Users/yiyihe/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology/Research/Energy_resilience/' # Macbook Pro\n",
    "\n",
    "maxlag = 60 # 3, 5, 7, 15, 30, 45, 60\n",
    "diff_res_dir = f\"01_data/processed/csv/GC/daily_station_diff1/daily_station_maxlag_{maxlag}\"\n",
    "\n",
    "# Load original gc result dataframe\n",
    "original_gc_daily = pd.read_csv(home_dir+f'01_data/processed/csv/GC/daily/granger_daily_max{maxlag}_pvalue.csv',\n",
    "                                 index_col=0)\n",
    "\n",
    "original_gc_daily_stations = original_gc_daily.station_id.values\n",
    "\n",
    "# Iterate through files in differenced daily time series folder\n",
    "for file in tqdm(os.listdir(os.path.join(home_dir, diff_res_dir))):\n",
    "    # Extract station id\n",
    "    station_id = int(file.split('_')[1][:-4])\n",
    "    if station_id in original_gc_daily_stations:\n",
    "        if file[-3:] == 'csv':\n",
    "            # Read dataframe\n",
    "            station_diff_gc = pd.read_csv(os.path.join(home_dir, diff_res_dir, file), index_col=0)\n",
    "            # Now we are ready to extract the outputs from this dataframe\n",
    "            var_list = list(set([i.split(\"_diff\")[0] for i in station_diff_gc.columns.values]))\n",
    "            for var in var_list:\n",
    "                var_pvalue_min = station_diff_gc.filter(regex=rf'{var}_diff\\d_p-value_min').values.flatten()[0]\n",
    "                var_f_pvalues = list(station_diff_gc.filter(regex=rf'{var}_diff\\d_f_p-value').values.flatten())\n",
    "                var_chi_pvalues = list(station_diff_gc.filter(regex=rf'{var}_diff\\d_Chi_p-value').values.flatten())\n",
    "                # Update original dataframe            \n",
    "                row_idx  = original_gc_daily[original_gc_daily['station_id'] == station_id].index\n",
    "                original_gc_daily.at[row_idx.item(), f'{var}_p-value_min'] = var_pvalue_min.item()\n",
    "                original_gc_daily.at[row_idx.item(), f'{var}_f_p-value'] = [float(x) for x in var_f_pvalues]\n",
    "                original_gc_daily.at[row_idx.item(), f'{var}_Chi_p-value'] = [float(x) for x in var_chi_pvalues]\n",
    "    else:\n",
    "        continue\n",
    "original_gc_daily.to_csv(home_dir + f'01_data/processed/csv/GC/daily/granger_daily_max{maxlag}_pvalue_diff_update.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize GC results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each maxlag clean the gc results dataframe\n",
    "# select only min p-values\n",
    "home_dir = '/Users/yiyihe/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology/Research/Energy_resilience/' # Macbook Pro\n",
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/' # Office Mac\n",
    "\n",
    "maxlags = [3, 24, 72, 120]\n",
    "\n",
    "var_lst = ['t2m', 'tp', 'wind_speed']\n",
    "cols_select = [var+'_p-value_min' for var in var_lst]\n",
    "\n",
    "for maxlag in maxlags:\n",
    "    # Hourly gc results (updated with differenced time series)\n",
    "    hourly_gc_res = pd.read_csv(home_dir + f'01_data/processed/csv/GC/hourly/granger_hourly_max{maxlag}_pvalue_diff_update.csv',\n",
    "                                index_col=0)\n",
    "    hourly_gc_select = hourly_gc_res[['station_id'] + cols_select]\n",
    "    hourly_gc_select.to_csv(home_dir+f'01_data/processed/csv/GC/hourly/granger_hourly_max{maxlag}_pvalue_diff_update_select.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing GC results for maxlag=3\n",
      "Number of stations included in the analysis: 519\n",
      "412 out of 519 stations, t2m Granger Cause blackout percent.\n",
      "216 out of 519 stations, tp Granger Cause blackout percent.\n",
      "300 out of 519 stations, wind_speed Granger Cause blackout percent.\n",
      ".....................\n",
      "Summarizing GC results for maxlag=24\n",
      "Number of stations included in the analysis: 517\n",
      "485 out of 517 stations, t2m Granger Cause blackout percent.\n",
      "406 out of 517 stations, tp Granger Cause blackout percent.\n",
      "440 out of 517 stations, wind_speed Granger Cause blackout percent.\n",
      ".....................\n",
      "Summarizing GC results for maxlag=72\n",
      "Number of stations included in the analysis: 517\n",
      "498 out of 517 stations, t2m Granger Cause blackout percent.\n",
      "438 out of 517 stations, tp Granger Cause blackout percent.\n",
      "465 out of 517 stations, wind_speed Granger Cause blackout percent.\n",
      ".....................\n",
      "Summarizing GC results for maxlag=120\n",
      "Number of stations included in the analysis: 512\n",
      "499 out of 512 stations, t2m Granger Cause blackout percent.\n",
      "454 out of 512 stations, tp Granger Cause blackout percent.\n",
      "470 out of 512 stations, wind_speed Granger Cause blackout percent.\n",
      ".....................\n"
     ]
    }
   ],
   "source": [
    "maxlags = [3, 24, 72, 120]\n",
    "var_lst = ['t2m', 'tp', 'wind_speed']\n",
    "\n",
    "for maxlag in maxlags:\n",
    "    gc_hourly_select_df = pd.read_csv(home_dir+f'01_data/processed/csv/GC/hourly/granger_hourly_max{maxlag}_pvalue_diff_update_select.csv',\n",
    "                                    index_col=0)\n",
    "    number_of_stations = len(set(gc_hourly_select_df.station_id.values))\n",
    "    print(f\"Summarizing GC results for maxlag={maxlag}\")\n",
    "    print(f\"Number of stations included in the analysis: {number_of_stations}\")\n",
    "    for var in var_lst:\n",
    "        x = sum((gc_hourly_select_df[f'{var}_p-value_min']<0.05)*1)\n",
    "        print(f\"{x} out of {number_of_stations} stations, {var} Granger Cause blackout percent.\")\n",
    "    print(\".....................\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each maxlag clean the gc results dataframe\n",
    "# select only min p-values\n",
    "home_dir = '/Users/yiyihe/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology/Research/Energy_resilience/' # Macbook Pro\n",
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/' # Office Mac\n",
    "\n",
    "maxlags = [3, 5, 7, 15, 30, 45, 60]\n",
    "\n",
    "var_lst = ['t2m_max', 'tp_max', 'wind_speed_max']\n",
    "cols_select = [var+'_p-value_min' for var in var_lst]\n",
    "\n",
    "for maxlag in maxlags:\n",
    "    # Hourly gc results (updated with differenced time series)\n",
    "    daily_gc_res = pd.read_csv(home_dir + f'01_data/processed/csv/GC/daily/granger_daily_max{maxlag}_pvalue_diff_update.csv',\n",
    "                                index_col=0)\n",
    "    daily_gc_select = daily_gc_res[['station_id'] + cols_select]\n",
    "    daily_gc_select.to_csv(home_dir+f'01_data/processed/csv/GC/daily/granger_daily_max{maxlag}_pvalue_diff_update_select.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing GC results for maxlag=5\n",
      "Number of stations included in the analysis: 512\n",
      "132 out of 512 stations, t2m_max Granger Cause blackout percent.\n",
      "196 out of 512 stations, tp_max Granger Cause blackout percent.\n",
      "111 out of 512 stations, wind_speed_max Granger Cause blackout percent.\n",
      ".....................\n",
      "Summarizing GC results for maxlag=15\n",
      "Number of stations included in the analysis: 500\n",
      "201 out of 500 stations, t2m_max Granger Cause blackout percent.\n",
      "250 out of 500 stations, tp_max Granger Cause blackout percent.\n",
      "177 out of 500 stations, wind_speed_max Granger Cause blackout percent.\n",
      ".....................\n",
      "Summarizing GC results for maxlag=30\n",
      "Number of stations included in the analysis: 472\n",
      "255 out of 472 stations, t2m_max Granger Cause blackout percent.\n",
      "290 out of 472 stations, tp_max Granger Cause blackout percent.\n",
      "246 out of 472 stations, wind_speed_max Granger Cause blackout percent.\n",
      ".....................\n",
      "Summarizing GC results for maxlag=45\n",
      "Number of stations included in the analysis: 436\n",
      "296 out of 436 stations, t2m_max Granger Cause blackout percent.\n",
      "290 out of 436 stations, tp_max Granger Cause blackout percent.\n",
      "284 out of 436 stations, wind_speed_max Granger Cause blackout percent.\n",
      ".....................\n",
      "Summarizing GC results for maxlag=60\n",
      "Number of stations included in the analysis: 414\n",
      "312 out of 414 stations, t2m_max Granger Cause blackout percent.\n",
      "308 out of 414 stations, tp_max Granger Cause blackout percent.\n",
      "306 out of 414 stations, wind_speed_max Granger Cause blackout percent.\n",
      ".....................\n"
     ]
    }
   ],
   "source": [
    "maxlags = [5, 15, 30, 45, 60]\n",
    "var_lst = ['t2m_max', 'tp_max', 'wind_speed_max']\n",
    "\n",
    "for maxlag in maxlags:\n",
    "    gc_daily_select_df = pd.read_csv(home_dir+f'01_data/processed/csv/GC/daily/granger_daily_max{maxlag}_pvalue_diff_update_select.csv',\n",
    "                                    index_col=0)\n",
    "    number_of_stations = len(set(gc_daily_select_df.station_id.values))\n",
    "    print(f\"Summarizing GC results for maxlag={maxlag}\")\n",
    "    print(f\"Number of stations included in the analysis: {number_of_stations}\")\n",
    "    for var in var_lst:\n",
    "        x = sum((gc_daily_select_df[f'{var}_p-value_min']<0.05)*1)\n",
    "        print(f\"{x} out of {number_of_stations} stations, {var} Granger Cause blackout percent.\")\n",
    "    print(\".....................\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARDL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation between daily weather aggregate variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_6dc0a_row0_col0, #T_6dc0a_row1_col1, #T_6dc0a_row2_col2, #T_6dc0a_row3_col3, #T_6dc0a_row4_col4, #T_6dc0a_row5_col5, #T_6dc0a_row6_col6, #T_6dc0a_row7_col7 {\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6dc0a_row0_col1, #T_6dc0a_row0_col4, #T_6dc0a_row0_col5, #T_6dc0a_row3_col6, #T_6dc0a_row3_col7, #T_6dc0a_row4_col0, #T_6dc0a_row6_col2, #T_6dc0a_row6_col3, #T_6dc0a_row7_col3 {\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6dc0a_row0_col2, #T_6dc0a_row1_col0 {\n",
       "  background-color: #4055c8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6dc0a_row0_col3, #T_6dc0a_row2_col7 {\n",
       "  background-color: #6f92f3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6dc0a_row0_col6 {\n",
       "  background-color: #7295f4;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6dc0a_row0_col7, #T_6dc0a_row6_col1 {\n",
       "  background-color: #7396f5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6dc0a_row1_col2 {\n",
       "  background-color: #d24b40;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6dc0a_row1_col3 {\n",
       "  background-color: #f39475;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6dc0a_row1_col4 {\n",
       "  background-color: #93b5fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6dc0a_row1_col5 {\n",
       "  background-color: #b1cbfc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6dc0a_row1_col6 {\n",
       "  background-color: #a2c1ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6dc0a_row1_col7 {\n",
       "  background-color: #a9c6fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6dc0a_row2_col0 {\n",
       "  background-color: #4358cb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6dc0a_row2_col1 {\n",
       "  background-color: #d44e41;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6dc0a_row2_col3 {\n",
       "  background-color: #cc403a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6dc0a_row2_col4 {\n",
       "  background-color: #779af7;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6dc0a_row2_col5 {\n",
       "  background-color: #9ebeff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6dc0a_row2_col6 {\n",
       "  background-color: #6a8bef;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6dc0a_row3_col0 {\n",
       "  background-color: #4257c9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6dc0a_row3_col1 {\n",
       "  background-color: #f6a586;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6dc0a_row3_col2 {\n",
       "  background-color: #d0473d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6dc0a_row3_col4 {\n",
       "  background-color: #5a78e4;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6dc0a_row3_col5, #T_6dc0a_row4_col3, #T_6dc0a_row4_col7 {\n",
       "  background-color: #86a9fc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6dc0a_row4_col1 {\n",
       "  background-color: #8db0fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6dc0a_row4_col2 {\n",
       "  background-color: #7597f6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6dc0a_row4_col5, #T_6dc0a_row5_col4 {\n",
       "  background-color: #de614d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6dc0a_row4_col6 {\n",
       "  background-color: #84a7fc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6dc0a_row5_col0 {\n",
       "  background-color: #3c4ec2;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6dc0a_row5_col1 {\n",
       "  background-color: #adc9fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6dc0a_row5_col2 {\n",
       "  background-color: #9dbdff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6dc0a_row5_col3 {\n",
       "  background-color: #aec9fc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6dc0a_row5_col6, #T_6dc0a_row7_col1 {\n",
       "  background-color: #7b9ff9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6dc0a_row5_col7 {\n",
       "  background-color: #82a6fb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6dc0a_row6_col0 {\n",
       "  background-color: #455cce;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6dc0a_row6_col4 {\n",
       "  background-color: #5875e1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6dc0a_row6_col5 {\n",
       "  background-color: #4e68d8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6dc0a_row6_col7, #T_6dc0a_row7_col6 {\n",
       "  background-color: #e67259;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6dc0a_row7_col0 {\n",
       "  background-color: #485fd1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6dc0a_row7_col2 {\n",
       "  background-color: #3f53c6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6dc0a_row7_col4 {\n",
       "  background-color: #5b7ae5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6dc0a_row7_col5 {\n",
       "  background-color: #5470de;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_6dc0a\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_6dc0a_level0_col0\" class=\"col_heading level0 col0\" >daily_blackout_minutes</th>\n",
       "      <th id=\"T_6dc0a_level0_col1\" class=\"col_heading level0 col1\" >t2m_min</th>\n",
       "      <th id=\"T_6dc0a_level0_col2\" class=\"col_heading level0 col2\" >t2m_median</th>\n",
       "      <th id=\"T_6dc0a_level0_col3\" class=\"col_heading level0 col3\" >t2m_max</th>\n",
       "      <th id=\"T_6dc0a_level0_col4\" class=\"col_heading level0 col4\" >wind_speed_median</th>\n",
       "      <th id=\"T_6dc0a_level0_col5\" class=\"col_heading level0 col5\" >wind_speed_max</th>\n",
       "      <th id=\"T_6dc0a_level0_col6\" class=\"col_heading level0 col6\" >tp_median</th>\n",
       "      <th id=\"T_6dc0a_level0_col7\" class=\"col_heading level0 col7\" >tp_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_6dc0a_level0_row0\" class=\"row_heading level0 row0\" >daily_blackout_minutes</th>\n",
       "      <td id=\"T_6dc0a_row0_col0\" class=\"data row0 col0\" >1.000000</td>\n",
       "      <td id=\"T_6dc0a_row0_col1\" class=\"data row0 col1\" >0.020774</td>\n",
       "      <td id=\"T_6dc0a_row0_col2\" class=\"data row0 col2\" >0.027144</td>\n",
       "      <td id=\"T_6dc0a_row0_col3\" class=\"data row0 col3\" >0.024727</td>\n",
       "      <td id=\"T_6dc0a_row0_col4\" class=\"data row0 col4\" >-0.002525</td>\n",
       "      <td id=\"T_6dc0a_row0_col5\" class=\"data row0 col5\" >0.004347</td>\n",
       "      <td id=\"T_6dc0a_row0_col6\" class=\"data row0 col6\" >0.034892</td>\n",
       "      <td id=\"T_6dc0a_row0_col7\" class=\"data row0 col7\" >0.040650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6dc0a_level0_row1\" class=\"row_heading level0 row1\" >t2m_min</th>\n",
       "      <td id=\"T_6dc0a_row1_col0\" class=\"data row1 col0\" >0.020774</td>\n",
       "      <td id=\"T_6dc0a_row1_col1\" class=\"data row1 col1\" >1.000000</td>\n",
       "      <td id=\"T_6dc0a_row1_col2\" class=\"data row1 col2\" >0.911219</td>\n",
       "      <td id=\"T_6dc0a_row1_col3\" class=\"data row1 col3\" >0.725396</td>\n",
       "      <td id=\"T_6dc0a_row1_col4\" class=\"data row1 col4\" >0.265783</td>\n",
       "      <td id=\"T_6dc0a_row1_col5\" class=\"data row1 col5\" >0.357374</td>\n",
       "      <td id=\"T_6dc0a_row1_col6\" class=\"data row1 col6\" >0.192918</td>\n",
       "      <td id=\"T_6dc0a_row1_col7\" class=\"data row1 col7\" >0.218318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6dc0a_level0_row2\" class=\"row_heading level0 row2\" >t2m_median</th>\n",
       "      <td id=\"T_6dc0a_row2_col0\" class=\"data row2 col0\" >0.027144</td>\n",
       "      <td id=\"T_6dc0a_row2_col1\" class=\"data row2 col1\" >0.911219</td>\n",
       "      <td id=\"T_6dc0a_row2_col2\" class=\"data row2 col2\" >1.000000</td>\n",
       "      <td id=\"T_6dc0a_row2_col3\" class=\"data row2 col3\" >0.921124</td>\n",
       "      <td id=\"T_6dc0a_row2_col4\" class=\"data row2 col4\" >0.187497</td>\n",
       "      <td id=\"T_6dc0a_row2_col5\" class=\"data row2 col5\" >0.300769</td>\n",
       "      <td id=\"T_6dc0a_row2_col6\" class=\"data row2 col6\" >0.007608</td>\n",
       "      <td id=\"T_6dc0a_row2_col7\" class=\"data row2 col7\" >0.023848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6dc0a_level0_row3\" class=\"row_heading level0 row3\" >t2m_max</th>\n",
       "      <td id=\"T_6dc0a_row3_col0\" class=\"data row3 col0\" >0.024727</td>\n",
       "      <td id=\"T_6dc0a_row3_col1\" class=\"data row3 col1\" >0.725396</td>\n",
       "      <td id=\"T_6dc0a_row3_col2\" class=\"data row3 col2\" >0.921124</td>\n",
       "      <td id=\"T_6dc0a_row3_col3\" class=\"data row3 col3\" >1.000000</td>\n",
       "      <td id=\"T_6dc0a_row3_col4\" class=\"data row3 col4\" >0.102492</td>\n",
       "      <td id=\"T_6dc0a_row3_col5\" class=\"data row3 col5\" >0.235172</td>\n",
       "      <td id=\"T_6dc0a_row3_col6\" class=\"data row3 col6\" >-0.170036</td>\n",
       "      <td id=\"T_6dc0a_row3_col7\" class=\"data row3 col7\" >-0.169363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6dc0a_level0_row4\" class=\"row_heading level0 row4\" >wind_speed_median</th>\n",
       "      <td id=\"T_6dc0a_row4_col0\" class=\"data row4 col0\" >-0.002525</td>\n",
       "      <td id=\"T_6dc0a_row4_col1\" class=\"data row4 col1\" >0.265783</td>\n",
       "      <td id=\"T_6dc0a_row4_col2\" class=\"data row4 col2\" >0.187497</td>\n",
       "      <td id=\"T_6dc0a_row4_col3\" class=\"data row4 col3\" >0.102492</td>\n",
       "      <td id=\"T_6dc0a_row4_col4\" class=\"data row4 col4\" >1.000000</td>\n",
       "      <td id=\"T_6dc0a_row4_col5\" class=\"data row4 col5\" >0.872144</td>\n",
       "      <td id=\"T_6dc0a_row4_col6\" class=\"data row4 col6\" >0.091777</td>\n",
       "      <td id=\"T_6dc0a_row4_col7\" class=\"data row4 col7\" >0.104201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6dc0a_level0_row5\" class=\"row_heading level0 row5\" >wind_speed_max</th>\n",
       "      <td id=\"T_6dc0a_row5_col0\" class=\"data row5 col0\" >0.004347</td>\n",
       "      <td id=\"T_6dc0a_row5_col1\" class=\"data row5 col1\" >0.357374</td>\n",
       "      <td id=\"T_6dc0a_row5_col2\" class=\"data row5 col2\" >0.300769</td>\n",
       "      <td id=\"T_6dc0a_row5_col3\" class=\"data row5 col3\" >0.235172</td>\n",
       "      <td id=\"T_6dc0a_row5_col4\" class=\"data row5 col4\" >0.872144</td>\n",
       "      <td id=\"T_6dc0a_row5_col5\" class=\"data row5 col5\" >1.000000</td>\n",
       "      <td id=\"T_6dc0a_row5_col6\" class=\"data row5 col6\" >0.067437</td>\n",
       "      <td id=\"T_6dc0a_row5_col7\" class=\"data row5 col7\" >0.087814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6dc0a_level0_row6\" class=\"row_heading level0 row6\" >tp_median</th>\n",
       "      <td id=\"T_6dc0a_row6_col0\" class=\"data row6 col0\" >0.034892</td>\n",
       "      <td id=\"T_6dc0a_row6_col1\" class=\"data row6 col1\" >0.192918</td>\n",
       "      <td id=\"T_6dc0a_row6_col2\" class=\"data row6 col2\" >0.007608</td>\n",
       "      <td id=\"T_6dc0a_row6_col3\" class=\"data row6 col3\" >-0.170036</td>\n",
       "      <td id=\"T_6dc0a_row6_col4\" class=\"data row6 col4\" >0.091777</td>\n",
       "      <td id=\"T_6dc0a_row6_col5\" class=\"data row6 col5\" >0.067437</td>\n",
       "      <td id=\"T_6dc0a_row6_col6\" class=\"data row6 col6\" >1.000000</td>\n",
       "      <td id=\"T_6dc0a_row6_col7\" class=\"data row6 col7\" >0.812423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6dc0a_level0_row7\" class=\"row_heading level0 row7\" >tp_max</th>\n",
       "      <td id=\"T_6dc0a_row7_col0\" class=\"data row7 col0\" >0.040650</td>\n",
       "      <td id=\"T_6dc0a_row7_col1\" class=\"data row7 col1\" >0.218318</td>\n",
       "      <td id=\"T_6dc0a_row7_col2\" class=\"data row7 col2\" >0.023848</td>\n",
       "      <td id=\"T_6dc0a_row7_col3\" class=\"data row7 col3\" >-0.169363</td>\n",
       "      <td id=\"T_6dc0a_row7_col4\" class=\"data row7 col4\" >0.104201</td>\n",
       "      <td id=\"T_6dc0a_row7_col5\" class=\"data row7 col5\" >0.087814</td>\n",
       "      <td id=\"T_6dc0a_row7_col6\" class=\"data row7 col6\" >0.812423</td>\n",
       "      <td id=\"T_6dc0a_row7_col7\" class=\"data row7 col7\" >1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x376f3aa90>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set home directory\n",
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/'\n",
    "# Load station daily data\n",
    "daily_df = pd.read_csv(home_dir + \"01_data/processed/csv/daily_519station_13weather.csv\", index_col=0)\n",
    "\n",
    "# Correlation\n",
    "correlation_matrix = daily_df[['daily_blackout_minutes',\n",
    "                                        't2m_min',\n",
    "                                        't2m_median',\n",
    "                                        't2m_max',\n",
    "                                        'wind_speed_median',\n",
    "                                        'wind_speed_max',\n",
    "                                        'tp_median',\n",
    "                                        'tp_max']].corr()\n",
    "correlation_matrix.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the aggregate of one particular weather variable is highly correlated with one another. We decide to use the maximum for each weather variable: t2m_max, tp_max, and wind_speed_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_e5c37_row0_col0, #T_e5c37_row1_col1, #T_e5c37_row2_col2, #T_e5c37_row3_col3 {\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e5c37_row0_col1 {\n",
       "  background-color: #6f92f3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e5c37_row0_col2, #T_e5c37_row1_col3, #T_e5c37_row2_col0, #T_e5c37_row3_col1 {\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e5c37_row0_col3 {\n",
       "  background-color: #7396f5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e5c37_row1_col0 {\n",
       "  background-color: #4055c8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e5c37_row1_col2 {\n",
       "  background-color: #86a9fc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e5c37_row2_col1 {\n",
       "  background-color: #aec9fc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e5c37_row2_col3 {\n",
       "  background-color: #82a6fb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e5c37_row3_col0 {\n",
       "  background-color: #455cce;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e5c37_row3_col2 {\n",
       "  background-color: #5470de;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_e5c37\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e5c37_level0_col0\" class=\"col_heading level0 col0\" >daily_blackout_minutes</th>\n",
       "      <th id=\"T_e5c37_level0_col1\" class=\"col_heading level0 col1\" >t2m_max</th>\n",
       "      <th id=\"T_e5c37_level0_col2\" class=\"col_heading level0 col2\" >wind_speed_max</th>\n",
       "      <th id=\"T_e5c37_level0_col3\" class=\"col_heading level0 col3\" >tp_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e5c37_level0_row0\" class=\"row_heading level0 row0\" >daily_blackout_minutes</th>\n",
       "      <td id=\"T_e5c37_row0_col0\" class=\"data row0 col0\" >1.000000</td>\n",
       "      <td id=\"T_e5c37_row0_col1\" class=\"data row0 col1\" >0.024727</td>\n",
       "      <td id=\"T_e5c37_row0_col2\" class=\"data row0 col2\" >0.004347</td>\n",
       "      <td id=\"T_e5c37_row0_col3\" class=\"data row0 col3\" >0.040650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e5c37_level0_row1\" class=\"row_heading level0 row1\" >t2m_max</th>\n",
       "      <td id=\"T_e5c37_row1_col0\" class=\"data row1 col0\" >0.024727</td>\n",
       "      <td id=\"T_e5c37_row1_col1\" class=\"data row1 col1\" >1.000000</td>\n",
       "      <td id=\"T_e5c37_row1_col2\" class=\"data row1 col2\" >0.235172</td>\n",
       "      <td id=\"T_e5c37_row1_col3\" class=\"data row1 col3\" >-0.169363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e5c37_level0_row2\" class=\"row_heading level0 row2\" >wind_speed_max</th>\n",
       "      <td id=\"T_e5c37_row2_col0\" class=\"data row2 col0\" >0.004347</td>\n",
       "      <td id=\"T_e5c37_row2_col1\" class=\"data row2 col1\" >0.235172</td>\n",
       "      <td id=\"T_e5c37_row2_col2\" class=\"data row2 col2\" >1.000000</td>\n",
       "      <td id=\"T_e5c37_row2_col3\" class=\"data row2 col3\" >0.087814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e5c37_level0_row3\" class=\"row_heading level0 row3\" >tp_max</th>\n",
       "      <td id=\"T_e5c37_row3_col0\" class=\"data row3 col0\" >0.040650</td>\n",
       "      <td id=\"T_e5c37_row3_col1\" class=\"data row3 col1\" >-0.169363</td>\n",
       "      <td id=\"T_e5c37_row3_col2\" class=\"data row3 col2\" >0.087814</td>\n",
       "      <td id=\"T_e5c37_row3_col3\" class=\"data row3 col3\" >1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x3778988b0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation_matrix = daily_df[['daily_blackout_minutes',\n",
    "                                        't2m_max',\n",
    "                                        'wind_speed_max',\n",
    "                                        'tp_max']].corr()\n",
    "correlation_matrix.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show low correlation between the max of three weather variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ARDL_findlag_daily(maxlag, target, predictor_lst, station_lst, ic, daily_df):\n",
    "    # Initiate an empty dictionary\n",
    "    res_dic = {}\n",
    "\n",
    "    # Iterate through all stations\n",
    "    for s_id in tqdm(station_lst):\n",
    "            try:\n",
    "                # Subset station data\n",
    "                station_df = daily_df[daily_df['station_id'] == s_id].sort_values(by='date')\n",
    "                # Ignore stations with less than 90 days/ 3 months worth of data\n",
    "                if len(station_df) <= 90:\n",
    "                     continue\n",
    "                else:\n",
    "                    # Find optimum lag and store station id with optimum lag in dictionary\n",
    "                    sel_res = ardl_select_order(\n",
    "                        station_df[target],\n",
    "                        exog=station_df[predictors_lst],\n",
    "                        maxlag=maxlag,\n",
    "                        ic= ic,# string\n",
    "                        maxorder=maxlag,\n",
    "                        causal=True,\n",
    "                        trend='ct'\n",
    "                        )\n",
    "                    # optimum lag values for endogenous variables\n",
    "                    endo_res_lst = list(sel_res.bic.values[0][1].values())\n",
    "                    # insert the optimum lag value for exdogenous variable\n",
    "                    endo_res_lst.insert(0, sel_res.bic.values[0][0])\n",
    "                    # store station id and optimum lag values in dictionary\n",
    "                    res_dic[s_id] = endo_res_lst # key: station id, value:[pct_blackout_lag, t2m_lag, wind_speed_lag, tp_lag]\n",
    "            except ValueError:\n",
    "                 print('error')\n",
    "    # Save results in dataframe             \n",
    "    res_df = pd.DataFrame.from_dict(res_dic, orient='index')\n",
    "\n",
    "    # Rename columns using meaningful names\n",
    "    res_final_df = res_df.reset_index().rename(columns={**{'index': 'station_id', 0: 'blackout_minutes_lag'},\n",
    "                                                        **{i+1: val+'_lag' for i, val in enumerate(predictors_lst)}})\n",
    "    return res_final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (3435420696.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[78], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    break\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "# Set home directory\n",
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/'\n",
    "# Load station daily data\n",
    "daily_df = pd.read_csv(home_dir + \"01_data/processed/csv/daily_519station_13weather.csv\", index_col=0)\n",
    "\n",
    "# Loop through different maxlag values\n",
    "for maxlag in [5, 10, 15, 30, 45, 60]: # unit: days\n",
    "    print(f'I am working on maxlag={maxlag} now.')\n",
    "    # Set target variable\n",
    "    target = 'daily_blackout_minutes'\n",
    "    # Set list of predictor variables\n",
    "    predictors_lst = [\n",
    "                    't2m_max',\n",
    "                    'wind_speed_max',\n",
    "                    'tp_max']\n",
    "    # list of unique station ids\n",
    "    station_lst = list(set(daily_df.station_id.unique()))\n",
    "    # Information criterion\n",
    "    ic = 'bic'\n",
    "    # Run ARDL find lag\n",
    "    result_df = run_ARDL_findlag_daily(maxlag, target, predictor_lst, station_lst, ic, daily_df)\n",
    "    # Save output dataframe to csv\n",
    "    result_df.to_csv(home_dir + f\"01_data/processed/csv/ARDL_daily_optimum_{ic}_max{maxlag}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>date</th>\n",
       "      <th>t2m_mean</th>\n",
       "      <th>t2m_max</th>\n",
       "      <th>t2m_min</th>\n",
       "      <th>t2m_median</th>\n",
       "      <th>tp_mean</th>\n",
       "      <th>tp_max</th>\n",
       "      <th>tp_min</th>\n",
       "      <th>tp_median</th>\n",
       "      <th>tp_sum</th>\n",
       "      <th>wind_speed_mean</th>\n",
       "      <th>wind_speed_max</th>\n",
       "      <th>wind_speed_min</th>\n",
       "      <th>wind_speed_median</th>\n",
       "      <th>daily_blackout_minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-07-08</td>\n",
       "      <td>296.166617</td>\n",
       "      <td>299.24072</td>\n",
       "      <td>293.98560</td>\n",
       "      <td>295.799435</td>\n",
       "      <td>9.275324e-04</td>\n",
       "      <td>3.313719e-03</td>\n",
       "      <td>7.747673e-06</td>\n",
       "      <td>1.117562e-03</td>\n",
       "      <td>2.226078e-02</td>\n",
       "      <td>5.699010</td>\n",
       "      <td>7.550663</td>\n",
       "      <td>4.363706</td>\n",
       "      <td>5.122588</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-07-09</td>\n",
       "      <td>295.860636</td>\n",
       "      <td>298.48560</td>\n",
       "      <td>294.01050</td>\n",
       "      <td>295.375610</td>\n",
       "      <td>4.631401e-04</td>\n",
       "      <td>1.136347e-03</td>\n",
       "      <td>1.545996e-07</td>\n",
       "      <td>5.342960e-04</td>\n",
       "      <td>1.111536e-02</td>\n",
       "      <td>6.752629</td>\n",
       "      <td>8.633310</td>\n",
       "      <td>5.489735</td>\n",
       "      <td>6.040919</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-07-10</td>\n",
       "      <td>295.965729</td>\n",
       "      <td>299.11914</td>\n",
       "      <td>293.96020</td>\n",
       "      <td>295.257570</td>\n",
       "      <td>4.478330e-04</td>\n",
       "      <td>6.784946e-04</td>\n",
       "      <td>5.487353e-06</td>\n",
       "      <td>5.421415e-04</td>\n",
       "      <td>1.074799e-02</td>\n",
       "      <td>7.102749</td>\n",
       "      <td>9.077559</td>\n",
       "      <td>5.863512</td>\n",
       "      <td>6.397305</td>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-07-11</td>\n",
       "      <td>295.709585</td>\n",
       "      <td>298.28320</td>\n",
       "      <td>293.79126</td>\n",
       "      <td>295.232055</td>\n",
       "      <td>2.354888e-04</td>\n",
       "      <td>6.237924e-04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.711296e-04</td>\n",
       "      <td>5.651732e-03</td>\n",
       "      <td>6.524960</td>\n",
       "      <td>8.474811</td>\n",
       "      <td>5.160284</td>\n",
       "      <td>6.002289</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-07-12</td>\n",
       "      <td>296.420979</td>\n",
       "      <td>298.74194</td>\n",
       "      <td>293.71875</td>\n",
       "      <td>296.584840</td>\n",
       "      <td>4.240637e-04</td>\n",
       "      <td>6.549954e-04</td>\n",
       "      <td>8.772127e-06</td>\n",
       "      <td>5.448125e-04</td>\n",
       "      <td>7.633146e-03</td>\n",
       "      <td>6.024988</td>\n",
       "      <td>7.767821</td>\n",
       "      <td>4.536693</td>\n",
       "      <td>5.608815</td>\n",
       "      <td>409.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299376</th>\n",
       "      <td>572</td>\n",
       "      <td>2023-12-17</td>\n",
       "      <td>294.954003</td>\n",
       "      <td>300.24332</td>\n",
       "      <td>289.23890</td>\n",
       "      <td>294.639085</td>\n",
       "      <td>8.553097e-07</td>\n",
       "      <td>1.722574e-06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.523463e-07</td>\n",
       "      <td>2.052743e-05</td>\n",
       "      <td>1.857829</td>\n",
       "      <td>2.220973</td>\n",
       "      <td>1.237675</td>\n",
       "      <td>1.907954</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299377</th>\n",
       "      <td>572</td>\n",
       "      <td>2023-12-18</td>\n",
       "      <td>294.000486</td>\n",
       "      <td>299.26468</td>\n",
       "      <td>289.42383</td>\n",
       "      <td>293.379240</td>\n",
       "      <td>3.922040e-07</td>\n",
       "      <td>1.716614e-06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>9.412896e-06</td>\n",
       "      <td>2.429915</td>\n",
       "      <td>3.125128</td>\n",
       "      <td>2.021494</td>\n",
       "      <td>2.301438</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299378</th>\n",
       "      <td>572</td>\n",
       "      <td>2023-12-19</td>\n",
       "      <td>293.360410</td>\n",
       "      <td>298.80225</td>\n",
       "      <td>288.74588</td>\n",
       "      <td>292.886940</td>\n",
       "      <td>7.102953e-08</td>\n",
       "      <td>1.704709e-06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.704709e-06</td>\n",
       "      <td>2.159092</td>\n",
       "      <td>2.861767</td>\n",
       "      <td>1.683862</td>\n",
       "      <td>2.078050</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299379</th>\n",
       "      <td>572</td>\n",
       "      <td>2023-12-20</td>\n",
       "      <td>292.857461</td>\n",
       "      <td>299.13712</td>\n",
       "      <td>287.81485</td>\n",
       "      <td>292.097045</td>\n",
       "      <td>2.841155e-07</td>\n",
       "      <td>4.261732e-07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.261732e-07</td>\n",
       "      <td>6.818771e-06</td>\n",
       "      <td>1.667471</td>\n",
       "      <td>2.025997</td>\n",
       "      <td>1.281688</td>\n",
       "      <td>1.599746</td>\n",
       "      <td>388.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299380</th>\n",
       "      <td>572</td>\n",
       "      <td>2023-12-21</td>\n",
       "      <td>294.103109</td>\n",
       "      <td>300.31640</td>\n",
       "      <td>286.92050</td>\n",
       "      <td>295.032230</td>\n",
       "      <td>3.874302e-08</td>\n",
       "      <td>4.261732e-07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.261732e-07</td>\n",
       "      <td>1.416571</td>\n",
       "      <td>1.888933</td>\n",
       "      <td>1.135638</td>\n",
       "      <td>1.343450</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>299381 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        station_id        date    t2m_mean    t2m_max    t2m_min  t2m_median  \\\n",
       "0                1  2016-07-08  296.166617  299.24072  293.98560  295.799435   \n",
       "1                1  2016-07-09  295.860636  298.48560  294.01050  295.375610   \n",
       "2                1  2016-07-10  295.965729  299.11914  293.96020  295.257570   \n",
       "3                1  2016-07-11  295.709585  298.28320  293.79126  295.232055   \n",
       "4                1  2016-07-12  296.420979  298.74194  293.71875  296.584840   \n",
       "...            ...         ...         ...        ...        ...         ...   \n",
       "299376         572  2023-12-17  294.954003  300.24332  289.23890  294.639085   \n",
       "299377         572  2023-12-18  294.000486  299.26468  289.42383  293.379240   \n",
       "299378         572  2023-12-19  293.360410  298.80225  288.74588  292.886940   \n",
       "299379         572  2023-12-20  292.857461  299.13712  287.81485  292.097045   \n",
       "299380         572  2023-12-21  294.103109  300.31640  286.92050  295.032230   \n",
       "\n",
       "             tp_mean        tp_max        tp_min     tp_median        tp_sum  \\\n",
       "0       9.275324e-04  3.313719e-03  7.747673e-06  1.117562e-03  2.226078e-02   \n",
       "1       4.631401e-04  1.136347e-03  1.545996e-07  5.342960e-04  1.111536e-02   \n",
       "2       4.478330e-04  6.784946e-04  5.487353e-06  5.421415e-04  1.074799e-02   \n",
       "3       2.354888e-04  6.237924e-04  0.000000e+00  2.711296e-04  5.651732e-03   \n",
       "4       4.240637e-04  6.549954e-04  8.772127e-06  5.448125e-04  7.633146e-03   \n",
       "...              ...           ...           ...           ...           ...   \n",
       "299376  8.553097e-07  1.722574e-06  0.000000e+00  8.523463e-07  2.052743e-05   \n",
       "299377  3.922040e-07  1.716614e-06  0.000000e+00  0.000000e+00  9.412896e-06   \n",
       "299378  7.102953e-08  1.704709e-06  0.000000e+00  0.000000e+00  1.704709e-06   \n",
       "299379  2.841155e-07  4.261732e-07  0.000000e+00  4.261732e-07  6.818771e-06   \n",
       "299380  3.874302e-08  4.261732e-07  0.000000e+00  0.000000e+00  4.261732e-07   \n",
       "\n",
       "        wind_speed_mean  wind_speed_max  wind_speed_min  wind_speed_median  \\\n",
       "0              5.699010        7.550663        4.363706           5.122588   \n",
       "1              6.752629        8.633310        5.489735           6.040919   \n",
       "2              7.102749        9.077559        5.863512           6.397305   \n",
       "3              6.524960        8.474811        5.160284           6.002289   \n",
       "4              6.024988        7.767821        4.536693           5.608815   \n",
       "...                 ...             ...             ...                ...   \n",
       "299376         1.857829        2.220973        1.237675           1.907954   \n",
       "299377         2.429915        3.125128        2.021494           2.301438   \n",
       "299378         2.159092        2.861767        1.683862           2.078050   \n",
       "299379         1.667471        2.025997        1.281688           1.599746   \n",
       "299380         1.416571        1.888933        1.135638           1.343450   \n",
       "\n",
       "        daily_blackout_minutes  \n",
       "0                         25.0  \n",
       "1                         88.0  \n",
       "2                         81.0  \n",
       "3                          7.0  \n",
       "4                        409.0  \n",
       "...                        ...  \n",
       "299376                     0.0  \n",
       "299377                    15.0  \n",
       "299378                     0.0  \n",
       "299379                   388.0  \n",
       "299380                     0.0  \n",
       "\n",
       "[299381 rows x 16 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
