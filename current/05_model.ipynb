{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----********************-----\n",
    "\n",
    "# Created Time: 2025/07/13\n",
    "\n",
    "# Last updated: 2025/07/21\n",
    "\n",
    "# Author: Yiyi He\n",
    "\n",
    "### Use Case\n",
    "\n",
    "# This notebook explores the application of Granger Causality and autoregressive models\n",
    "# 1.\n",
    "\n",
    "# -----********************-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Stats\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "from statsmodels.tsa.api import ARDL\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "from statsmodels.tsa.ardl import ardl_select_order\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "# Geo\n",
    "from shapely.geometry import Point, Polygon\n",
    "# import geopandas as gpd\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.options.display.max_rows = 1000\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# Processing\n",
    "from tqdm import tqdm\n",
    "import functools as ft\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stationarity Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 519/519 [1:12:38<00:00,  8.40s/it]\n"
     ]
    }
   ],
   "source": [
    "# Hourly\n",
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/' # Office Mac\n",
    "home_dir = '/Users/yiyihe/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology/Research/Energy_resilience/' # Macbook Pro\n",
    "\n",
    "hourly_df = pd.read_csv(home_dir + \"01_data/processed/csv/hourly_519station_3weather.csv\",\n",
    "                        index_col=0)\n",
    "\n",
    "# Generate a list of station ids\n",
    "station_lst = list(set(hourly_df.station_id.unique()))\n",
    "\n",
    "adf_res = {}\n",
    "for s_id in tqdm(station_lst):\n",
    "        # Extracting the dataframe for a/one station\n",
    "        station_df = hourly_df[hourly_df['station_id'] == s_id].sort_values(by='datetime')\n",
    "        # Extact time series\n",
    "        pct_blackout_ts = station_df.pct_blackout.values # Target\n",
    "        t2m_ts = station_df.t2m.values # Predictor\n",
    "        tp_ts = station_df.tp.values # Predictor\n",
    "        wind_speed_ts = station_df.wind_speed.values # Predictor\n",
    "\n",
    "        adf_res[s_id] = [\n",
    "                        adfuller(pct_blackout_ts)[1], # p-value\n",
    "                        adfuller(t2m_ts)[1],          # p-value\n",
    "                        adfuller(tp_ts)[1],           # p-value\n",
    "                        adfuller(wind_speed_ts)[1]    # p-value\n",
    "        ]\n",
    "adf_res_df = pd.DataFrame.from_dict(adf_res, orient='index').reset_index()\n",
    "adf_res_df.rename(columns={\n",
    "    'index': 'station_id',\n",
    "    0: 'pct_blackout_pvalue',\n",
    "    1: 't2m_pvalue',\n",
    "    2: 'tp_pvalue',\n",
    "    3: 'wind_speed_pvalue'\n",
    "    }, inplace=True)\n",
    "adf_res_df.to_csv(home_dir + '01_data/processed/csv/ADF/hourly_ADF_test_pvalues.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pct_blackout_pvalue</th>\n",
       "      <th>t2m_pvalue</th>\n",
       "      <th>tp_pvalue</th>\n",
       "      <th>wind_speed_pvalue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.007</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.140</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     pct_blackout_pvalue t2m_pvalue tp_pvalue wind_speed_pvalue\n",
       "mean               0.001      0.229     0.006             0.007\n",
       "std                0.007      0.263     0.063             0.043\n",
       "min                0.000      0.000     0.000             0.000\n",
       "25%                0.000      0.010     0.000             0.000\n",
       "50%                0.000      0.130     0.000             0.000\n",
       "75%                0.000      0.358     0.000             0.000\n",
       "max                0.140      0.951     0.903             0.571"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary statistics\n",
    "hourly_adf_res = pd.read_csv(home_dir + '01_data/processed/csv/Stationarity_test/hourly_ADF_test_pvalues.csv',\n",
    "                             index_col=0)\n",
    "\n",
    "# summarize number of stationary time series\n",
    "# (hourly_adf_res.iloc[:, 1:]<0.05).sum()\n",
    "# Summary statistics for p values\n",
    "(hourly_adf_res.iloc[:, 1:]).describe().iloc[1:].applymap(lambda x: f\"{x:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 519/519 [05:09<00:00,  1.67it/s]\n"
     ]
    }
   ],
   "source": [
    "# Daily\n",
    "# Set home directory\n",
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/' # Office Mac\n",
    "home_dir = '/Users/yiyihe/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology/Research/Energy_resilience/' # Macbook Pro\n",
    "\n",
    "# Load station daily data\n",
    "daily_df = pd.read_csv(home_dir + \"01_data/processed/csv/daily_519station_13weather.csv\", index_col=0)\n",
    "\n",
    "# Generate a list of station ids\n",
    "station_lst = list(set(daily_df.station_id.unique()))\n",
    "\n",
    "# Predictors\n",
    "weather_variables = ['t2m', 'wind_speed', 'tp']\n",
    "aggregations = ['median', 'mean', 'max']\n",
    "predictor_lst = ['daily_blackout_minutes'] + [w + \"_\" + a for w in weather_variables for a in aggregations]\n",
    "\n",
    "adf_res = {}\n",
    "for s_id in tqdm(station_lst):\n",
    "        # Extracting the dataframe for a/one station\n",
    "        station_df = daily_df[daily_df['station_id'] == s_id].sort_values(by='date')\n",
    "        adf_res_lst = []\n",
    "        for var in predictor_lst:\n",
    "                # Extact time series\n",
    "                ts = station_df[var].values\n",
    "                adf_res_lst.append(adfuller(ts)[1]) # append pvalue\n",
    "\n",
    "        adf_res[s_id] = adf_res_lst\n",
    "\n",
    "adf_res_df = pd.DataFrame.from_dict(adf_res, orient='index').reset_index()\n",
    "# Dictionary to update column names\n",
    "rename_dict = {'index': 'station_id'}\n",
    "rename_dict.update({i: var_name + '_pvalue' for i, var_name in enumerate(predictor_lst)})\n",
    "# Rename columns\n",
    "adf_res_df.rename(columns=rename_dict, inplace=True)\n",
    "adf_res_df.to_csv(home_dir + '01_data/processed/csv/ADF/daily_ADF_test_pvalues.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>daily_blackout_minutes_pvalue</th>\n",
       "      <th>t2m_median_pvalue</th>\n",
       "      <th>t2m_mean_pvalue</th>\n",
       "      <th>t2m_max_pvalue</th>\n",
       "      <th>wind_speed_median_pvalue</th>\n",
       "      <th>wind_speed_mean_pvalue</th>\n",
       "      <th>wind_speed_max_pvalue</th>\n",
       "      <th>tp_median_pvalue</th>\n",
       "      <th>tp_mean_pvalue</th>\n",
       "      <th>tp_max_pvalue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.030</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.121</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.682</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.948</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     daily_blackout_minutes_pvalue t2m_median_pvalue t2m_mean_pvalue  \\\n",
       "mean                         0.030             0.432           0.440   \n",
       "std                          0.121             0.317           0.316   \n",
       "min                          0.000             0.000           0.000   \n",
       "25%                          0.000             0.134           0.154   \n",
       "50%                          0.000             0.410           0.424   \n",
       "75%                          0.000             0.680           0.682   \n",
       "max                          0.948             1.000           1.000   \n",
       "\n",
       "     t2m_max_pvalue wind_speed_median_pvalue wind_speed_mean_pvalue  \\\n",
       "mean          0.329                    0.044                  0.049   \n",
       "std           0.281                    0.133                  0.143   \n",
       "min           0.000                    0.000                  0.000   \n",
       "25%           0.078                    0.000                  0.000   \n",
       "50%           0.261                    0.000                  0.000   \n",
       "75%           0.526                    0.017                  0.025   \n",
       "max           0.999                    0.979                  0.997   \n",
       "\n",
       "     wind_speed_max_pvalue tp_median_pvalue tp_mean_pvalue tp_max_pvalue  \n",
       "mean                 0.093            0.077          0.072         0.083  \n",
       "std                  0.165            0.171          0.163         0.184  \n",
       "min                  0.000            0.000          0.000         0.000  \n",
       "25%                  0.000            0.000          0.000         0.000  \n",
       "50%                  0.006            0.002          0.001         0.003  \n",
       "75%                  0.116            0.064          0.056         0.057  \n",
       "max                  0.999            1.000          0.999         1.000  "
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary statistics\n",
    "daily_adf_res = pd.read_csv(home_dir + '01_data/processed/csv/Stationarity_test/daily_ADF_test_pvalues.csv',\n",
    "                             index_col=0)\n",
    "\n",
    "# summarize number of stationary time series\n",
    "(daily_adf_res.iloc[:, 1:]<0.05).sum()\n",
    "\n",
    "# Summary statistics for p values\n",
    "(daily_adf_res.iloc[:, 1:]).describe().iloc[1:].applymap(lambda x: f\"{x:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KPSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 519/519 [00:30<00:00, 17.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# Hourly\n",
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/' # Office Mac\n",
    "home_dir = '/Users/yiyihe/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology/Research/Energy_resilience/' # Macbook Pro\n",
    "\n",
    "hourly_df = pd.read_csv(home_dir + \"01_data/processed/csv/hourly_519station_3weather.csv\",\n",
    "                        index_col=0)\n",
    "\n",
    "# Generate a list of station ids\n",
    "station_lst = list(set(hourly_df.station_id.unique()))\n",
    "\n",
    "kpss_res = {}\n",
    "for s_id in tqdm(station_lst):\n",
    "        # Extracting the dataframe for a/one station\n",
    "        station_df = hourly_df[hourly_df['station_id'] == s_id].sort_values(by='datetime')\n",
    "        # Extact time series\n",
    "        pct_blackout_ts = station_df.pct_blackout.values # Target\n",
    "        t2m_ts = station_df.t2m.values # Predictor\n",
    "        tp_ts = station_df.tp.values # Predictor\n",
    "        wind_speed_ts = station_df.wind_speed.values # Predictor\n",
    "\n",
    "        kpss_res[s_id] = [\n",
    "                        kpss(pct_blackout_ts, regression=\"c\", nlags=\"auto\")[1], # p-value\n",
    "                        kpss(t2m_ts, regression=\"c\", nlags=\"auto\")[1],          # p-value\n",
    "                        kpss(tp_ts, regression=\"c\", nlags=\"auto\")[1],           # p-value\n",
    "                        kpss(wind_speed_ts, regression=\"c\", nlags=\"auto\")[1]    # p-value\n",
    "        ]\n",
    "kpss_res_df = pd.DataFrame.from_dict(kpss_res, orient='index').reset_index()\n",
    "kpss_res_df.rename(columns={\n",
    "    'index': 'station_id',\n",
    "    0: 'pct_blackout_pvalue',\n",
    "    1: 't2m_pvalue',\n",
    "    2: 'tp_pvalue',\n",
    "    3: 'wind_speed_pvalue'\n",
    "    }, inplace=True)\n",
    "kpss_res_df.to_csv(home_dir + '01_data/processed/csv/Stationarity_test/hourly_KPSS_test_pvalues.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 519/519 [00:01<00:00, 306.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# Daily\n",
    "# Set home directory\n",
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/' # Office Mac\n",
    "home_dir = '/Users/yiyihe/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology/Research/Energy_resilience/' # Macbook Pro\n",
    "\n",
    "# Load station daily data\n",
    "daily_df = pd.read_csv(home_dir + \"01_data/processed/csv/daily_519station_13weather.csv\", index_col=0)\n",
    "\n",
    "# Generate a list of station ids\n",
    "station_lst = list(set(daily_df.station_id.unique()))\n",
    "\n",
    "# Predictors\n",
    "weather_variables = ['t2m', 'wind_speed', 'tp']\n",
    "aggregations = ['median', 'mean', 'max']\n",
    "predictor_lst = ['daily_blackout_minutes'] + [w + \"_\" + a for w in weather_variables for a in aggregations]\n",
    "\n",
    "kpss_res = {}\n",
    "for s_id in tqdm(station_lst):\n",
    "        # Extracting the dataframe for a/one station\n",
    "        station_df = daily_df[daily_df['station_id'] == s_id].sort_values(by='date')\n",
    "        kpss_res_lst = []\n",
    "        for var in predictor_lst:\n",
    "                # Extact time series\n",
    "                ts = station_df[var].values\n",
    "                kpss_res_lst.append(kpss(ts, regression=\"c\", nlags=\"auto\")[1]) # append pvalue\n",
    "\n",
    "        kpss_res[s_id] = kpss_res_lst\n",
    "        # break\n",
    "kpss_res_df = pd.DataFrame.from_dict(kpss_res, orient='index').reset_index()\n",
    "# Dictionary to update column names\n",
    "rename_dict = {'index': 'station_id'}\n",
    "rename_dict.update({i: var_name + '_pvalue' for i, var_name in enumerate(predictor_lst)})\n",
    "# Rename columns\n",
    "kpss_res_df.rename(columns=rename_dict, inplace=True)\n",
    "kpss_res_df.to_csv(home_dir + '01_data/processed/csv/Stationarity_test/daily_KPSS_test_pvalues.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differencing Timeseries (based on ADF test results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference_series(series, order=1):\n",
    "    \"\"\"\n",
    "    Perform differencing on a pandas Series.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    series : pd.Series\n",
    "        The time series to difference\n",
    "    order : int\n",
    "        Order of differencing (1 for first difference, 2 for second, etc.)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series\n",
    "        Differenced series\n",
    "    \"\"\"\n",
    "    differenced = series.copy()\n",
    "    \n",
    "    for i in range(order):\n",
    "        differenced = differenced.diff().dropna()\n",
    "    \n",
    "    return differenced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hourly differecing order=1 (that is not stationary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hourly\n",
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/' # Office Mac\n",
    "home_dir = '/Users/yiyihe/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology/Research/Energy_resilience/' # Macbook Pro\n",
    "\n",
    "hourly_df = pd.read_csv(home_dir + \"01_data/processed/csv/hourly_519station_3weather.csv\",\n",
    "                        index_col=0)\n",
    "\n",
    "# Generate a list of station ids\n",
    "station_lst = list(set(hourly_df.station_id.unique()))\n",
    "\n",
    "adf_res_df = pd.read_csv(home_dir + '01_data/processed/csv/ADF/hourly_ADF_test_pvalues.csv', index_col=0)\n",
    "\n",
    "var_names = ['pct_blackout', 't2m', 'tp', 'wind_speed']\n",
    "\n",
    "for s_id in station_lst:\n",
    "    # check ADF test result for station\n",
    "    ADF_result = adf_res_df[adf_res_df.station_id == s_id].values[0][1:]<0.05 # array of four boolean values\n",
    "    if ADF_result.sum() == 4: # all variables are stationary, no action needed\n",
    "        continue\n",
    "    else:\n",
    "        print(f'I am working on station id = {s_id}')\n",
    "        if ADF_result[0] == True: # The target timeseries is stationary, therefore we only need to drop the first observation\n",
    "            var_to_process = [var for var, keep in zip(var_names, ADF_result<0.05) if keep]\n",
    "            # original station df\n",
    "            station_df = hourly_df[hourly_df['station_id'] == s_id].sort_values(by='datetime')\n",
    "\n",
    "            station_pct_blackout_new = station_df.pct_blackout.values[1:] # new array of pct blackout values (eliminated the first entry)\n",
    "            station_new = pd.DataFrame(station_pct_blackout_new, columns=['pct_blackout_shift1'])\n",
    "            for var in var_to_process:\n",
    "                var_diff = difference_series(station_df[var]) # differencing order=1\n",
    "                if adfuller(var_diff)[1]>0.05: # is order 1 differencing does not work\n",
    "                    print(s_id, var)\n",
    "                else:\n",
    "                    # add var_diff as another column with correct name\n",
    "                    station_new[f'{var}_diff1'] = var_diff.values\n",
    "        else: # the target timeseries is not stationary, we need to difference it first\n",
    "            station_pct_blackout_diff = difference_series(station_df['pct_blackout']) # difference the pct blackout\n",
    "            station_new = pd.DataFrame(station_pct_blackout_diff.values, columns=['pct_blackout_diff1'])\n",
    "            if adfuller(station_pct_blackout_diff)[1]>0.05: # is order 1 differencing does not work\n",
    "                print(s_id, 'diff1 does not work for this target variable')\n",
    "            else:\n",
    "                # process predictor variables\n",
    "                var_to_process = [var for var, keep in zip(var_names, ADF_result<0.05) if keep][1:] # take away pct blackout\n",
    "                for var in var_to_process:\n",
    "                    var_diff = difference_series(station_df[var]) # differencing order=1\n",
    "                    if adfuller(var_diff)[1]>0.05: # is order 1 differencing does not work\n",
    "                        print(s_id, var)\n",
    "                    else:\n",
    "                    # add var_diff as another column with correct name\n",
    "                        station_new[f'{var}_diff1'] = var_diff.values\n",
    "\n",
    "            \n",
    "    station_new.to_csv(home_dir + f\"01_data/processed/csv/hourly_diff1/station_{s_id}_hourly_diff1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the results of differencing works or not, in other words, if the time series are indeed stationary now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/' # Office Mac\n",
    "home_dir = '/Users/yiyihe/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology/Research/Energy_resilience/' # Macbook Pro\n",
    "\n",
    "# Check if all the time series in the differenced dataset are stationary\n",
    "diff_dir = home_dir + \"01_data/processed/csv/hourly_diff1/\"\n",
    "for item in os.listdir(diff_dir):\n",
    "    if item[-3:] == 'csv':\n",
    "        df = pd.read_csv(os.path.join(diff_dir, item), index_col=0)\n",
    "        # Perform ADF test on all columns\n",
    "        for col in df.columns:\n",
    "            col_ts = df[col].values\n",
    "            adf_res = adfuller(col_ts)[1]\n",
    "            if adf_res >= 0.05:\n",
    "                # Print filename and column name if time series is not stationary\n",
    "                print(item, col) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After differencing (order=1), all the time series are now stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily differecing order=1 (that is not stationary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 519/519 [01:09<00:00,  7.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# Hourly\n",
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/' # Office Mac\n",
    "home_dir = '/Users/yiyihe/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology/Research/Energy_resilience/' # Macbook Pro\n",
    "\n",
    "daily_df = pd.read_csv(home_dir + \"01_data/processed/csv/daily_519station_13weather.csv\",\n",
    "                        index_col=0)\n",
    "\n",
    "# Generate a list of station ids\n",
    "station_lst = list(set(daily_df.station_id.unique()))\n",
    "\n",
    "adf_res_df = pd.read_csv(home_dir + '01_data/processed/csv/Stationarity_test/daily_ADF_test_pvalues.csv', index_col=0)\n",
    "\n",
    "# Variable names\n",
    "weather_variables = ['t2m', 'wind_speed', 'tp']\n",
    "aggregations = ['median', 'mean', 'max']\n",
    "var_names = ['daily_blackout_minutes'] + [w + \"_\" + a for w in weather_variables for a in aggregations]\n",
    "\n",
    "for s_id in tqdm(station_lst):\n",
    "    # check ADF test result for station\n",
    "    ADF_result = adf_res_df[adf_res_df.station_id == s_id].values[0][1:]<0.05 # array of four boolean values indicating stationarity for variables\n",
    "\n",
    "    if ADF_result.sum() == len(var_names): # all variables are stationary, no action needed\n",
    "        continue\n",
    "    else:\n",
    "        # print(f'I am working on station id = {s_id}')\n",
    "        if ADF_result[0] == True: # The target timeseries is stationary, therefore we only need to drop the first observation\n",
    "            var_to_process = [var for var, keep in zip(var_names, ADF_result<0.05) if keep] # list of variables that are not stationary\n",
    "            # original station df\n",
    "            station_df = daily_df[daily_df['station_id'] == s_id].sort_values(by='date')\n",
    "            \n",
    "            station_blackout_minutes_new = station_df.daily_blackout_minutes.values[1:] # new array of pct blackout values (eliminated the first entry)\n",
    "            station_new = pd.DataFrame(station_blackout_minutes_new, columns=['blackout_minutes_shift1'])\n",
    "\n",
    "            flag = 1\n",
    "            # First check if any var needs second order differencing\n",
    "            for var in var_to_process:\n",
    "                var_diff = difference_series(station_df[var]) # differencing order=1\n",
    "                if adfuller(var_diff)[1]>=0.05:\n",
    "                    flag = 2\n",
    "\n",
    "            if flag == 1:\n",
    "            # order 1 differencing is enough for this station\n",
    "                for var in var_to_process:\n",
    "                    var_diff = difference_series(station_df[var]) # differencing order=1\n",
    "                    # add var_diff as another column with correct name\n",
    "                    station_new[f'{var}_diff1'] = var_diff.values\n",
    "            elif flag == 2:\n",
    "                # need to shift blackout minutes by 2\n",
    "                station_blackout_minutes_new = station_df.daily_blackout_minutes.values[2:] # new array of pct blackout values (eliminated the first 2 entries)\n",
    "                station_new = pd.DataFrame(station_blackout_minutes_new, columns=['blackout_minutes_shift2']) # redefine station_new dataframe\n",
    "                for var in var_to_process:\n",
    "                    var_diff = difference_series(station_df[var]) # differencing order=1\n",
    "                    # check if this variable needs second differencing\n",
    "                    if adfuller(var_diff)[1]>=0.05:\n",
    "                        var_diff2 = difference_series(var_diff) # second diff\n",
    "                        station_new[f'{var}_diff2'] = var_diff2.values\n",
    "                    else: # this variable does not need second differencing\n",
    "                        var_diff = var_diff.values[1:]\n",
    "                        station_new[f'{var}_diff1'] = var_diff\n",
    "\n",
    "        else: # the target timeseries is not stationary, we need to difference it first\n",
    "            station_blackout_minutes_diff = difference_series(station_df['daily_blackout_minutes']) # difference the daily blackout minutes\n",
    "            station_new = pd.DataFrame(station_blackout_minutes_diff.values, columns=['daily_blackout_minutes_diff1'])\n",
    "            if adfuller(station_blackout_minutes_diff)[1]>=0.05: # is order 1 differencing does not work for blackout minutes\n",
    "                print(s_id, 'diff1 does not work for this target variable')\n",
    "            else:\n",
    "                # process predictor variables\n",
    "                var_to_process = [var for var, keep in zip(var_names, ADF_result<0.05) if keep][1:] # take away blackout minutes\n",
    "\n",
    "                flag = 1\n",
    "                # First check if any var needs second order differencing\n",
    "                for var in var_to_process:\n",
    "                    var_diff = difference_series(station_df[var]) # differencing order=1\n",
    "                    if adfuller(var_diff)[1]>=0.05:\n",
    "                        flag = 2\n",
    "\n",
    "                if flag == 1:\n",
    "                # order 1 differencing is enough for this station\n",
    "                    for var in var_to_process:\n",
    "                        var_diff = difference_series(station_df[var]) # differencing order=1\n",
    "                        # add var_diff as another column with correct name\n",
    "                        station_new[f'{var}_diff1'] = var_diff.values\n",
    "\n",
    "                elif flag == 2:\n",
    "                    # need to shift blackout minutes by 2\n",
    "                    station_blackout_minutes_new = station_df.daily_blackout_minutes.values[2:] # new array of pct blackout values (eliminated the first 2 entries)\n",
    "                    station_new = pd.DataFrame(station_blackout_minutes_new, columns=['blackout_minutes_shift2']) # redefine station_new dataframe\n",
    "                    for var in var_to_process:\n",
    "                        var_diff = difference_series(station_df[var]) # differencing order=1\n",
    "                        # check if this variable needs second differencing\n",
    "                        if adfuller(var_diff)[1]>=0.05:\n",
    "                            var_diff2 = difference_series(var_diff) # second diff\n",
    "                            station_new[f'{var}_diff2'] = var_diff2.values\n",
    "                        else: # this variable does not need second differencing\n",
    "                            var_diff = var_diff.values[1:]\n",
    "                            station_new[f'{var}_diff1'] = var_diff\n",
    "\n",
    "    station_new.to_csv(home_dir + f\"01_data/processed/csv/daily_diff1/station_{s_id}_daily_diff{flag}.csv\")\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the differencing works for all stations and all variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/' # Office Mac\n",
    "home_dir = '/Users/yiyihe/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology/Research/Energy_resilience/' # Macbook Pro\n",
    "\n",
    "# Check if all the time series in the differenced dataset are stationary\n",
    "diff_dir = home_dir + \"01_data/processed/csv/daily_diff1/\"\n",
    "for item in os.listdir(diff_dir):\n",
    "    if item[-3:] == 'csv':\n",
    "        df = pd.read_csv(os.path.join(diff_dir, item), index_col=0)\n",
    "        # Perform ADF test on all columns\n",
    "        for col in df.columns:\n",
    "            col_ts = df[col].values\n",
    "            adf_res = adfuller(col_ts)[1]\n",
    "            if adf_res >= 0.05:\n",
    "                # Print filename and column name if time series is not stationary\n",
    "                print(item, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After stationarity check, the following six stations will be removed from the daily analysis:\n",
    "- station id: 183, 423, 559, 315, 103, 62"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Granger Causality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GC with hourly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/'\n",
    "\n",
    "hourly_df = pd.read_csv(home_dir + \"01_data/processed/csv/hourly_519station_3weather.csv\",\n",
    "                        index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function that runs the Granger Causality test on hourly station data. \\\n",
    "For each station, run the Granger Causality test with the following set up:\\\n",
    " \\\n",
    "**Target variable**: \\\n",
    "'pct_blackout' \\\n",
    "**Predictor variables**: \n",
    "1. 't2m' (temperature) \n",
    "2. 'tp' (precipitation) \n",
    "3. 'wind_speed' (wind speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gc_hourly(maxlag, target, predictor, station_lst, hourly_df):\n",
    "    # Initiate an empty dictionary for storing test results\n",
    "    gc_dic = {}\n",
    "\n",
    "    # Iterate through all stations\n",
    "    for s_id in tqdm(station_lst):\n",
    "        # Extracting the dataframe for a/one station\n",
    "        station_df = hourly_df[hourly_df['station_id'] == s_id].sort_values(by='datetime')\n",
    "        try:\n",
    "            # Check if the number of observations available at the station is sufficient for the gc model\n",
    "            if len(station_df) <= maxlag + 2 + 1: # maxlag + num_variables (target, predictor) + 1\n",
    "                print(f\"Skipping Station {s_id} due to insufficient observations for maxlag={maxlag}\")\n",
    "                continue\n",
    "            else:\n",
    "                # Run granger causality test on station hourly data\n",
    "                test_result = grangercausalitytests(\n",
    "                    station_df[[target, predictor]], maxlag=maxlag, addconst=True, verbose=False)\n",
    "                # Save test values\n",
    "                F_test_p_values = [round(test_result[i+1][0]['ssr_ftest'][1],4) for i in range(maxlag)]\n",
    "                Chi_squared_p_values = [round(test_result[i+1][0]['ssr_chi2test'][1],4) for i in range(maxlag)]\n",
    "                p_values_min = np.min(F_test_p_values+Chi_squared_p_values)\n",
    "                # Key: station id, Value: list of 1. minimum F/Chi p values 2. F-test p values for all lags 3. Chi-square test p-values for all lags\n",
    "                gc_dic[s_id] = [p_values_min, F_test_p_values, Chi_squared_p_values]\n",
    "        except ValueError:\n",
    "            print(f\"Skipping Station {s_id} due to insufficient observations for maxlag={maxlag}\")\n",
    "    # Convert to dataframe        \n",
    "    gc_df = pd.DataFrame.from_dict(gc_dic, orient='index').reset_index()\n",
    "    gc_df.rename(columns={\n",
    "    'index':'station_id',\n",
    "    0:f'{predictor}_p-value_min',\n",
    "    1:f'{predictor}_f_p-value',\n",
    "    2:f'{predictor}_Chi_p-value'\n",
    "    }, inplace=True)\n",
    "    return gc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 519/519 [00:22<00:00, 22.99it/s]\n",
      "100%|██████████| 519/519 [00:22<00:00, 22.99it/s]\n",
      "100%|██████████| 519/519 [00:23<00:00, 22.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# Set home directory\n",
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/'\n",
    "# Read hourly data\n",
    "hourly_df = pd.read_csv(home_dir + \"01_data/processed/csv/hourly_519station_3weather.csv\",\n",
    "                        index_col=0)\n",
    "# Set target variable\n",
    "target = 'pct_blackout'\n",
    "# Set max lag for all variables\n",
    "maxlag = 3\n",
    "# Generate a list of station ids\n",
    "station_lst = list(set(hourly_df.station_id.unique()))\n",
    "# List of predictor variables\n",
    "predictor_lst = ['t2m', 'tp', 'wind_speed']\n",
    "\n",
    "# Initiate empty dictionary to store test output dataframes\n",
    "res_dic = {}\n",
    "# Loop through the list of predictors\n",
    "for predictor in predictor_lst:\n",
    "    # Run gc test\n",
    "    df_res = run_gc_hourly(maxlag, target, predictor, station_lst, hourly_df)\n",
    "    # Save output dataframe in dictionary key: predictor, value: dataframe with test values\n",
    "    res_dic[predictor] = df_res\n",
    "\n",
    "# Join resulting dataframes for all predictors together\n",
    "dfs = [res_dic[p] for p in predictor_lst]\n",
    "df_joined= ft.reduce(lambda left, right: pd.merge(left, right, on='station_id'), dfs)\n",
    "# Save output as new csv file\n",
    "df_joined.to_csv(home_dir + f\"01_data/processed/csv/GC/granger_hourly_max{maxlag}_pvalue.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GC on hourly data after first-order differencing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 329/329 [12:59<00:00,  2.37s/it]\n"
     ]
    }
   ],
   "source": [
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/' # Office Mac\n",
    "# home_dir = '/Users/yiyihe/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology/Research/Energy_resilience/' # Macbook Pro\n",
    "# Folder that contains the differenced data\n",
    "diff_dir = \"01_data/processed/csv/hourly_diff1\"\n",
    "# Set max lag\n",
    "maxlag = 3\n",
    "# Iterate through station files (first order differencing applied and all time series are stationary)\n",
    "for file in tqdm(os.listdir(home_dir + diff_dir)):\n",
    "    if file[-3:] == 'csv':\n",
    "        # Extract station id\n",
    "        station_id = int(file.split('_')[1])\n",
    "        df = pd.read_csv(os.path.join(home_dir + diff_dir, file), index_col=0)\n",
    "        # Initiate empty dictionary to store results\n",
    "        gc_res = {}\n",
    "        # Target time series: percent blackout\n",
    "        target_ts = df.iloc[:, 0].values\n",
    "        # Go though rest of the columns and perform GC test. Note, stations have different column numbers, depending on how many of the original are not stationary\n",
    "        try:\n",
    "            # Check if the number of observations available at the station is sufficient for the gc model\n",
    "            if len(df) <= maxlag + 2 + 1: # maxlag + num_variables (target, predictor) + 1\n",
    "                print(f\"Skipping Station {station_id} due to insufficient observations for maxlag={maxlag}\")\n",
    "                continue\n",
    "            else:\n",
    "                for col in list(df.columns)[1:]:\n",
    "                    col_name = col.split('_')[0]\n",
    "                    test_result = grangercausalitytests(\n",
    "                                                        df[[list(df.columns)[0], col]],\n",
    "                                                        maxlag=maxlag,\n",
    "                                                        addconst=True,\n",
    "                                                        verbose=False)\n",
    "                    F_test_p_values = [round(test_result[i+1][0]['ssr_ftest'][1],4) for i in range(maxlag)]\n",
    "                    Chi_squared_p_values = [round(test_result[i+1][0]['ssr_chi2test'][1],4) for i in range(maxlag)]\n",
    "                    p_values_min = np.min(F_test_p_values+Chi_squared_p_values)\n",
    "                    gc_res[col + '_p-value_min'] = p_values_min\n",
    "                    gc_res[col + '_f_p-value'] = F_test_p_values\n",
    "                    gc_res[col + '_Chi_p-value'] = Chi_squared_p_values\n",
    "                # Store results in a dataframe\n",
    "                gc_res_df = pd.DataFrame.from_dict(gc_res)\n",
    "                # Save station results to new csv\n",
    "                gc_res_df.to_csv(home_dir + f\"01_data/processed/csv/GC/hourly_station_diff1/hourly_station_maxlag_{maxlag}/station_{station_id}.csv\")\n",
    "        except ValueError:\n",
    "            print(f\"Skipping Station {station_id} due to insufficient observations for maxlag={maxlag}\")\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update original GC hourly results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/' # Office Mac\n",
    "# home_dir = '/Users/yiyihe/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology/Research/Energy_resilience/' # Macbook Pro\n",
    "\n",
    "maxlag = 3\n",
    "diff_res_dir = f\"01_data/processed/csv/GC/hourly_station_diff1/hourly_station_maxlag_{maxlag}\"\n",
    "\n",
    "# Load original gc result dataframe\n",
    "original_gc_hourly = pd.read_csv(home_dir+f'01_data/processed/csv/GC/hourly/granger_hourly_max{maxlag}_pvalue.csv',\n",
    "                                 index_col=0)\n",
    "\n",
    "original_gc_hourly_stations = original_gc_hourly.station_id.values\n",
    "\n",
    "for file in os.listdir(os.path.join(home_dir, diff_res_dir)):\n",
    "    # Extract station id\n",
    "    station_id = int(file.split('_')[1][:-4])\n",
    "    if station_id in original_gc_hourly_stations:\n",
    "        if file[-3:] == 'csv':\n",
    "            # Read dataframe\n",
    "            station_diff_gc = pd.read_csv(os.path.join(home_dir, diff_res_dir, file), index_col=0)\n",
    "            # Now we are ready to extract the outputs from this dataframe\n",
    "            var_list = list(set([i.split(\"_diff1\")[0] for i in station_diff_gc.columns.values]))\n",
    "            for var in var_list:\n",
    "                var_pvalue_min = station_diff_gc[f'{var}_diff1_p-value_min'].values[0]\n",
    "                var_f_pvalues = list(station_diff_gc[f'{var}_diff1_f_p-value'].values)\n",
    "                var_chi_pvalues = list(station_diff_gc[f'{var}_diff1_Chi_p-value'].values)\n",
    "                # Update original dataframe            \n",
    "                row_idx  = original_gc_hourly[original_gc_hourly['station_id'] == station_id].index\n",
    "                original_gc_hourly.at[row_idx.item(), f'{var}_p-value_min'] = var_pvalue_min.item()\n",
    "                original_gc_hourly.at[row_idx.item(), f'{var}_f_p-value'] = [float(x) for x in var_f_pvalues]\n",
    "                original_gc_hourly.at[row_idx.item(), f'{var}_Chi_p-value'] = [float(x) for x in var_chi_pvalues]\n",
    "    else:\n",
    "        continue\n",
    "original_gc_hourly.to_csv(home_dir + f'01_data/processed/csv/GC/hourly/granger_hourly_max{maxlag}_pvalue_diff_update.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>t2m_p-value_min</th>\n",
       "      <th>t2m_f_p-value</th>\n",
       "      <th>t2m_Chi_p-value</th>\n",
       "      <th>tp_p-value_min</th>\n",
       "      <th>tp_f_p-value</th>\n",
       "      <th>tp_Chi_p-value</th>\n",
       "      <th>wind_speed_p-value_min</th>\n",
       "      <th>wind_speed_f_p-value</th>\n",
       "      <th>wind_speed_Chi_p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0271</td>\n",
       "      <td>[0.0272, 0.0746, 0.1287, 0.1963, 0.3036, 0.380...</td>\n",
       "      <td>[0.0271, 0.0742, 0.1279, 0.195, 0.3017, 0.378,...</td>\n",
       "      <td>0.0985</td>\n",
       "      <td>[0.786, 0.1352, 0.0992, 0.1542, 0.2243, 0.2233...</td>\n",
       "      <td>[0.7859, 0.1347, 0.0985, 0.153, 0.2224, 0.221,...</td>\n",
       "      <td>0.2261</td>\n",
       "      <td>[0.3995, 0.3685, 0.5548, 0.5856, 0.5533, 0.294...</td>\n",
       "      <td>[0.3993, 0.3679, 0.5539, 0.5844, 0.5515, 0.292...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.0453</td>\n",
       "      <td>[0.9078, 0.4611, 0.6229, 0.4407, 0.162, 0.0653...</td>\n",
       "      <td>[0.9078, 0.461, 0.6227, 0.4404, 0.1616, 0.065,...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[0.0004, 0.0005, 0.0003, 0.0007, 0.0014, 0.000...</td>\n",
       "      <td>[0.0004, 0.0004, 0.0003, 0.0007, 0.0014, 0.000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[0.0737, 0.4865, 0.2713, 0.0608, 0.0, 0.0, 0.0...</td>\n",
       "      <td>[0.0736, 0.4862, 0.2707, 0.0604, 0.0, 0.0, 0.0...</td>\n",
       "      <td>0.3917</td>\n",
       "      <td>[0.8354, 0.9068, 0.644, 0.527, 0.55, 0.6509, 0...</td>\n",
       "      <td>[0.8353, 0.9067, 0.6436, 0.5262, 0.5489, 0.649...</td>\n",
       "      <td>0.0236</td>\n",
       "      <td>[0.0628, 0.2473, 0.4359, 0.0495, 0.065, 0.1052...</td>\n",
       "      <td>[0.0627, 0.2469, 0.4353, 0.0491, 0.0644, 0.104...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   station_id  t2m_p-value_min  \\\n",
       "0           1           0.0271   \n",
       "1           2           0.0000   \n",
       "2           3           0.0000   \n",
       "\n",
       "                                       t2m_f_p-value  \\\n",
       "0  [0.0272, 0.0746, 0.1287, 0.1963, 0.3036, 0.380...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  [0.0737, 0.4865, 0.2713, 0.0608, 0.0, 0.0, 0.0...   \n",
       "\n",
       "                                     t2m_Chi_p-value  tp_p-value_min  \\\n",
       "0  [0.0271, 0.0742, 0.1279, 0.195, 0.3017, 0.378,...          0.0985   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...          0.0453   \n",
       "2  [0.0736, 0.4862, 0.2707, 0.0604, 0.0, 0.0, 0.0...          0.3917   \n",
       "\n",
       "                                        tp_f_p-value  \\\n",
       "0  [0.786, 0.1352, 0.0992, 0.1542, 0.2243, 0.2233...   \n",
       "1  [0.9078, 0.4611, 0.6229, 0.4407, 0.162, 0.0653...   \n",
       "2  [0.8354, 0.9068, 0.644, 0.527, 0.55, 0.6509, 0...   \n",
       "\n",
       "                                      tp_Chi_p-value  wind_speed_p-value_min  \\\n",
       "0  [0.7859, 0.1347, 0.0985, 0.153, 0.2224, 0.221,...                  0.2261   \n",
       "1  [0.9078, 0.461, 0.6227, 0.4404, 0.1616, 0.065,...                  0.0000   \n",
       "2  [0.8353, 0.9067, 0.6436, 0.5262, 0.5489, 0.649...                  0.0236   \n",
       "\n",
       "                                wind_speed_f_p-value  \\\n",
       "0  [0.3995, 0.3685, 0.5548, 0.5856, 0.5533, 0.294...   \n",
       "1  [0.0004, 0.0005, 0.0003, 0.0007, 0.0014, 0.000...   \n",
       "2  [0.0628, 0.2473, 0.4359, 0.0495, 0.065, 0.1052...   \n",
       "\n",
       "                              wind_speed_Chi_p-value  \n",
       "0  [0.3993, 0.3679, 0.5539, 0.5844, 0.5515, 0.292...  \n",
       "1  [0.0004, 0.0004, 0.0003, 0.0007, 0.0014, 0.000...  \n",
       "2  [0.0627, 0.2469, 0.4353, 0.0491, 0.0644, 0.104...  "
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/' # Office Mac\n",
    "home_dir = '/Users/yiyihe/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology/Research/Energy_resilience/' # Macbook Pro\n",
    "\n",
    "maxlag = 24 # hourly lags: 24, 72, 120\n",
    "gc_hourly_original = pd.read_csv(home_dir + f\"01_data/processed/csv/GC/granger_hourly_max{maxlag}_pvalue.csv\",\n",
    "                                 index_col=0)\n",
    "gc_hourly_original.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GC with daily data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gc_daily(maxlag, target, predictor, station_lst, daily_df):\n",
    "    # Initiate an empty dictionary for storing test results\n",
    "    gc_dic = {}\n",
    "\n",
    "    # Iterate through all stations\n",
    "    for s_id in tqdm(station_lst):\n",
    "        # Extracting the dataframe for a/one station\n",
    "        station_df = daily_df[daily_df['station_id'] == s_id].sort_values(by='date') # Sort by date\n",
    "        try:\n",
    "            # Check if the number of observations available at the station is sufficient for the gc model\n",
    "            if len(station_df) <= maxlag + 2 + 1: # maxlag + num_variables (target, predictor) + 1\n",
    "                print(f\"Skipping Station {s_id} due to insufficient observations for maxlag={maxlag}\")\n",
    "                continue\n",
    "            else:\n",
    "                # Run granger causality test on station daily data\n",
    "                test_result = grangercausalitytests(\n",
    "                    station_df[[target, predictor]], maxlag=maxlag, addconst=True, verbose=False)\n",
    "                # Save test values\n",
    "                F_test_p_values = [round(test_result[i+1][0]['ssr_ftest'][1],4) for i in range(maxlag)]\n",
    "                Chi_squared_p_values = [round(test_result[i+1][0]['ssr_chi2test'][1],4) for i in range(maxlag)]\n",
    "                p_values_min = np.min(F_test_p_values+Chi_squared_p_values)\n",
    "                # Key: station id, Value: list of 1. minimum F/Chi p values 2. F-test p values for all lags 3. Chi-square test p-values for all lags\n",
    "                gc_dic[s_id] = [p_values_min, F_test_p_values, Chi_squared_p_values]\n",
    "        except ValueError:\n",
    "            print(f\"Skipping Station {s_id} due to insufficient observations for maxlag={maxlag}\")\n",
    "    # Convert to dataframe        \n",
    "    gc_df = pd.DataFrame.from_dict(gc_dic, orient='index').reset_index()\n",
    "    gc_df.rename(columns={\n",
    "    'index':'station_id',\n",
    "    0:f'{predictor}_p-value_min',\n",
    "    1:f'{predictor}_f_p-value',\n",
    "    2:f'{predictor}_Chi_p-value'\n",
    "    }, inplace=True)\n",
    "    return gc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set home directory\n",
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/'\n",
    "# Load station daily data\n",
    "daily_df = pd.read_csv(home_dir + \"01_data/processed/csv/daily_519station_13weather.csv\", index_col=0)\n",
    "\n",
    "# Target\n",
    "target = \"daily_blackout_minutes\"\n",
    "# Predictors\n",
    "weather_variables = ['t2m', 'wind_speed', 'tp']\n",
    "aggregations = ['median', 'mean', 'max']\n",
    "predictor_lst = [w + \"_\" + a for w in weather_variables for a in aggregations]\n",
    "\n",
    "# Create a list of unique station ids\n",
    "station_id_lst = list(set(daily_df.station_id.unique()))\n",
    "\n",
    "# Loop through multiple max lag values\n",
    "for maxlag in [3, 5, 7, 15, 30, 45, 60]:\n",
    "    # Initiate empty dictionary to store test output dataframes\n",
    "    res_dic = {}\n",
    "    # Loop through the list of predictors\n",
    "    for predictor in predictor_lst:\n",
    "        # Run gc test\n",
    "        df_res = run_gc_daily(maxlag, target, predictor, station_lst, daily_df)\n",
    "        # Save output dataframe in dictionary key: predictor, value: dataframe with test values\n",
    "        res_dic[predictor] = df_res\n",
    "\n",
    "    # Join resulting dataframes for all predictors together\n",
    "    dfs = [res_dic[p] for p in predictor_lst]\n",
    "    df_joined= ft.reduce(lambda left, right: pd.merge(left, right, on='station_id'), dfs)\n",
    "    # Save output as new csv file\n",
    "    df_joined.to_csv(home_dir + f\"01_data/processed/csv/granger_daily_max{maxlag}_pvalue.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GC on daily data after first-order differencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/' # Office Mac\n",
    "home_dir = '/Users/yiyihe/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology/Research/Energy_resilience/' # Macbook Pro\n",
    "# Folder that contains the differenced data\n",
    "diff_dir = \"01_data/processed/csv/daily_diff1\"\n",
    "# Set max lag\n",
    "maxlag = 60   # 3, 5, 7, 15, 30, 45, 60\n",
    "# Iterate through station files (first order differencing applied and all time series are stationary)\n",
    "for file in tqdm(os.listdir(home_dir + diff_dir)):\n",
    "    if file[-3:] == 'csv':\n",
    "        # Extract station id\n",
    "        station_id = int(file.split('_')[1])\n",
    "        df = pd.read_csv(os.path.join(home_dir + diff_dir, file), index_col=0)\n",
    "        # Initiate empty dictionary to store results\n",
    "        gc_res = {}\n",
    "        # Target time series: percent blackout\n",
    "        target_ts = df.iloc[:, 0].values\n",
    "        # Go though rest of the columns and perform GC test. Note, stations have different column numbers, depending on how many of the original are not stationary\n",
    "        try:\n",
    "            # Check if the number of observations available at the station is sufficient for the gc model\n",
    "            if len(df) <= maxlag + 2 + 1: # maxlag + num_variables (target, predictor) + 1\n",
    "                print(f\"Skipping Station {station_id} due to insufficient observations for maxlag={maxlag}\")\n",
    "                continue\n",
    "            else:\n",
    "                for col in list(df.columns)[1:]:\n",
    "                    col_name = col.split('_diff')[0]\n",
    "                    test_result = grangercausalitytests(\n",
    "                                                        df[[list(df.columns)[0], col]],\n",
    "                                                        maxlag=maxlag,\n",
    "                                                        addconst=True,\n",
    "                                                        verbose=False\n",
    "                                                        )\n",
    "                    F_test_p_values = [round(test_result[i+1][0]['ssr_ftest'][1],4) for i in range(maxlag)] # this collects the p values for all lags\n",
    "                    Chi_squared_p_values = [round(test_result[i+1][0]['ssr_chi2test'][1],4) for i in range(maxlag)] # similar here\n",
    "                    p_values_min = np.min(F_test_p_values+Chi_squared_p_values)\n",
    "                    gc_res[col + '_p-value_min'] = p_values_min\n",
    "                    gc_res[col + '_f_p-value'] = F_test_p_values\n",
    "                    gc_res[col + '_Chi_p-value'] = Chi_squared_p_values\n",
    "                # Store results in a dataframe\n",
    "                gc_res_df = pd.DataFrame.from_dict(gc_res)\n",
    "                # Save station results to new csv\n",
    "                gc_res_df.to_csv(home_dir + f\"01_data/processed/csv/GC/daily_station_diff1/daily_station_maxlag_{maxlag}/station_{station_id}.csv\")\n",
    "        except ValueError:\n",
    "            print(f\"Skipping Station {station_id} due to insufficient observations for maxlag={maxlag}\")\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update original GC daily results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:02<00:00, 141.66it/s]\n"
     ]
    }
   ],
   "source": [
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/' # Office Mac\n",
    "home_dir = '/Users/yiyihe/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology/Research/Energy_resilience/' # Macbook Pro\n",
    "\n",
    "maxlag = 60 # 3, 5, 7, 15, 30, 45, 60\n",
    "diff_res_dir = f\"01_data/processed/csv/GC/daily_station_diff1/daily_station_maxlag_{maxlag}\"\n",
    "\n",
    "# Load original gc result dataframe\n",
    "original_gc_daily = pd.read_csv(home_dir+f'01_data/processed/csv/GC/daily/granger_daily_max{maxlag}_pvalue.csv',\n",
    "                                 index_col=0)\n",
    "\n",
    "original_gc_daily_stations = original_gc_daily.station_id.values\n",
    "\n",
    "# Iterate through files in differenced daily time series folder\n",
    "for file in tqdm(os.listdir(os.path.join(home_dir, diff_res_dir))):\n",
    "    # Extract station id\n",
    "    station_id = int(file.split('_')[1][:-4])\n",
    "    if station_id in original_gc_daily_stations:\n",
    "        if file[-3:] == 'csv':\n",
    "            # Read dataframe\n",
    "            station_diff_gc = pd.read_csv(os.path.join(home_dir, diff_res_dir, file), index_col=0)\n",
    "            # Now we are ready to extract the outputs from this dataframe\n",
    "            var_list = list(set([i.split(\"_diff\")[0] for i in station_diff_gc.columns.values]))\n",
    "            for var in var_list:\n",
    "                var_pvalue_min = station_diff_gc.filter(regex=rf'{var}_diff\\d_p-value_min').values.flatten()[0]\n",
    "                var_f_pvalues = list(station_diff_gc.filter(regex=rf'{var}_diff\\d_f_p-value').values.flatten())\n",
    "                var_chi_pvalues = list(station_diff_gc.filter(regex=rf'{var}_diff\\d_Chi_p-value').values.flatten())\n",
    "                # Update original dataframe            \n",
    "                row_idx  = original_gc_daily[original_gc_daily['station_id'] == station_id].index\n",
    "                original_gc_daily.at[row_idx.item(), f'{var}_p-value_min'] = var_pvalue_min.item()\n",
    "                original_gc_daily.at[row_idx.item(), f'{var}_f_p-value'] = [float(x) for x in var_f_pvalues]\n",
    "                original_gc_daily.at[row_idx.item(), f'{var}_Chi_p-value'] = [float(x) for x in var_chi_pvalues]\n",
    "    else:\n",
    "        continue\n",
    "original_gc_daily.to_csv(home_dir + f'01_data/processed/csv/GC/daily/granger_daily_max{maxlag}_pvalue_diff_update.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize GC results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each maxlag clean the gc results dataframe\n",
    "# select only min p-values\n",
    "home_dir = '/Users/yiyihe/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology/Research/Energy_resilience/' # Macbook Pro\n",
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/' # Office Mac\n",
    "\n",
    "maxlags = [3, 24, 72, 120]\n",
    "\n",
    "var_lst = ['t2m', 'tp', 'wind_speed']\n",
    "cols_select = [var+'_p-value_min' for var in var_lst]\n",
    "\n",
    "for maxlag in maxlags:\n",
    "    # Hourly gc results (updated with differenced time series)\n",
    "    hourly_gc_res = pd.read_csv(home_dir + f'01_data/processed/csv/GC/hourly/granger_hourly_max{maxlag}_pvalue_diff_update.csv',\n",
    "                                index_col=0)\n",
    "    hourly_gc_select = hourly_gc_res[['station_id'] + cols_select]\n",
    "    hourly_gc_select.to_csv(home_dir+f'01_data/processed/csv/GC/hourly/granger_hourly_max{maxlag}_pvalue_diff_update_select.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing GC results for maxlag=3\n",
      "Number of stations included in the analysis: 519\n",
      "412 out of 519 stations, t2m Granger Cause blackout percent.\n",
      "216 out of 519 stations, tp Granger Cause blackout percent.\n",
      "300 out of 519 stations, wind_speed Granger Cause blackout percent.\n",
      ".....................\n",
      "Summarizing GC results for maxlag=24\n",
      "Number of stations included in the analysis: 517\n",
      "485 out of 517 stations, t2m Granger Cause blackout percent.\n",
      "406 out of 517 stations, tp Granger Cause blackout percent.\n",
      "440 out of 517 stations, wind_speed Granger Cause blackout percent.\n",
      ".....................\n",
      "Summarizing GC results for maxlag=72\n",
      "Number of stations included in the analysis: 517\n",
      "498 out of 517 stations, t2m Granger Cause blackout percent.\n",
      "438 out of 517 stations, tp Granger Cause blackout percent.\n",
      "465 out of 517 stations, wind_speed Granger Cause blackout percent.\n",
      ".....................\n",
      "Summarizing GC results for maxlag=120\n",
      "Number of stations included in the analysis: 512\n",
      "499 out of 512 stations, t2m Granger Cause blackout percent.\n",
      "454 out of 512 stations, tp Granger Cause blackout percent.\n",
      "470 out of 512 stations, wind_speed Granger Cause blackout percent.\n",
      ".....................\n"
     ]
    }
   ],
   "source": [
    "maxlags = [3, 24, 72, 120]\n",
    "var_lst = ['t2m', 'tp', 'wind_speed']\n",
    "\n",
    "for maxlag in maxlags:\n",
    "    gc_hourly_select_df = pd.read_csv(home_dir+f'01_data/processed/csv/GC/hourly/granger_hourly_max{maxlag}_pvalue_diff_update_select.csv',\n",
    "                                    index_col=0)\n",
    "    number_of_stations = len(set(gc_hourly_select_df.station_id.values))\n",
    "    print(f\"Summarizing GC results for maxlag={maxlag}\")\n",
    "    print(f\"Number of stations included in the analysis: {number_of_stations}\")\n",
    "    for var in var_lst:\n",
    "        x = sum((gc_hourly_select_df[f'{var}_p-value_min']<0.05)*1)\n",
    "        print(f\"{x} out of {number_of_stations} stations, {var} Granger Cause blackout percent.\")\n",
    "    print(\".....................\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each maxlag clean the gc results dataframe\n",
    "# select only min p-values\n",
    "home_dir = '/Users/yiyihe/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology/Research/Energy_resilience/' # Macbook Pro\n",
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/' # Office Mac\n",
    "\n",
    "maxlags = [3, 5, 7, 15, 30, 45, 60]\n",
    "\n",
    "var_lst = ['t2m_max', 'tp_max', 'wind_speed_max']\n",
    "cols_select = [var+'_p-value_min' for var in var_lst]\n",
    "\n",
    "for maxlag in maxlags:\n",
    "    # Hourly gc results (updated with differenced time series)\n",
    "    daily_gc_res = pd.read_csv(home_dir + f'01_data/processed/csv/GC/daily/granger_daily_max{maxlag}_pvalue_diff_update.csv',\n",
    "                                index_col=0)\n",
    "    daily_gc_select = daily_gc_res[['station_id'] + cols_select]\n",
    "    daily_gc_select.to_csv(home_dir+f'01_data/processed/csv/GC/daily/granger_daily_max{maxlag}_pvalue_diff_update_select.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing GC results for maxlag=5\n",
      "Number of stations included in the analysis: 512\n",
      "132 out of 512 stations, t2m_max Granger Cause blackout percent.\n",
      "196 out of 512 stations, tp_max Granger Cause blackout percent.\n",
      "111 out of 512 stations, wind_speed_max Granger Cause blackout percent.\n",
      ".....................\n",
      "Summarizing GC results for maxlag=15\n",
      "Number of stations included in the analysis: 500\n",
      "201 out of 500 stations, t2m_max Granger Cause blackout percent.\n",
      "250 out of 500 stations, tp_max Granger Cause blackout percent.\n",
      "177 out of 500 stations, wind_speed_max Granger Cause blackout percent.\n",
      ".....................\n",
      "Summarizing GC results for maxlag=30\n",
      "Number of stations included in the analysis: 472\n",
      "255 out of 472 stations, t2m_max Granger Cause blackout percent.\n",
      "290 out of 472 stations, tp_max Granger Cause blackout percent.\n",
      "246 out of 472 stations, wind_speed_max Granger Cause blackout percent.\n",
      ".....................\n",
      "Summarizing GC results for maxlag=45\n",
      "Number of stations included in the analysis: 436\n",
      "296 out of 436 stations, t2m_max Granger Cause blackout percent.\n",
      "290 out of 436 stations, tp_max Granger Cause blackout percent.\n",
      "284 out of 436 stations, wind_speed_max Granger Cause blackout percent.\n",
      ".....................\n",
      "Summarizing GC results for maxlag=60\n",
      "Number of stations included in the analysis: 414\n",
      "312 out of 414 stations, t2m_max Granger Cause blackout percent.\n",
      "308 out of 414 stations, tp_max Granger Cause blackout percent.\n",
      "306 out of 414 stations, wind_speed_max Granger Cause blackout percent.\n",
      ".....................\n"
     ]
    }
   ],
   "source": [
    "maxlags = [5, 15, 30, 45, 60]\n",
    "var_lst = ['t2m_max', 'tp_max', 'wind_speed_max']\n",
    "\n",
    "for maxlag in maxlags:\n",
    "    gc_daily_select_df = pd.read_csv(home_dir+f'01_data/processed/csv/GC/daily/granger_daily_max{maxlag}_pvalue_diff_update_select.csv',\n",
    "                                    index_col=0)\n",
    "    number_of_stations = len(set(gc_daily_select_df.station_id.values))\n",
    "    print(f\"Summarizing GC results for maxlag={maxlag}\")\n",
    "    print(f\"Number of stations included in the analysis: {number_of_stations}\")\n",
    "    for var in var_lst:\n",
    "        x = sum((gc_daily_select_df[f'{var}_p-value_min']<0.05)*1)\n",
    "        print(f\"{x} out of {number_of_stations} stations, {var} Granger Cause blackout percent.\")\n",
    "    print(\".....................\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARDL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation between daily weather aggregate variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_8310c_row0_col0, #T_8310c_row1_col1, #T_8310c_row2_col2, #T_8310c_row3_col3, #T_8310c_row4_col4, #T_8310c_row5_col5, #T_8310c_row6_col6, #T_8310c_row7_col7 {\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8310c_row0_col1, #T_8310c_row0_col4, #T_8310c_row0_col5, #T_8310c_row3_col6, #T_8310c_row3_col7, #T_8310c_row4_col0, #T_8310c_row6_col2, #T_8310c_row6_col3, #T_8310c_row7_col3 {\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8310c_row0_col2, #T_8310c_row1_col0 {\n",
       "  background-color: #4055c8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8310c_row0_col3, #T_8310c_row2_col7 {\n",
       "  background-color: #6f92f3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8310c_row0_col6 {\n",
       "  background-color: #7295f4;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8310c_row0_col7, #T_8310c_row6_col1 {\n",
       "  background-color: #7396f5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8310c_row1_col2 {\n",
       "  background-color: #d24b40;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8310c_row1_col3 {\n",
       "  background-color: #f39475;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_8310c_row1_col4 {\n",
       "  background-color: #93b5fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_8310c_row1_col5 {\n",
       "  background-color: #b1cbfc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_8310c_row1_col6 {\n",
       "  background-color: #a2c1ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_8310c_row1_col7 {\n",
       "  background-color: #a9c6fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_8310c_row2_col0 {\n",
       "  background-color: #4358cb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8310c_row2_col1 {\n",
       "  background-color: #d44e41;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8310c_row2_col3 {\n",
       "  background-color: #cc403a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8310c_row2_col4 {\n",
       "  background-color: #779af7;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8310c_row2_col5 {\n",
       "  background-color: #9ebeff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_8310c_row2_col6 {\n",
       "  background-color: #6a8bef;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8310c_row3_col0 {\n",
       "  background-color: #4257c9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8310c_row3_col1 {\n",
       "  background-color: #f6a586;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_8310c_row3_col2 {\n",
       "  background-color: #d0473d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8310c_row3_col4 {\n",
       "  background-color: #5a78e4;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8310c_row3_col5, #T_8310c_row4_col3, #T_8310c_row4_col7 {\n",
       "  background-color: #86a9fc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8310c_row4_col1 {\n",
       "  background-color: #8db0fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_8310c_row4_col2 {\n",
       "  background-color: #7597f6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8310c_row4_col5, #T_8310c_row5_col4 {\n",
       "  background-color: #de614d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8310c_row4_col6 {\n",
       "  background-color: #84a7fc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8310c_row5_col0 {\n",
       "  background-color: #3c4ec2;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8310c_row5_col1 {\n",
       "  background-color: #adc9fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_8310c_row5_col2 {\n",
       "  background-color: #9dbdff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_8310c_row5_col3 {\n",
       "  background-color: #aec9fc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_8310c_row5_col6, #T_8310c_row7_col1 {\n",
       "  background-color: #7b9ff9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8310c_row5_col7 {\n",
       "  background-color: #82a6fb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8310c_row6_col0 {\n",
       "  background-color: #455cce;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8310c_row6_col4 {\n",
       "  background-color: #5875e1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8310c_row6_col5 {\n",
       "  background-color: #4e68d8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8310c_row6_col7, #T_8310c_row7_col6 {\n",
       "  background-color: #e67259;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8310c_row7_col0 {\n",
       "  background-color: #485fd1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8310c_row7_col2 {\n",
       "  background-color: #3f53c6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8310c_row7_col4 {\n",
       "  background-color: #5b7ae5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_8310c_row7_col5 {\n",
       "  background-color: #5470de;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_8310c\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_8310c_level0_col0\" class=\"col_heading level0 col0\" >daily_blackout_minutes</th>\n",
       "      <th id=\"T_8310c_level0_col1\" class=\"col_heading level0 col1\" >t2m_min</th>\n",
       "      <th id=\"T_8310c_level0_col2\" class=\"col_heading level0 col2\" >t2m_median</th>\n",
       "      <th id=\"T_8310c_level0_col3\" class=\"col_heading level0 col3\" >t2m_max</th>\n",
       "      <th id=\"T_8310c_level0_col4\" class=\"col_heading level0 col4\" >wind_speed_median</th>\n",
       "      <th id=\"T_8310c_level0_col5\" class=\"col_heading level0 col5\" >wind_speed_max</th>\n",
       "      <th id=\"T_8310c_level0_col6\" class=\"col_heading level0 col6\" >tp_median</th>\n",
       "      <th id=\"T_8310c_level0_col7\" class=\"col_heading level0 col7\" >tp_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_8310c_level0_row0\" class=\"row_heading level0 row0\" >daily_blackout_minutes</th>\n",
       "      <td id=\"T_8310c_row0_col0\" class=\"data row0 col0\" >1.000000</td>\n",
       "      <td id=\"T_8310c_row0_col1\" class=\"data row0 col1\" >0.020774</td>\n",
       "      <td id=\"T_8310c_row0_col2\" class=\"data row0 col2\" >0.027144</td>\n",
       "      <td id=\"T_8310c_row0_col3\" class=\"data row0 col3\" >0.024727</td>\n",
       "      <td id=\"T_8310c_row0_col4\" class=\"data row0 col4\" >-0.002525</td>\n",
       "      <td id=\"T_8310c_row0_col5\" class=\"data row0 col5\" >0.004347</td>\n",
       "      <td id=\"T_8310c_row0_col6\" class=\"data row0 col6\" >0.034892</td>\n",
       "      <td id=\"T_8310c_row0_col7\" class=\"data row0 col7\" >0.040650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8310c_level0_row1\" class=\"row_heading level0 row1\" >t2m_min</th>\n",
       "      <td id=\"T_8310c_row1_col0\" class=\"data row1 col0\" >0.020774</td>\n",
       "      <td id=\"T_8310c_row1_col1\" class=\"data row1 col1\" >1.000000</td>\n",
       "      <td id=\"T_8310c_row1_col2\" class=\"data row1 col2\" >0.911219</td>\n",
       "      <td id=\"T_8310c_row1_col3\" class=\"data row1 col3\" >0.725396</td>\n",
       "      <td id=\"T_8310c_row1_col4\" class=\"data row1 col4\" >0.265783</td>\n",
       "      <td id=\"T_8310c_row1_col5\" class=\"data row1 col5\" >0.357374</td>\n",
       "      <td id=\"T_8310c_row1_col6\" class=\"data row1 col6\" >0.192918</td>\n",
       "      <td id=\"T_8310c_row1_col7\" class=\"data row1 col7\" >0.218318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8310c_level0_row2\" class=\"row_heading level0 row2\" >t2m_median</th>\n",
       "      <td id=\"T_8310c_row2_col0\" class=\"data row2 col0\" >0.027144</td>\n",
       "      <td id=\"T_8310c_row2_col1\" class=\"data row2 col1\" >0.911219</td>\n",
       "      <td id=\"T_8310c_row2_col2\" class=\"data row2 col2\" >1.000000</td>\n",
       "      <td id=\"T_8310c_row2_col3\" class=\"data row2 col3\" >0.921124</td>\n",
       "      <td id=\"T_8310c_row2_col4\" class=\"data row2 col4\" >0.187497</td>\n",
       "      <td id=\"T_8310c_row2_col5\" class=\"data row2 col5\" >0.300769</td>\n",
       "      <td id=\"T_8310c_row2_col6\" class=\"data row2 col6\" >0.007608</td>\n",
       "      <td id=\"T_8310c_row2_col7\" class=\"data row2 col7\" >0.023848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8310c_level0_row3\" class=\"row_heading level0 row3\" >t2m_max</th>\n",
       "      <td id=\"T_8310c_row3_col0\" class=\"data row3 col0\" >0.024727</td>\n",
       "      <td id=\"T_8310c_row3_col1\" class=\"data row3 col1\" >0.725396</td>\n",
       "      <td id=\"T_8310c_row3_col2\" class=\"data row3 col2\" >0.921124</td>\n",
       "      <td id=\"T_8310c_row3_col3\" class=\"data row3 col3\" >1.000000</td>\n",
       "      <td id=\"T_8310c_row3_col4\" class=\"data row3 col4\" >0.102492</td>\n",
       "      <td id=\"T_8310c_row3_col5\" class=\"data row3 col5\" >0.235172</td>\n",
       "      <td id=\"T_8310c_row3_col6\" class=\"data row3 col6\" >-0.170036</td>\n",
       "      <td id=\"T_8310c_row3_col7\" class=\"data row3 col7\" >-0.169363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8310c_level0_row4\" class=\"row_heading level0 row4\" >wind_speed_median</th>\n",
       "      <td id=\"T_8310c_row4_col0\" class=\"data row4 col0\" >-0.002525</td>\n",
       "      <td id=\"T_8310c_row4_col1\" class=\"data row4 col1\" >0.265783</td>\n",
       "      <td id=\"T_8310c_row4_col2\" class=\"data row4 col2\" >0.187497</td>\n",
       "      <td id=\"T_8310c_row4_col3\" class=\"data row4 col3\" >0.102492</td>\n",
       "      <td id=\"T_8310c_row4_col4\" class=\"data row4 col4\" >1.000000</td>\n",
       "      <td id=\"T_8310c_row4_col5\" class=\"data row4 col5\" >0.872144</td>\n",
       "      <td id=\"T_8310c_row4_col6\" class=\"data row4 col6\" >0.091777</td>\n",
       "      <td id=\"T_8310c_row4_col7\" class=\"data row4 col7\" >0.104201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8310c_level0_row5\" class=\"row_heading level0 row5\" >wind_speed_max</th>\n",
       "      <td id=\"T_8310c_row5_col0\" class=\"data row5 col0\" >0.004347</td>\n",
       "      <td id=\"T_8310c_row5_col1\" class=\"data row5 col1\" >0.357374</td>\n",
       "      <td id=\"T_8310c_row5_col2\" class=\"data row5 col2\" >0.300769</td>\n",
       "      <td id=\"T_8310c_row5_col3\" class=\"data row5 col3\" >0.235172</td>\n",
       "      <td id=\"T_8310c_row5_col4\" class=\"data row5 col4\" >0.872144</td>\n",
       "      <td id=\"T_8310c_row5_col5\" class=\"data row5 col5\" >1.000000</td>\n",
       "      <td id=\"T_8310c_row5_col6\" class=\"data row5 col6\" >0.067437</td>\n",
       "      <td id=\"T_8310c_row5_col7\" class=\"data row5 col7\" >0.087814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8310c_level0_row6\" class=\"row_heading level0 row6\" >tp_median</th>\n",
       "      <td id=\"T_8310c_row6_col0\" class=\"data row6 col0\" >0.034892</td>\n",
       "      <td id=\"T_8310c_row6_col1\" class=\"data row6 col1\" >0.192918</td>\n",
       "      <td id=\"T_8310c_row6_col2\" class=\"data row6 col2\" >0.007608</td>\n",
       "      <td id=\"T_8310c_row6_col3\" class=\"data row6 col3\" >-0.170036</td>\n",
       "      <td id=\"T_8310c_row6_col4\" class=\"data row6 col4\" >0.091777</td>\n",
       "      <td id=\"T_8310c_row6_col5\" class=\"data row6 col5\" >0.067437</td>\n",
       "      <td id=\"T_8310c_row6_col6\" class=\"data row6 col6\" >1.000000</td>\n",
       "      <td id=\"T_8310c_row6_col7\" class=\"data row6 col7\" >0.812423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8310c_level0_row7\" class=\"row_heading level0 row7\" >tp_max</th>\n",
       "      <td id=\"T_8310c_row7_col0\" class=\"data row7 col0\" >0.040650</td>\n",
       "      <td id=\"T_8310c_row7_col1\" class=\"data row7 col1\" >0.218318</td>\n",
       "      <td id=\"T_8310c_row7_col2\" class=\"data row7 col2\" >0.023848</td>\n",
       "      <td id=\"T_8310c_row7_col3\" class=\"data row7 col3\" >-0.169363</td>\n",
       "      <td id=\"T_8310c_row7_col4\" class=\"data row7 col4\" >0.104201</td>\n",
       "      <td id=\"T_8310c_row7_col5\" class=\"data row7 col5\" >0.087814</td>\n",
       "      <td id=\"T_8310c_row7_col6\" class=\"data row7 col6\" >0.812423</td>\n",
       "      <td id=\"T_8310c_row7_col7\" class=\"data row7 col7\" >1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1080697c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set home directory\n",
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/'\n",
    "# Load station daily data\n",
    "daily_df = pd.read_csv(home_dir + \"01_data/processed/csv/daily_519station_13weather.csv\", index_col=0)\n",
    "\n",
    "# Correlation\n",
    "correlation_matrix = daily_df[['daily_blackout_minutes',\n",
    "                                        't2m_min',\n",
    "                                        't2m_median',\n",
    "                                        't2m_max',\n",
    "                                        'wind_speed_median',\n",
    "                                        'wind_speed_max',\n",
    "                                        'tp_median',\n",
    "                                        'tp_max']].corr()\n",
    "correlation_matrix.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the aggregate of one particular weather variable is highly correlated with one another. We decide to use the maximum for each weather variable: t2m_max, tp_max, and wind_speed_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_424fb_row0_col0, #T_424fb_row1_col1, #T_424fb_row2_col2, #T_424fb_row3_col3 {\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_424fb_row0_col1 {\n",
       "  background-color: #6f92f3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_424fb_row0_col2, #T_424fb_row1_col3, #T_424fb_row2_col0, #T_424fb_row3_col1 {\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_424fb_row0_col3 {\n",
       "  background-color: #7396f5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_424fb_row1_col0 {\n",
       "  background-color: #4055c8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_424fb_row1_col2 {\n",
       "  background-color: #86a9fc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_424fb_row2_col1 {\n",
       "  background-color: #aec9fc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_424fb_row2_col3 {\n",
       "  background-color: #82a6fb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_424fb_row3_col0 {\n",
       "  background-color: #455cce;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_424fb_row3_col2 {\n",
       "  background-color: #5470de;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_424fb\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_424fb_level0_col0\" class=\"col_heading level0 col0\" >daily_blackout_minutes</th>\n",
       "      <th id=\"T_424fb_level0_col1\" class=\"col_heading level0 col1\" >t2m_max</th>\n",
       "      <th id=\"T_424fb_level0_col2\" class=\"col_heading level0 col2\" >wind_speed_max</th>\n",
       "      <th id=\"T_424fb_level0_col3\" class=\"col_heading level0 col3\" >tp_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_424fb_level0_row0\" class=\"row_heading level0 row0\" >daily_blackout_minutes</th>\n",
       "      <td id=\"T_424fb_row0_col0\" class=\"data row0 col0\" >1.000000</td>\n",
       "      <td id=\"T_424fb_row0_col1\" class=\"data row0 col1\" >0.024727</td>\n",
       "      <td id=\"T_424fb_row0_col2\" class=\"data row0 col2\" >0.004347</td>\n",
       "      <td id=\"T_424fb_row0_col3\" class=\"data row0 col3\" >0.040650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_424fb_level0_row1\" class=\"row_heading level0 row1\" >t2m_max</th>\n",
       "      <td id=\"T_424fb_row1_col0\" class=\"data row1 col0\" >0.024727</td>\n",
       "      <td id=\"T_424fb_row1_col1\" class=\"data row1 col1\" >1.000000</td>\n",
       "      <td id=\"T_424fb_row1_col2\" class=\"data row1 col2\" >0.235172</td>\n",
       "      <td id=\"T_424fb_row1_col3\" class=\"data row1 col3\" >-0.169363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_424fb_level0_row2\" class=\"row_heading level0 row2\" >wind_speed_max</th>\n",
       "      <td id=\"T_424fb_row2_col0\" class=\"data row2 col0\" >0.004347</td>\n",
       "      <td id=\"T_424fb_row2_col1\" class=\"data row2 col1\" >0.235172</td>\n",
       "      <td id=\"T_424fb_row2_col2\" class=\"data row2 col2\" >1.000000</td>\n",
       "      <td id=\"T_424fb_row2_col3\" class=\"data row2 col3\" >0.087814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_424fb_level0_row3\" class=\"row_heading level0 row3\" >tp_max</th>\n",
       "      <td id=\"T_424fb_row3_col0\" class=\"data row3 col0\" >0.040650</td>\n",
       "      <td id=\"T_424fb_row3_col1\" class=\"data row3 col1\" >-0.169363</td>\n",
       "      <td id=\"T_424fb_row3_col2\" class=\"data row3 col2\" >0.087814</td>\n",
       "      <td id=\"T_424fb_row3_col3\" class=\"data row3 col3\" >1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x14dc54c40>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation_matrix = daily_df[['daily_blackout_minutes',\n",
    "                                        't2m_max',\n",
    "                                        'wind_speed_max',\n",
    "                                        'tp_max']].corr()\n",
    "correlation_matrix.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show low correlation between the max of three weather variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ardl_results(res, station_id, sel_res=None):\n",
    "    \"\"\"\n",
    "    Extract key results from fitted ARDL model\n",
    "    \"\"\"\n",
    "    # Basic model info\n",
    "    results_dict = {\n",
    "        'station_id': station_id,\n",
    "        'model_type': 'ARDL',\n",
    "        'endog_var': res.model.endog_names,\n",
    "        'exog_var': res.model.exog_names,\n",
    "        'n_obs': res.nobs,\n",
    "        'al_lags': str(sel_res.model.ar_lags),\n",
    "        'dl_lags': str(sel_res.model.dl_lags),\n",
    "        'log_likelihood': res.llf,\n",
    "        'aic': res.aic,\n",
    "        'bic': res.bic,\n",
    "        'hqic': res.hqic,\n",
    "        'fitted_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "    \n",
    "    # Extract coefficients, std errors, t-stats, p-values\n",
    "    params = res.params\n",
    "    std_errors = res.bse\n",
    "    tvalues = res.tvalues\n",
    "    pvalues = res.pvalues\n",
    "    conf_int = res.conf_int()\n",
    "    \n",
    "    # Store coefficient information\n",
    "    for i, param_name in enumerate(params.index):\n",
    "        results_dict[f'coef_{param_name}'] = params.iloc[i]\n",
    "        results_dict[f'se_{param_name}'] = std_errors.iloc[i]\n",
    "        results_dict[f'tstat_{param_name}'] = tvalues.iloc[i]\n",
    "        results_dict[f'pvalue_{param_name}'] = pvalues.iloc[i]\n",
    "        results_dict[f'ci_lower_{param_name}'] = conf_int.iloc[i, 0]\n",
    "        results_dict[f'ci_upper_{param_name}'] = conf_int.iloc[i, 1]\n",
    "    \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set home directory\n",
    "home_dir = '/Users/yiyihe/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology/Research/Energy_resilience/' # Macbook Pro\n",
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/' # Office Mac\n",
    "\n",
    "# Original hourly dataset (without differencing) is stored\n",
    "hourly_df = pd.read_csv(home_dir + \"01_data/processed/csv/hourly_519station_3weather.csv\",\n",
    "                        index_col=0)\n",
    "\n",
    "# Station ids where there EXISTS a non-stationary time series in the station data\n",
    "hourly_diff_station_dir = \"01_data/processed/csv/diff/hourly_diff1\"\n",
    "hourly_diff_station_ids = []\n",
    "for file in os.listdir(os.path.join(home_dir, hourly_diff_station_dir)):\n",
    "    if file[-3:] == \"csv\":\n",
    "        s_id = int(file.split(\"_\")[1])\n",
    "        hourly_diff_station_ids.append(s_id)\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/519 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "####\n",
    "maxlag = 24   # maxlag values: 3, 24, 72, 120\n",
    "\n",
    "# Output folder\n",
    "out_dir = home_dir + f'01_data/processed/csv/ARDL/hourly/lag{maxlag}/'\n",
    "\n",
    "# Stations already processed\n",
    "finished_stations = []\n",
    "for file in os.listdir(out_dir):\n",
    "    if file[-3:] == \"csv\":\n",
    "        finished_stations.append(int(file.split('_')[1]))\n",
    "\n",
    "\n",
    "# List of all stations ids\n",
    "station_lst = list(set(hourly_df.station_id.unique()))\n",
    "\n",
    "# Weather variables\n",
    "predictor_lst = ['t2m', 'tp', 'wind_speed']\n",
    "\n",
    "for s_id in tqdm(station_lst):\n",
    "    # Skip processed stations\n",
    "    if s_id in finished_stations:\n",
    "        continue\n",
    "    else:\n",
    "        try:\n",
    "            if s_id in hourly_diff_station_ids: # there are timeseries in the station df that is not stationary\n",
    "                # First check the differenced time series data for this station\n",
    "                # Note: here 't2m' is common but other variables may come up too so we need to account for that\n",
    "                # Read the differenced time series\n",
    "                station_diff_df = pd.read_csv(home_dir+f\"01_data/processed/csv/diff/hourly_diff1/station_{s_id}_hourly_diff1.csv\", index_col=0)\n",
    "                # Get the original df of the station too\n",
    "                station_df = hourly_df[hourly_df['station_id'] == s_id].sort_values(by='datetime')\n",
    "\n",
    "                # Find columns to that needs to shift\n",
    "                diffed_cols = [item.split(\"_diff\")[0] for item in station_diff_df.columns.values[1:]]\n",
    "                cols_to_shift = [x for x in predictor_lst if x not in diffed_cols]\n",
    "\n",
    "                # Find how many positions to shift (1 or 2)\n",
    "                shift_pos = station_df.shape[0] - station_diff_df.shape[0]\n",
    "                \n",
    "                # Shift the cols that were not differenced\n",
    "                for col in cols_to_shift:\n",
    "                    station_diff_df[f\"{col}_shift{shift_pos}\"] = station_df[col][shift_pos:].values\n",
    "                \n",
    "                # Now that the dataframe is ready, for ARDL\n",
    "                # ARDL select order\n",
    "                # Select correct lag order\n",
    "\n",
    "                sel_res = ardl_select_order(\n",
    "                                endog = station_diff_df.filter(regex='^pct_blackout').squeeze(), # engoenous var: hourly percent blackout\n",
    "                                maxlag = maxlag,     # The maximum lag to consider for the endogenous variable.\n",
    "                                exog = station_diff_df.iloc[:, 1:], # all columns except for the first col (pct blackout)\n",
    "                                maxorder = maxlag,  # a common max lag length for all exog variables\n",
    "                                trend = 'ct',       # Constant and time trend\n",
    "                                causal = True,     # exclude lag 0 of exog variables\n",
    "                                ic = 'bic',         # string\n",
    "                                glob = True         # consider all possible submodels of the largest model or only if smaller order lags must be included if larger order lags are\n",
    "                                )\n",
    "\n",
    "                res = sel_res.model.fit() # Fit ARDL model\n",
    "                res_dict = extract_ardl_results(res, s_id, sel_res)\n",
    "                res_df = pd.DataFrame([res_dict])\n",
    "                res_df.to_csv(out_dir + f'station_{s_id}_ardl_lag{maxlag}.csv')\n",
    "\n",
    "            else:\n",
    "                station_df = hourly_df[hourly_df['station_id'] == s_id].sort_values(by='datetime')\n",
    "                # ARDL select order\n",
    "                # Select correct lag order\n",
    "\n",
    "                sel_res = ardl_select_order(\n",
    "                                endog = station_df.filter(regex='^pct_blackout').squeeze(), # engoenous var: hourly percent blackout\n",
    "                                maxlag = maxlag,     # The maximum lag to consider for the endogenous variable.\n",
    "                                exog = station_df.iloc[:, 3:], # all columns except for station_id, datetime, pct blackout)\n",
    "                                maxorder = maxlag,  # a common max lag length for all exog variables\n",
    "                                trend = 'ct',       # Constant and time trend\n",
    "                                causal = True,     # exclude lag 0 of exog variables\n",
    "                                ic = 'bic',         # string\n",
    "                                glob = True         # consider all possible submodels of the largest model or only if smaller order lags must be included if larger order lags are\n",
    "                                )\n",
    "\n",
    "                res = sel_res.model.fit() # Fit ARDL model\n",
    "                res_dict = extract_ardl_results(res, s_id, sel_res)\n",
    "                res_df = pd.DataFrame([res_dict])\n",
    "                res_df.to_csv(out_dir + f'station_{s_id}_ardl_lag{maxlag}.csv')\n",
    "        except ValueError:\n",
    "            print(f\"Visit station {s_id} later\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function that runs ARDL on target and predictor time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ARDL_findlag_daily(maxlag, target, predictor_lst, station_lst, ic, daily_df):\n",
    "    # Initiate an empty dictionary\n",
    "    res_dic = {}\n",
    "\n",
    "    # Iterate through all stations\n",
    "    for s_id in tqdm(station_lst):\n",
    "            try:\n",
    "                # Subset station data\n",
    "                station_df = daily_df[daily_df['station_id'] == s_id].sort_values(by='date')\n",
    "                # Ignore stations with less than 90 days/ 3 months worth of data\n",
    "                if len(station_df) <= 90:\n",
    "                     continue\n",
    "                else:\n",
    "                    # Find optimum lag and store station id with optimum lag in dictionary\n",
    "                    sel_res = ardl_select_order(\n",
    "                        station_df[target],\n",
    "                        exog=station_df[predictors_lst],\n",
    "                        maxlag=maxlag,\n",
    "                        ic= ic,# string\n",
    "                        maxorder=maxlag,\n",
    "                        causal=True,\n",
    "                        trend='ct'\n",
    "                        )\n",
    "                    # optimum lag values for endogenous variables\n",
    "                    endo_res_lst = list(sel_res.bic.values[0][1].values())\n",
    "                    # insert the optimum lag value for exdogenous variable\n",
    "                    endo_res_lst.insert(0, sel_res.bic.values[0][0])\n",
    "                    # store station id and optimum lag values in dictionary\n",
    "                    res_dic[s_id] = endo_res_lst # key: station id, value:[pct_blackout_lag, t2m_lag, wind_speed_lag, tp_lag]\n",
    "            except ValueError:\n",
    "                 print('error')\n",
    "    # Save results in dataframe             \n",
    "    res_df = pd.DataFrame.from_dict(res_dic, orient='index')\n",
    "\n",
    "    # Rename columns using meaningful names\n",
    "    res_final_df = res_df.reset_index().rename(columns={**{'index': 'station_id', 0: 'blackout_minutes_lag'},\n",
    "                                                        **{i+1: val+'_lag' for i, val in enumerate(predictors_lst)}})\n",
    "    return res_final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set home directory\n",
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/'\n",
    "# Load station daily data\n",
    "daily_df = pd.read_csv(home_dir + \"01_data/processed/csv/daily_519station_13weather.csv\", index_col=0)\n",
    "\n",
    "# Loop through different maxlag values\n",
    "for maxlag in [5, 10, 15, 30, 45, 60]: # unit: days\n",
    "    print(f'I am working on maxlag={maxlag} now.')\n",
    "    # Set target variable\n",
    "    target = 'daily_blackout_minutes'\n",
    "    # Set list of predictor variables\n",
    "    predictors_lst = [\n",
    "                    't2m_max',\n",
    "                    'wind_speed_max',\n",
    "                    'tp_max']\n",
    "    # list of unique station ids\n",
    "    station_lst = list(set(daily_df.station_id.unique()))\n",
    "    # Information criterion\n",
    "    ic = 'bic'\n",
    "    # Run ARDL find lag\n",
    "    result_df = run_ARDL_findlag_daily(maxlag, target, predictor_lst, station_lst, ic, daily_df)\n",
    "    # Save output dataframe to csv\n",
    "    result_df.to_csv(home_dir + f\"01_data/processed/csv/ARDL_daily_optimum_{ic}_max{maxlag}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "india0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
