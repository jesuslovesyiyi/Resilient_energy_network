{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3312a839-2071-4b84-b7d1-27258ba537d9",
   "metadata": {},
   "source": [
    "# Creating Uniform Voltage Data\n",
    "Before we start working on our voltage data, we have to convert it into a uniform format.  \n",
    "\n",
    "The two main data sources in this project are the current operating ESMI station data, which is scraped from the Prayas ESMI website using the india_esmi_scraper.py scraper, as well as the Harvard Dataverse Data (link?).  \n",
    "The two are "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4a1d6c-9619-458e-8476-03bc4342120c",
   "metadata": {},
   "source": [
    "# Aggregating Voltage Data into Usable Metrics\n",
    "Before we can perform any analysis on correlation between weather factors and power outages in India, we first aggregate the power data into something that can be used.  \n",
    "\n",
    "The data being processed here is the station data that comes directly from the india_esmi_scraper or from the previous Harvard Dataverse station data.  \n",
    "ESMI data comes minute-wise, and ERA5 weather data comes hour-wise, so it is necessary to minimally convert the ESMI data to hourly data.  \n",
    "Additionally, the ESMI data is not complete, meaning we have to apply our discretion when choosing the times and stations with which to perform our analysis.  \n",
    "To give ourselves the maximum flexibility, we will process the ESMI data into four versions:  \n",
    "- Outage Events: timestamped with outage beginning and duration\n",
    "- Hourly Data: each complete hour with the percentage of the time period being spent at voltage = 0\n",
    "- Daily Aggregate Outage Events: each complete day with the frequency of power outage events and average duration of said events, aggregated from the first outage events dataset\n",
    "- Minute-wise Data for Future Interpolation: because the data is incomplete, but in some cases may only be minorly incomplete, we can set some arbitrary threshold of missing minutes that we are willing to interpolate, and save that trimmed set of minute-wise data for later interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535fe27f-8bb4-462c-bd68-e8a7f3c17ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Path to the folder containing CSV files\n",
    "folder_path = \"C:/Users/user/Desktop/data_check/Original_Study_Stations/\"\n",
    "\n",
    "# Path to the folder where you want to save the result files\n",
    "result_folder = \"C:/Users/user/Desktop/data_check/only_0\"\n",
    "\n",
    "# Ensure the result folder exists, if not create it\n",
    "os.makedirs(result_folder, exist_ok=True)  # Create folder only if it doesn't exist\n",
    "\n",
    "# Get a list of CSV files in the folder\n",
    "csv_files = [file_name for file_name in os.listdir(folder_path) if file_name.endswith('.csv')]\n",
    "\n",
    "# Create a progress bar with tqdm\n",
    "with tqdm(total=len(csv_files), desc=\"Processing CSV files\") as pbar:\n",
    "    for file_name in csv_files:\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(os.path.join(folder_path, file_name))\n",
    "\n",
    "        # Filter rows where voltage column holds the value of 0\n",
    "        df_filtered = df[df['voltage'] == 0]\n",
    "\n",
    "        # Save the filtered DataFrame to a new CSV file\n",
    "        result_file_name = os.path.splitext(file_name)[0] + \"_only0.csv\"\n",
    "        result_file_path = os.path.join(result_folder, result_file_name)\n",
    "        df_filtered.to_csv(result_file_path, index=False)\n",
    "\n",
    "        # Update the progress bar\n",
    "        pbar.update(1)\n",
    "\n",
    "\n",
    "# Read the original CSV file\n",
    "df = pd.read_csv(\"D:/ESMI_Dataset/Final_Data/power_output_analysis/India_all_stations_power_outage_data.csv\")\n",
    "\n",
    "# Convert the 'datetime' column to datetime objects\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "# Sort the DataFrame by 'station_name' and 'datetime'\n",
    "df = df.sort_values(by=['station_name', 'datetime'])\n",
    "\n",
    "# Initialize an empty list to store the power outage events\n",
    "power_outages = []\n",
    "\n",
    "# Initialize variables to keep track of the current outage\n",
    "current_station = None\n",
    "current_start = None\n",
    "current_end = None\n",
    "current_duration = 0\n",
    "current_rows = 0\n",
    "\n",
    "# Create a tqdm progress bar\n",
    "progress_bar = tqdm(total=len(df), desc=\"Processing data\", unit=\"rows\")\n",
    "\n",
    "# Iterate through the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    progress_bar.update(1)  # Update progress bar\n",
    "    if current_station is None:\n",
    "        # Start a new outage event\n",
    "        current_station = row['station_name']\n",
    "        current_start = row['datetime']\n",
    "        current_end = row['datetime']\n",
    "        current_duration = 1\n",
    "        current_rows = 1\n",
    "    elif row['station_name'] == current_station and row['datetime'] == current_end + pd.Timedelta(minutes=1):\n",
    "        # Continue current outage event\n",
    "        current_end = row['datetime']\n",
    "        current_duration += 1\n",
    "        current_rows += 1\n",
    "    else:\n",
    "        # End current outage event and start a new one\n",
    "        power_outages.append([current_station, current_duration, current_start, current_end, current_rows])\n",
    "        current_station = row['station_name']\n",
    "        current_start = row['datetime']\n",
    "        current_end = row['datetime']\n",
    "        current_duration = 1\n",
    "        current_rows = 1\n",
    "\n",
    "# Add the last outage event to the list\n",
    "if current_station is not None:\n",
    "    power_outages.append([current_station, current_duration, current_start, current_end, current_rows])\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()\n",
    "\n",
    "# Convert the list of outage events into a DataFrame\n",
    "outages_df = pd.DataFrame(power_outages, columns=['station_name', 'duration', 'start', 'end', 'number_of_rows'])\n",
    "\n",
    "# Write the DataFrame to a new CSV file\n",
    "outages_df.to_csv(\"D:/ESMI_Dataset/Final_Data/power_output_analysis/power_outages.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226203d3-28d1-4f83-9679-a16d0f190724",
   "metadata": {},
   "source": [
    "# Converting hourly data into daily aggregate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e6b1d1-4c9c-4db8-b2d3-36cf09bb13fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the folder containing the hourly weather variables CSV files\n",
    "folder_path = 'D:/ESMI_Dataset/EAR5_Data/raw_ESMI_boundary'\n",
    "\n",
    "# Create a sub-folder called 'daily'\n",
    "daily_folder = os.path.join(folder_path, 'daily')\n",
    "os.makedirs(daily_folder, exist_ok=True)\n",
    "\n",
    "# Get a list of all CSV files in the folder\n",
    "files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "\n",
    "# Iterate through each CSV file\n",
    "for file in files:\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(os.path.join(folder_path, file))\n",
    "    \n",
    "    # Convert 'datetime' column to datetime type\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    \n",
    "    # Extract station information from the current CSV file\n",
    "    station_info = df[['station_id', 'station_lat', 'station_lon']].iloc[0]\n",
    "    \n",
    "    # Aggregate daily average for 't2m', 'd2m', 'sp'\n",
    "    daily_avg = df.groupby(df['datetime'].dt.date)[['t2m', 'd2m', 'sp']].mean()\n",
    "    \n",
    "    # Aggregate daily average of squared values for 'u10' and 'v10'\n",
    "    df['u10_sq'] = df['u10'] ** 2\n",
    "    df['v10_sq'] = df['v10'] ** 2\n",
    "    daily_avg_squared = df.groupby(df['datetime'].dt.date)[['u10_sq', 'v10_sq']].mean()\n",
    "    \n",
    "    # Calculate daily average wind speed\n",
    "    daily_avg['wind_speed'] = np.sqrt(daily_avg_squared['u10_sq'] + daily_avg_squared['v10_sq'])\n",
    "    \n",
    "    # Extract 'tp' value for 11:00 pm of each day\n",
    "    df['hour'] = df['datetime'].dt.hour\n",
    "    last_hour_tp = df[df['hour'] == 23].groupby(df['datetime'].dt.date)['tp'].last()\n",
    "    \n",
    "    # Merge daily average, wind speed, and last hour tp columns\n",
    "    daily_data = daily_avg.join(last_hour_tp, rsuffix='_last_hour')\n",
    "    \n",
    "    # Add station information to daily data\n",
    "    daily_data['station_id'] = station_info['station_id']\n",
    "    daily_data['station_lat'] = station_info['station_lat']\n",
    "    daily_data['station_lon'] = station_info['station_lon']\n",
    "    daily_data.reset_index(inplace=True)\n",
    "    \n",
    "    # Reorder columns to have station information first\n",
    "    cols = ['station_id', 'station_lat', 'station_lon', 'datetime', 't2m', 'd2m', 'sp', 'wind_speed', 'tp_last_hour']\n",
    "    daily_data = daily_data[cols]\n",
    "    daily_data.rename(columns={'datetime': 'date'}, inplace=True)\n",
    "    \n",
    "    # Save the aggregated daily weather variables to a new CSV file in the 'daily' sub-folder\n",
    "    output_file = os.path.join(daily_folder, file.split('.')[0] + '_daily.csv')\n",
    "    daily_data.to_csv(output_file, index=False)\n",
    "    print(f\"Daily aggregated data saved to: {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
