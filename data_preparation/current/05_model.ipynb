{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----********************-----\n",
    "\n",
    "# Created Time: 2025/07/13\n",
    "\n",
    "# Last updated: 2025/07/13\n",
    "\n",
    "# Author: Yiyi He\n",
    "\n",
    "### Use Case\n",
    "\n",
    "# This notebook explores the application of autoregressive models\n",
    "# 1. \n",
    "\n",
    "# -----********************-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Stats\n",
    "from statsmodels.tsa.api import ARDL\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "from statsmodels.tsa.ardl import ardl_select_order\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "# Geo\n",
    "from shapely.geometry import Point, Polygon\n",
    "# import geopandas as gpd\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.options.display.max_rows = 1000\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Processing\n",
    "from tqdm import tqdm\n",
    "import functools as ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Granger Causality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GC with hourly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/'\n",
    "\n",
    "hourly_df = pd.read_csv(home_dir + \"01_data/processed/csv/hourly_519station_3weather.csv\",\n",
    "                        index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gc_hourly(maxlag, target, predictor, station_lst, hourly_df):\n",
    "    # Initiate an empty dictionary\n",
    "    gc_dic = {}\n",
    "\n",
    "    # List of stations visited\n",
    "    # existing_keys = station_id_t2m_gc_max72_dic.keys()\n",
    "    # Iterate through all stations\n",
    "    for s_id in tqdm(station_lst):\n",
    "        if s_id == 518 or s_id == 563: # stations with too few records to work with\n",
    "            continue\n",
    "        else:\n",
    "            # Subset station data\n",
    "            station_df = hourly_df[hourly_df['station_id'] == s_id].sort_values(by='datetime')\n",
    "            station_df\n",
    "            test_result = grangercausalitytests(\n",
    "                station_df[[target, predictor]], maxlag=maxlag, addconst=True, verbose=False)\n",
    "            F_test_p_values = [round(test_result[i+1][0]['ssr_ftest'][1],4) for i in range(maxlag)]\n",
    "            Chi_squared_p_values = [round(test_result[i+1][0]['ssr_chi2test'][1],4) for i in range(maxlag)]\n",
    "            p_values_min = np.min(F_test_p_values+Chi_squared_p_values)\n",
    "            # Key: station id, Value: list of 1. minimum F/Chi p values 2. F-test p values for all lags 3. Chi-square test p-values for all lags\n",
    "            gc_dic[s_id] = [p_values_min, F_test_p_values, Chi_squared_p_values]\n",
    "            \n",
    "    gc_df = pd.DataFrame.from_dict(gc_dic, orient='index').reset_index()\n",
    "    gc_df.rename(columns={\n",
    "    'index':'station_id',\n",
    "    0:f'{predictor}_p-value_min',\n",
    "    1:f'{predictor}_f_p-value',\n",
    "    2:f'{predictor}_Chi_p-value'\n",
    "    }, inplace=True)\n",
    "    return gc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 9/519 [01:10<53:16,  6.27s/it]  "
     ]
    }
   ],
   "source": [
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/'\n",
    "\n",
    "hourly_df = pd.read_csv(home_dir + \"01_data/processed/csv/hourly_519station_3weather.csv\",\n",
    "                        index_col=0)\n",
    "target = 'pct_blackout'\n",
    "maxlag = 72\n",
    "station_lst = list(set(hourly_df.station_id.unique()))\n",
    "predictor_lst = ['t2m', 'tp', 'wind_speed']\n",
    "res_dic = {}\n",
    "for predictor in predictor_lst:\n",
    "    df_res = run_gc_hourly(maxlag, target, predictor, station_lst, hourly_df)\n",
    "    res_dic[predictor] = df_res\n",
    "\n",
    "dfs = [res_dic[p] for p in predictor_lst]\n",
    "df_joined= ft.reduce(lambda left, right: pd.merge(left, right, on='station_id'), dfs)\n",
    "df_joined.to_csv(home_dir + \"01_data/processed/csv/granger_max{maxlag}_pvalue.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 519/519 [1:31:05<00:00, 10.53s/it]\n"
     ]
    }
   ],
   "source": [
    "# Create a list of unique station ids\n",
    "station_id_lst = list(set(hourly_df.station_id.unique()))\n",
    "# Set max lag\n",
    "maxlag = 72\n",
    "# Target\n",
    "target = \"pct_blackout\"\n",
    "# Predictor\n",
    "predictor = \"tp\" # t2m, wind_speed\n",
    "\n",
    "# Initiate an empty dictionary\n",
    "tp_gc_max72_dic = {}\n",
    "\n",
    "# List of stations visited\n",
    "# existing_keys = station_id_t2m_gc_max72_dic.keys()\n",
    "# Iterate through all stations\n",
    "for s_id in tqdm(station_id_lst):\n",
    "    if s_id == 518 or s_id == 563: # stations with too few records to work with\n",
    "        continue\n",
    "    else:\n",
    "        # Subset station data\n",
    "        station_df = hourly_df[hourly_df['station_id'] == s_id].sort_values(by='datetime')\n",
    "        test_result = grangercausalitytests(\n",
    "            station_df[[target, predictor]], maxlag=maxlag, addconst=True, verbose=False)\n",
    "        F_test_p_values = [round(test_result[i+1][0]['ssr_ftest'][1],4) for i in range(maxlag)]\n",
    "        Chi_squared_p_values = [round(test_result[i+1][0]['ssr_chi2test'][1],4) for i in range(maxlag)]\n",
    "        p_values_min = np.min(F_test_p_values+Chi_squared_p_values)\n",
    "        # Key: station id, Value: list of 1. minimum F/Chi p values 2. F-test p values for all lags 3. Chi-square test p-values for all lags\n",
    "        tp_gc_max72_dic[s_id] = [p_values_min, F_test_p_values, Chi_squared_p_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 519/519 [1:31:24<00:00, 10.57s/it]\n"
     ]
    }
   ],
   "source": [
    "# Create a list of unique station ids\n",
    "station_id_lst = list(set(hourly_df.station_id.unique()))\n",
    "# Set max lag\n",
    "maxlag = 72\n",
    "# Target\n",
    "target = \"pct_blackout\"\n",
    "# Predictor\n",
    "predictor = \"t2m\" # t2m, wind_speed\n",
    "\n",
    "# Initiate an empty dictionary\n",
    "t2m_gc_max72_dic = {}\n",
    "\n",
    "# List of stations visited\n",
    "# existing_keys = station_id_t2m_gc_max72_dic.keys()\n",
    "# Iterate through all stations\n",
    "for s_id in tqdm(station_id_lst):\n",
    "    if s_id == 518 or s_id == 563: # stations with too few records to work with\n",
    "        continue\n",
    "    else:\n",
    "        # Subset station data\n",
    "        station_df = hourly_df[hourly_df['station_id'] == s_id].sort_values(by='datetime')\n",
    "        test_result = grangercausalitytests(\n",
    "            station_df[[target, predictor]], maxlag=maxlag, addconst=True, verbose=False)\n",
    "        F_test_p_values = [round(test_result[i+1][0]['ssr_ftest'][1],4) for i in range(maxlag)]\n",
    "        Chi_squared_p_values = [round(test_result[i+1][0]['ssr_chi2test'][1],4) for i in range(maxlag)]\n",
    "        p_values_min = np.min(F_test_p_values+Chi_squared_p_values)\n",
    "        # Key: station id, Value: list of 1. minimum F/Chi p values 2. F-test p values for all lags 3. Chi-square test p-values for all lags\n",
    "        t2m_gc_max72_dic[s_id] = [p_values_min, F_test_p_values, Chi_squared_p_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 519/519 [1:33:30<00:00, 10.81s/it]\n"
     ]
    }
   ],
   "source": [
    "# Create a list of unique station ids\n",
    "station_id_lst = list(set(hourly_df.station_id.unique()))\n",
    "# Set max lag\n",
    "maxlag = 72\n",
    "# Target\n",
    "target = \"pct_blackout\"\n",
    "# Predictor\n",
    "predictor = \"wind_speed\" # t2m, wind_speed\n",
    "\n",
    "# Initiate an empty dictionary\n",
    "wind_speed_gc_max72_dic = {}\n",
    "\n",
    "# List of stations visited\n",
    "# existing_keys = station_id_t2m_gc_max72_dic.keys()\n",
    "# Iterate through all stations\n",
    "for s_id in tqdm(station_id_lst):\n",
    "    if s_id == 518 or s_id == 563: # stations with too few records to work with\n",
    "        continue\n",
    "    else:\n",
    "        # Subset station data\n",
    "        station_df = hourly_df[hourly_df['station_id'] == s_id].sort_values(by='datetime')\n",
    "        test_result = grangercausalitytests(\n",
    "            station_df[[target, predictor]], maxlag=maxlag, addconst=True, verbose=False)\n",
    "        F_test_p_values = [round(test_result[i+1][0]['ssr_ftest'][1],4) for i in range(maxlag)]\n",
    "        Chi_squared_p_values = [round(test_result[i+1][0]['ssr_chi2test'][1],4) for i in range(maxlag)]\n",
    "        p_values_min = np.min(F_test_p_values+Chi_squared_p_values)\n",
    "        # Key: station id, Value: list of 1. minimum F/Chi p values 2. F-test p values for all lags 3. Chi-square test p-values for all lags\n",
    "        wind_speed_gc_max72_dic[s_id] = [p_values_min, F_test_p_values, Chi_squared_p_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store granger causality results (p values for 2 tests, minimum p-value) for one predictor in dataframe\n",
    "tp_gc_max72_df = pd.DataFrame.from_dict(tp_gc_max72_dic, orient='index').reset_index()\n",
    "tp_gc_max72_df.rename(columns={\n",
    "    'index':'station_id',\n",
    "    0:'tp_p-value_min',\n",
    "    1:'tp_f_p-value',\n",
    "    2:'tp_Chi_p-value'\n",
    "}, inplace=True)\n",
    "# Join dataframes\n",
    "# dfs = [station_id_t2m_gc_max72_df, station_id_windspeed_gc_max72_df, station_id_tp_gc_max72_df]\n",
    "# df_joined= ft.reduce(lambda left, right: pd.merge(left, right, on='station_id'), dfs)\n",
    "# df_joined.to_csv('/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology/Research/Energy_resilience/01_data/2024_9_10_Tingyu/processed/station_514_gc_max72_pvalue_df.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
