{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----********************-----\n",
    "\n",
    "# Created Time: 2025/07/13\n",
    "\n",
    "# Last updated: 2025/07/13\n",
    "\n",
    "# Author: Yiyi He\n",
    "\n",
    "### Use Case\n",
    "\n",
    "# This notebook explores the application of autoregressive models\n",
    "# 1. \n",
    "\n",
    "# -----********************-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Stats\n",
    "from statsmodels.tsa.api import ARDL\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "from statsmodels.tsa.ardl import ardl_select_order\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "# Geo\n",
    "from shapely.geometry import Point, Polygon\n",
    "# import geopandas as gpd\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.options.display.max_rows = 1000\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Processing\n",
    "from tqdm import tqdm\n",
    "import functools as ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Granger Causality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GC with hourly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/'\n",
    "\n",
    "hourly_df = pd.read_csv(home_dir + \"01_data/processed/csv/hourly_519station_3weather.csv\",\n",
    "                        index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function that runs the Granger Causality test on hourly station data. \\\n",
    "For each station, run the Granger Causality test with the following set up:\\\n",
    " \\\n",
    "**Target variable**: \\\n",
    "'pct_blackout' \\\n",
    "**Predictor variables**: \n",
    "1. 't2m' (temperature) \n",
    "2. 'tp' (precipitation) \n",
    "3. 'wind_speed' (wind speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gc_hourly(maxlag, target, predictor, station_lst, hourly_df):\n",
    "    # Initiate an empty dictionary for storing test results\n",
    "    gc_dic = {}\n",
    "\n",
    "    # Iterate through all stations\n",
    "    for s_id in tqdm(station_lst):\n",
    "        # Extracting the dataframe for a/one station\n",
    "        station_df = hourly_df[hourly_df['station_id'] == s_id].sort_values(by='datetime')\n",
    "        try:\n",
    "            # Check if the number of observations available at the station is sufficient for the gc model\n",
    "            if len(station_df) <= maxlag + 2 + 1: # maxlag + num_variables (target, predictor) + 1\n",
    "                print(f\"Skipping Station {s_id} due to insufficient observations for maxlag={maxlag}\")\n",
    "                continue\n",
    "            else:\n",
    "                # Run granger causality test on station hourly data\n",
    "                test_result = grangercausalitytests(\n",
    "                    station_df[[target, predictor]], maxlag=maxlag, addconst=True, verbose=False)\n",
    "                # Save test values\n",
    "                F_test_p_values = [round(test_result[i+1][0]['ssr_ftest'][1],4) for i in range(maxlag)]\n",
    "                Chi_squared_p_values = [round(test_result[i+1][0]['ssr_chi2test'][1],4) for i in range(maxlag)]\n",
    "                p_values_min = np.min(F_test_p_values+Chi_squared_p_values)\n",
    "                # Key: station id, Value: list of 1. minimum F/Chi p values 2. F-test p values for all lags 3. Chi-square test p-values for all lags\n",
    "                gc_dic[s_id] = [p_values_min, F_test_p_values, Chi_squared_p_values]\n",
    "        except ValueError:\n",
    "            print(f\"Skipping Station {s_id} due to insufficient observations for maxlag={maxlag}\")\n",
    "    # Convert to dataframe        \n",
    "    gc_df = pd.DataFrame.from_dict(gc_dic, orient='index').reset_index()\n",
    "    gc_df.rename(columns={\n",
    "    'index':'station_id',\n",
    "    0:f'{predictor}_p-value_min',\n",
    "    1:f'{predictor}_f_p-value',\n",
    "    2:f'{predictor}_Chi_p-value'\n",
    "    }, inplace=True)\n",
    "    return gc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 54/519 [30:14<4:09:05, 32.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Station 58 due to insufficient observations for maxlag=120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 94/519 [48:09<2:24:04, 20.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Station 103 due to insufficient observations for maxlag=120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 119/519 [59:00<3:34:41, 32.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Station 130 due to insufficient observations for maxlag=120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 123/519 [1:00:10<2:32:05, 23.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Station 134 due to insufficient observations for maxlag=120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 354/519 [2:57:32<39:47, 14.47s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Station 386 due to insufficient observations for maxlag=120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 466/519 [3:55:57<12:56, 14.64s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Station 518 due to insufficient observations for maxlag=120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 509/519 [4:37:21<03:18, 19.81s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Station 563 due to insufficient observations for maxlag=120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 519/519 [4:40:29<00:00, 32.43s/it]\n",
      " 10%|█         | 54/519 [27:08<3:25:05, 26.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Station 58 due to insufficient observations for maxlag=120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 94/519 [45:17<2:45:21, 23.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Station 103 due to insufficient observations for maxlag=120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 119/519 [56:08<3:30:32, 31.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Station 130 due to insufficient observations for maxlag=120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 123/519 [57:07<2:15:26, 20.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Station 134 due to insufficient observations for maxlag=120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 354/519 [2:52:20<43:25, 15.79s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Station 386 due to insufficient observations for maxlag=120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 466/519 [3:44:38<10:34, 11.98s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Station 518 due to insufficient observations for maxlag=120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 509/519 [4:22:50<03:14, 19.45s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Station 563 due to insufficient observations for maxlag=120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 519/519 [4:25:41<00:00, 30.71s/it]\n",
      " 10%|█         | 54/519 [27:42<3:04:59, 23.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Station 58 due to insufficient observations for maxlag=120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 94/519 [46:13<2:36:52, 22.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Station 103 due to insufficient observations for maxlag=120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 119/519 [55:56<2:44:53, 24.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Station 130 due to insufficient observations for maxlag=120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 123/519 [56:43<1:48:52, 16.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Station 134 due to insufficient observations for maxlag=120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 354/519 [2:47:28<39:23, 14.33s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Station 386 due to insufficient observations for maxlag=120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 466/519 [3:39:14<08:09,  9.23s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Station 518 due to insufficient observations for maxlag=120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 509/519 [4:17:31<03:34, 21.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Station 563 due to insufficient observations for maxlag=120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 519/519 [4:20:58<00:00, 30.17s/it]\n"
     ]
    }
   ],
   "source": [
    "# Set home directory\n",
    "home_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/'\n",
    "# Read hourly data\n",
    "hourly_df = pd.read_csv(home_dir + \"01_data/processed/csv/hourly_519station_3weather.csv\",\n",
    "                        index_col=0)\n",
    "# Set target variable\n",
    "target = 'pct_blackout'\n",
    "# Set max lag for all variables\n",
    "maxlag = 120\n",
    "# Generate a list of station ids\n",
    "station_lst = list(set(hourly_df.station_id.unique()))\n",
    "# List of predictor variables\n",
    "predictor_lst = ['t2m', 'tp', 'wind_speed']\n",
    "\n",
    "# Initiate empty dictionary to store test output dataframes\n",
    "res_dic = {}\n",
    "# Loop through the list of predictors\n",
    "for predictor in predictor_lst:\n",
    "    # Run gc test\n",
    "    df_res = run_gc_hourly(maxlag, target, predictor, station_lst, hourly_df)\n",
    "    # Save output dataframe in dictionary key: predictor, value: dataframe with test values\n",
    "    res_dic[predictor] = df_res\n",
    "\n",
    "# Join resulting dataframes for all predictors together\n",
    "dfs = [res_dic[p] for p in predictor_lst]\n",
    "df_joined= ft.reduce(lambda left, right: pd.merge(left, right, on='station_id'), dfs)\n",
    "# Save output as new csv file\n",
    "df_joined.to_csv(home_dir + f\"01_data/processed/csv/granger_hourly_max{maxlag}_pvalue.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
