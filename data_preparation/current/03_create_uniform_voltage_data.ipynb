{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----********************-----\n",
    "\n",
    "# Created Time: 2025/07/03\n",
    "\n",
    "# Author: Yiyi He, Tiger Peng\n",
    "\n",
    "### Use Case\n",
    "\n",
    "# This notebook create uniform voltage data using the scraped minute-wise voltage data\n",
    "# The two main data sources in this project are the current operating ESMI station data,\n",
    "# which is scraped from the Prayas ESMI website using the india_esmi_scraper.py scraper,\n",
    "# as well as the Harvard Dataverse Data\n",
    "\n",
    "\n",
    "# The Harvard Dataverse Data is formatted in a 60 x n grid, with each row being labelled with an hour in the day, from 0-23,\n",
    "# and each of the columns being a minute of the hour from 0-59.\n",
    "\n",
    "# The scraped data is formatted in two columns, with the time column being the datetime down to the minute,\n",
    "# and the voltage column being the voltage from 0-255.\n",
    "# -----********************-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESMI and Harvard Dataverse ID\n",
    "\n",
    "In addition to having different data formats, the two data sources also use different numbering systems.\n",
    "Harvard Dataverse uses the names of the 528 stations covered in the time period of the dataset, between 2014-2018.\n",
    "The ESMI scraper uses the ESMI ids assigned in the dropdown elements used by the web scraper to select stations on the Prayas website.\n",
    "\n",
    "The India station locations were all cross-referenced (still need to document this), with duplicates and stations outside of the ERA5-land dataset removed. Each of the newly-scraped ESMI stations was given an ID number proceeding 528 (the last number assigned to the Dataverse stations), with duplicates in the Dataverse data removed.\n",
    "\n",
    "After this process, we have ids from 1-572 (with some missing because duplicates were removed), and a uniform data format.\n",
    "\n",
    "These files will all be stored in the india_processing/india_uniform directory, and their IDs can be referenced in the file ESMI_India_538_locations.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Dataverse Voltage Data\n",
    "\n",
    "We first need to verify that all of the stations deemed to be in the ERA5 dataset's range is in either the Dataverse dataset or the ESMI dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_name(name, exceptions = None):\n",
    "    formatted = name.split('-')[0].split('[')[0].strip()\n",
    "    \n",
    "    if exceptions:\n",
    "        return exceptions(formatted) # run the name through the entered exceptions function, if provided\n",
    "    else:\n",
    "        return formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the location information table, which we can use to map between station_id, ESMI_ID, and station names.\n",
    "# Target filename: ESMI_India_538_locations.csv\n",
    "locations_path = \"/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/01_data/processed/ESMI_India_538_locations.csv\"\n",
    "locations = pd.read_csv(locations_path, dtype={\"ESMI_ID\" : str, \"station_id\" : int}, usecols=[\"station_id\", \"ESMI_ID\", \"Location name\", \"District\", \"State\", \"Lat\", \"Lon\"])\n",
    "locations.rename(columns={\"Location name\": \"station_name\", \"District\" : \"district\", \"State\": \"state\", \"Lat\": \"lat\", \"Lon\": \"lon\"}, inplace=True)\n",
    "locations[\"station_name\"] = locations[\"station_name\"].apply(strip_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>station_name</th>\n",
       "      <th>ESMI_ID</th>\n",
       "      <th>district</th>\n",
       "      <th>state</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5th phase JP Nagar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bengaluru Urban</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>12.901092</td>\n",
       "      <td>77.589150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>80 feet road</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dhule</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>20.895199</td>\n",
       "      <td>74.775982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Adarsh Nagar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Saharsa</td>\n",
       "      <td>Bihar</td>\n",
       "      <td>25.883507</td>\n",
       "      <td>86.614919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Adgaon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nashik</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>20.037184</td>\n",
       "      <td>73.850136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Agarchitti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Chamoli</td>\n",
       "      <td>Uttarakhand</td>\n",
       "      <td>30.016116</td>\n",
       "      <td>79.308735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   station_id        station_name ESMI_ID         district        state  \\\n",
       "0           1  5th phase JP Nagar     NaN  Bengaluru Urban    Karnataka   \n",
       "1           2        80 feet road     NaN            Dhule  Maharashtra   \n",
       "2           3        Adarsh Nagar     NaN          Saharsa        Bihar   \n",
       "3           4              Adgaon     NaN           Nashik  Maharashtra   \n",
       "4           5          Agarchitti     NaN          Chamoli  Uttarakhand   \n",
       "\n",
       "         lat        lon  \n",
       "0  12.901092  77.589150  \n",
       "1  20.895199  74.775982  \n",
       "2  25.883507  86.614919  \n",
       "3  20.037184  73.850136  \n",
       "4  30.016116  79.308735  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cy/f8598vm53cggn74m01d5s0rc0000gr/T/ipykernel_33155/869827649.py:6: DtypeWarning: Columns (3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(os.path.join(dataverse_dir, file))\n"
     ]
    }
   ],
   "source": [
    "# Get the unique names from all of the Harvard Dataverse spreadsheets\n",
    "dataverse_dir = \"/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/01_data/processed/india_dataverse\"\n",
    "stations_found = set()\n",
    "for file in os.listdir(dataverse_dir):\n",
    "    if file.endswith('.csv'):\n",
    "        data = pd.read_csv(os.path.join(dataverse_dir, file))\n",
    "        stations_found.update(data[\"Location name\"].unique())\n",
    "\n",
    "stations_found = set([strip_name(x) for x in stations_found])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Kothrud', 'Hedayetpur', 'Guwahati Club', 'Ameerpet', 'Dudhimati', 'Nagaon', 'Vidya Nagar', 'Khatorbari', 'Chanho', 'Juri Par Shanti Path', 'Capital Electrical Subdivision', 'Kapoorthla', 'Borbheta', 'Besant Nagar', 'Datalpara', 'Ulubari', 'Kardaitola', 'Mahuadanr', 'ASEB Campus', 'Gohaibari', 'Tezpur', 'Chouparan', 'Saheed Nagar', 'Deopur', 'Bamunimaidan', 'Mihijam', 'Netarhat', 'Kolebira', 'Kairo', 'Lalmatia', 'Perka', 'Satbarwa', 'Banjara Hills', 'Rajdhani Masjid', 'GNB road', 'Nichinta', 'Zalim Khurd', 'Amrit Nagar', 'Sahakar Nagar', 'Tarun Nagar', 'Alipur', 'Haider Nagar', 'Bhurkunda', 'Bilasipara', 'Domadih'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add all of the station ids to a set to be checked against the available names/ids in either dataset\n",
    "station_names = set(locations[\"station_name\"])\n",
    "\n",
    "# Update, checking off all the names found in the dataverse name field\n",
    "stations_remaining = station_names.difference(stations_found)\n",
    "print(stations_remaining)\n",
    "len(stations_remaining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the station_ids of the stations still unaccounted for after the dataverse data\n",
    "station_ids_remaining = set(locations[(locations['station_name'].isin(stations_remaining))]['station_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB road\n",
      "Gohaibari\n",
      "ASEB Campus\n",
      "Guwahati Club\n",
      "Hedayetpur\n",
      "Bamunimaidan\n",
      "Juri Par Shanti Path\n",
      "Tarun Nagar\n",
      "Capital Electrical Subdivision\n",
      "Ulubari\n",
      "Vidya Nagar\n",
      "Datalpara\n",
      "{'Nepal _6417', 'VidyaNagar', 'Santacruz', 'Yadthare', 'GNB road Guwahati', 'Bamunimaidan Guwahati', 'Nepal _4496', 'Sukhbaderi (S)', 'ASEB Campus Guwahati', 'Maruti Vethika Road', 'Ameerpeth', 'MG Road Panjim', 'Ulubari Guwahati', 'Sukhbaderi (N)', 'Miramar Panjim', 'Alwarpet', 'KT_4355', 'Subhash road Ratnagiri', 'Tilak Ali Ratnagiri', 'Palamu_2', 'Tarun Nagar Guwahati', 'Karnataka_7', 'Surakalpeth Cuddalore', 'Nepal _8193', 'Brahmavar', 'Benaulim', 'Uttardah (S)', 'Kapoorthala', 'Carambolim', 'Sancole', 'Capital Electrical Subdivision Guwahati', 'KT_8961', 'Dehradun_5', 'Colva', 'Guwahati Club Guwahati', 'Devpur', 'Saheed Nagar Bhubaneswar', 'Sayapettai Chinnasandhu', 'Nepal _5791', 'Sahakarnagar', 'Masipidi', 'Gimhavne', 'Juri Par Shanti Path Guwahati', 'Siddhapura', 'Nakre', 'Gohaibari Guwahati', 'Nerul', 'Bhubaneswar_5', 'Varanasi_7721', 'Banjara hills Hyderabad', 'Betalbatim', 'Besant Nagar Chennai', 'Manjakuppam Cuddalore', 'Radgaon MIDC Ratnagiri', 'Hedayetpur Guwahati', 'Datalpara Guwahati'}\n"
     ]
    }
   ],
   "source": [
    "# Print all of the station names that we haven't found but expected to, ids less than 529, because those we expect to find in \n",
    "for id in station_ids_remaining:\n",
    "    if int(id) < 529:\n",
    "        print(locations[locations['station_id'] == id]['station_name'].values[0])\n",
    "\n",
    "print(stations_found.difference(station_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to deal with exceptions in name formatting\n",
    "def name_exceptions(name):\n",
    "    if name == \"VidyaNagar\":\n",
    "        return \"Vidya Nagar\"\n",
    "    if \"Guwahati\" in name: # Remove Guwahati from the end\n",
    "        return \" \".join(name.split(\" \")[:-1])\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149    Guwahati Club\n",
      "Name: station_name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "stations_found = set([strip_name(x, name_exceptions) for x in stations_found])\n",
    "stations_remaining = station_names.difference(stations_found)\n",
    "\n",
    "station_ids_remaining = set(locations[(locations['station_name'].isin(stations_remaining))]['station_id'])\n",
    "\n",
    "for id in station_ids_remaining:\n",
    "    if int(id) < 529:\n",
    "        print(locations[locations['station_id'] == id]['station_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "esmi_dir = \"/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/01_data/processed/esmi_scraped/india_esmi\"\n",
    "\n",
    "esmi_found = set() # set for the esmi_ids found\n",
    "for file in os.listdir(esmi_dir):\n",
    "    if file.endswith('.csv'):\n",
    "        esmi_found.add(file.split('+')[0])\n",
    "\n",
    "ids_found = set(locations[locations['ESMI_ID'].isin(esmi_found)]['station_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{161}\n"
     ]
    }
   ],
   "source": [
    "final_remaining = station_ids_remaining.difference(ids_found)\n",
    "print(final_remaining)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to uniform format\n",
    "\n",
    "First, we can simply rename the ESMI stations according to their uniform station id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform_dir = \"/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/01_data/processed/india_uniform\"\n",
    "esmi_dir = \"/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/01_data/processed/esmi_scraped/india_esmi\"\n",
    "\n",
    "# Copy the scraped EMSI data files from \"india_esmi\" to \"india_uniform\" and rename the file to \"station_[uniform_id]\"\n",
    "for file in os.listdir(esmi_dir):\n",
    "    if file.endswith('.csv'):\n",
    "        esmi_id = file.split('+')[0]\n",
    "        uniform_id = locations[locations['ESMI_ID'] == esmi_id]['station_id'].values[0]\n",
    "\n",
    "        shutil.copy(os.path.join(esmi_dir, file), os.path.join(uniform_dir, f'station_{uniform_id}.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>station_name</th>\n",
       "      <th>ESMI_ID</th>\n",
       "      <th>district</th>\n",
       "      <th>state</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>533</td>\n",
       "      <td>Srirampura</td>\n",
       "      <td>212</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>13.068737</td>\n",
       "      <td>77.614819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     station_id station_name  ESMI_ID   district      state        lat  \\\n",
       "498         533   Srirampura      212  Bengaluru  Karnataka  13.068737   \n",
       "\n",
       "           lon  \n",
       "498  77.614819  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [Optional] The code below checks what the \"uniform_id\" is for a given \"ESMI ID\"\n",
    "locations[locations.ESMI_ID == 212]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we convert the Dataverse format into our uniform format.\n",
    "This involves taking each of the individual rows, taking the date and hour columns, along with the index of the minutes columns and transforming that into a vertical 2xn of datetime, voltage values, which can be concatenated together.\n",
    "\n",
    "Then we will use our existing name parsing to section each of the sheets by station.\n",
    "After this has been done for every station, we can save it to our uniform data location, taking care not to override any existing sheets, as the stations which are duplicate between ESMI and Dataverse, ESMI takes priority.\n",
    "\n",
    "Also, as we discovered later in the Cleanup section, the 2019Jan_Jun.csv has data quality problems which have to be address before we can incorporate it into our uniform dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataverse_dir = \"/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/01_data/processed/india_dataverse\"\n",
    "\n",
    "# Filter out just the locations that aren't actively scraped ESMI locations\n",
    "# so we avoid mapping them to a location number when processing dataverse data.\n",
    "rem_locations = locations[pd.isna(locations['ESMI_ID'])]\n",
    "name_to_station_id = dict(zip(rem_locations['station_name'].to_list(), rem_locations['station_id'].to_list())) # Create dictionary to make id lookup more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(dataverse_dir):\n",
    "    if \"2019\" in file: # Skip 2019 for now\n",
    "        continue\n",
    "    print(file)\n",
    "    hd_df = pd.read_csv(os.path.join(dataverse_dir, file), header=0)\n",
    "\n",
    "    # Remove the weird excel export #VALUE! artifacts\n",
    "    hd_df.replace('#VALUE!', pd.NA, inplace=True)\n",
    "    hd_df.dropna(inplace=True)\n",
    "    hd_df.iloc[:, -60:] = hd_df.iloc[:, -60:].apply(pd.to_numeric)\n",
    "\n",
    "    # Merge the date and hour columns to create a pd datetime\n",
    "    if '/' in hd_df['Date'][0]:\n",
    "        hd_df.insert(loc=1, column='time', value=(pd.to_datetime(hd_df['Date'], format='%m/%d/%Y') + pd.to_timedelta(hd_df['Hour'], unit='h')))\n",
    "    elif '-' in hd_df['Date'][0]:\n",
    "        hd_df.insert(loc=1, column='time', value=(pd.to_datetime(hd_df['Date'], format='%d-%m-%Y') + pd.to_timedelta(hd_df['Hour'], unit='h')))\n",
    "    else:\n",
    "        print('Invalid date format')\n",
    "        break\n",
    "        \n",
    "    hd_df.drop(['Date', 'Hour'], axis=1, inplace=True)\n",
    "    \n",
    "    # Derive the uniform station_id from the station names\n",
    "    hd_df.rename(columns={'Location name': 'station_name'}, inplace=True)\n",
    "    \n",
    "    # Create a new column of uniform station_ids, assigning -1 if it is a station we have already discarded, so we can ignore it\n",
    "    hd_df.insert(loc=0, column='station_id', value=hd_df['station_name'].apply(lambda x : name_to_station_id.get(strip_name(x, exceptions=name_exceptions), -1)))\n",
    "    hd_df.drop('station_name', axis=1, inplace=True)\n",
    "    \n",
    "    unpivoted_rows = []\n",
    "    for index in tqdm(range(hd_df.shape[0])):\n",
    "        if hd_df.iloc[index, 0] == -1: # Skip irrelevant or already covered stations\n",
    "            continue\n",
    "            \n",
    "        unpivoted_row = hd_df.iloc[[index], :].melt(id_vars=['station_id', 'time'], ignore_index=True)\n",
    "        unpivoted_row['time'] = unpivoted_row['time'] + pd.to_timedelta(unpivoted_row['variable'].apply(lambda x : int(x.split(' ')[1])), unit='m')\n",
    "        \n",
    "        unpivoted_rows.append(unpivoted_row)\n",
    "    \n",
    "    unpivoted_df = pd.concat(unpivoted_rows)\n",
    "    unpivoted_df.drop('variable', axis=1, inplace=True)\n",
    "    unpivoted_df.rename(columns={'value': 'voltage'}, inplace=True)\n",
    "\n",
    "    # If the station_data already exists, append the new data on the end, otherwise, write directly to a new csv\n",
    "    for station_id in unpivoted_df['station_id'].unique():\n",
    "        output_path = os.path.join(uniform_dir, f'station_{station_id}.csv')\n",
    "        subset_df = unpivoted_df[unpivoted_df['station_id'] == station_id]\n",
    "        subset_df = subset_df.drop('station_id', axis=1)\n",
    "        \n",
    "        if os.path.exists(output_path):\n",
    "            station_df = pd.read_csv(output_path, header=0, parse_dates=['time'])\n",
    "            if station_df['time'].iloc[-1] < subset_df['time'].iloc[0]:\n",
    "                station_df = pd.concat((station_df, subset_df))\n",
    "                station_df.to_csv(output_path, index=False) # Saving to uniform dir\n",
    "        else:\n",
    "            subset_df.to_csv(output_path, index=False) # Saving to uniform dir\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with 2019Jan-Jun\n",
    "Because there are the presence of duplicate rows, both duplicate in the timestamp and station. I will just remove these duplicate rows.\n",
    "\n",
    "There is also the issue of erroneous voltage values, such as voltages of 535 or 999, which do not appear elsewhere in the dataverse dataset. I choose to simply ignore rows which contain values greater than 400 entirely, because in the way that we aggregate voltage data over hours or days, removal of a single datapoints is equivalent to removal of the entire hour anyways.\n",
    "\n",
    "We also take special care to simply remove the data of Bodireddypally-Prakasam and Kanheri Sarap, as the repeat rate is very high, and what can be salvaged may be questionable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(dataverse_dir):\n",
    "    if not \"2019\" in file: # Only operate on the 2019 dataset\n",
    "        continue\n",
    "    print(file)\n",
    "    hd_df = pd.read_csv(os.path.join(dataverse_dir, file), header=0)\n",
    "\n",
    "    # Remove the weird excel export #VALUE! artifacts\n",
    "    hd_df.replace('#VALUE!', pd.NA, inplace=True)\n",
    "    hd_df.dropna(inplace=True)\n",
    "    hd_df.iloc[:, -60:] = hd_df.iloc[:, -60:].apply(pd.to_numeric)\n",
    "    hd_df.iloc[:, -60:][hd_df.iloc[:, -60:] > 400] = pd.NA\n",
    "    hd_df.dropna(inplace=True) # Ignore rows with values greater than 400 \n",
    "\n",
    "    # Merge the date and hour columns to create a pd datetime\n",
    "    if '/' in hd_df['Date'][0]:\n",
    "        hd_df.insert(loc=1, column='time', value=(pd.to_datetime(hd_df['Date'], format='%m/%d/%Y') + pd.to_timedelta(hd_df['Hour'], unit='h')))\n",
    "    elif '-' in hd_df['Date'][0]:\n",
    "        hd_df.insert(loc=1, column='time', value=(pd.to_datetime(hd_df['Date'], format='%d-%m-%Y') + pd.to_timedelta(hd_df['Hour'], unit='h')))\n",
    "    else:\n",
    "        print('Invalid date format')\n",
    "        break\n",
    "        \n",
    "    hd_df.drop(['Date', 'Hour'], axis=1, inplace=True)\n",
    "    \n",
    "    # Derive the uniform station_id from the station names\n",
    "    hd_df.rename(columns={'Location name': 'station_name'}, inplace=True)\n",
    "    \n",
    "    # Create a new column of uniform station_ids, assigning -1 if it is a station we have already discarded, so we can ignore it\n",
    "    hd_df.insert(loc=0, column='station_id', value=hd_df['station_name'].apply(lambda x : name_to_station_id.get(strip_name(x, exceptions=name_exceptions), -1)))\n",
    "    hd_df.drop('station_name', axis=1, inplace=True)\n",
    "\n",
    "    hd_df = hd_df[hd_df['station_id'] != -1]\n",
    "    hd_df = hd_df[hd_df['station_id'] != 223]\n",
    "    hd_df = hd_df[hd_df['station_id'] != 86] # Remove all the high error rate and irrelevant stations\n",
    "\n",
    "    station_ids = hd_df['station_id'].unique()\n",
    "    \n",
    "    stations_2019 = {}\n",
    "    for station_id in station_ids:\n",
    "        print(station_id)\n",
    "        unpivoted_rows = []\n",
    "        station_df = hd_df[hd_df['station_id'] == station_id]\n",
    "\n",
    "        station_df = station_df[~station_df.duplicated(subset=['time'], keep=False)] # Remove duplicate times\n",
    "\n",
    "        for index in tqdm(range(station_df.shape[0])):\n",
    "            unpivoted_row = station_df.iloc[[index], :].melt(id_vars=['station_id', 'time'], ignore_index=True)\n",
    "            unpivoted_row['time'] = unpivoted_row['time'] + pd.to_timedelta(unpivoted_row['variable'].apply(lambda x : int(x.split(' ')[1])), unit='m')\n",
    "            unpivoted_rows.append(unpivoted_row)\n",
    "\n",
    "        unpivoted_df = pd.concat(unpivoted_rows)\n",
    "        unpivoted_df.drop('variable', axis=1, inplace=True)\n",
    "        unpivoted_df.rename(columns={'value': 'voltage'}, inplace=True)\n",
    "\n",
    "        stations_2019[station_id] = unpivoted_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the Quality of the Post-2019 Data\n",
    "\n",
    "We have reason to believe that the data quality post-2019 would be lower than that of before, because the program which pertained to the harvard dataverse sites ended in 2019. Just for a first order check before we incorporate the 2019 data, I will see if the mean and std of the voltage is substantially different before and after 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_ids = []\n",
    "mean_diffs = []\n",
    "mean1s = []\n",
    "mean2s = []\n",
    "std_diffs = []\n",
    "std1s = []\n",
    "std2s = []\n",
    "\n",
    "for station_id in stations_2019.keys():\n",
    "    output_path = os.path.join(uniform_dir, f'station_{station_id}.csv')\n",
    "    station_df_new = stations_2019[station_id]\n",
    "    \n",
    "    if os.path.exists(output_path):\n",
    "        station_df = pd.read_csv(output_path, header=0, parse_dates=['time'])\n",
    "        station_ids.append(station_id)\n",
    "        mean1 = station_df['voltage'].mean()\n",
    "        mean1s.append(mean1)\n",
    "        mean2 = station_df_new['voltage'].mean()\n",
    "        mean2s.append(mean2)\n",
    "\n",
    "        mean_diffs.append(np.abs(mean1 - mean2))\n",
    "\n",
    "        std1 = station_df['voltage'].std()\n",
    "        std1s.append(std1)\n",
    "        std2 = station_df_new['voltage'].std()\n",
    "        std2s.append(std2)\n",
    "\n",
    "        std_diffs.append(np.abs(std1 - std2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_diffs_arr = np.array(mean_diffs)\n",
    "means1_arr = np.array(mean1s)\n",
    "means2_arr = np.array(mean2s)\n",
    "std_diffs_arr = np.array(std_diffs)\n",
    "std1s_arr = np.array(std1s)\n",
    "std2s_arr = np.array(std2s)\n",
    "\n",
    "mean_diffs_normalized = mean_diffs_arr / std1s_arr\n",
    "sorted_diffs = np.argsort(mean_diffs_normalized)\n",
    "\n",
    "print(\"Mean Differences\")\n",
    "print(mean_diffs_arr[sorted_diffs[-10:]])\n",
    "print(mean_diffs_normalized[sorted_diffs[-10:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Differences\n",
    "\n",
    "[ 66.86783524 100.22082143  98.36311681  37.29250032  49.31260174\n",
    "  79.76807313  74.52002874 175.2608046  239.52322478 242.16721693]\n",
    "\n",
    "[0.95419399 1.0269152  1.0873201  1.17463552 1.40466063 1.51603714\n",
    " 1.68559889 4.38305177 7.22517917 9.97748979]\n",
    "\n",
    " Seeing as how some of the mean diffs are more than one full standard deviation, I think that Changmin's theory about the poor data quality of the 2019 dataset is valid, and for the time being, I will just save them to their own separate dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the station_data already exists, append the new data on the end, otherwise, write directly to a new csv\n",
    "post_2019_dir = \"../india_processing/india_uniform_2019\"\n",
    "\n",
    "for station_id, station_df in stations_2019.items():\n",
    "    output_path = os.path.join(post_2019_dir, f'station_{station_id}.csv') \n",
    "    station_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Aggregating Voltage Data into Usable Metrics\n",
    "\n",
    "Before we can perform any analysis on correlation between weather factors and power outages in India, we first aggregate the power data into something that can be used.\n",
    "\n",
    "The data being processed here is the station data that comes directly from the india_esmi_scraper or from the modified Harvard Dataverse station data.\n",
    "ESMI data comes minute-wise, and ERA5 weather data comes hour-wise, so it is necessary to minimally convert the ESMI data to hourly data.\n",
    "Additionally, the ESMI data is not complete, meaning we have to apply our discretion when choosing the times and stations with which to perform our analysis.\n",
    "To give ourselves the maximum flexibility, we will process the data into 7 versions:\n",
    "\n",
    "- **Outage Events**: timestamped with outage beginning and duration\n",
    "- **Hourly Data**: each complete hour with the percentage of the time period being spent at voltage = 0\n",
    "- **Daily Aggregate Outage Events**: each complete day with the frequency of power outage events and average duration of said events, aggregated from the first outage events dataset\n",
    "- **Interpolated Minute-wise Data**: because the data is incomplete, but in some cases may only be minorly incomplete, we can set some arbitrary threshold of missing minutes that we are willing to interpolate, and save that interpolated minutewise data\n",
    "- **Interpolated Hourly Data**: the same procedure to acquire the hourly data except apply to the interpolated data\n",
    "- **Interpolated Outages**: the same procedure used to acquire the outages, except applied to the interpolated data\n",
    "- **Interpolated Daily Aggregate Outage Events**: the same procedure used to acquire the daily aggregate outage frequency and duration, except applied to the interpolated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>voltage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-11-24 09:00:00</td>\n",
       "      <td>241.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-11-24 09:01:00</td>\n",
       "      <td>241.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-11-24 09:02:00</td>\n",
       "      <td>241.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-11-24 09:03:00</td>\n",
       "      <td>242.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-11-24 09:04:00</td>\n",
       "      <td>242.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 time  voltage\n",
       "0 2014-11-24 09:00:00    241.0\n",
       "1 2014-11-24 09:01:00    241.0\n",
       "2 2014-11-24 09:02:00    241.0\n",
       "3 2014-11-24 09:03:00    242.0\n",
       "4 2014-11-24 09:04:00    242.0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### [Optional]\n",
    "### Interpolation Experiments\n",
    "# First, let's determine what our interpolation method should be by looking at the shape of the voltage curves\n",
    "sample_path = os.path.join(uniform_dir, 'station_12.csv')\n",
    "sample_df = pd.read_csv(sample_path, header=0, index_col=None, parse_dates=['time'])\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='time', ylabel='voltage'>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### [Optional]\n",
    "sample_df[0:50].plot(kind='scatter', x='time', y='voltage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that while the overall trend is linear, at the two or three minute level, the data tends to take an almost stepwise format, staying at a value for one or two minutes before advancing to the next one.\n",
    "\n",
    "So, for my first pass, I'll be looking to perform some form of nearest-value fill, just filling either end of a missing range with either it's beginning or end value.\n",
    "\n",
    "Please note that I would only apply this form of interpolation to short quantities of missing information, one to three minutes, not large sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### [Optional]\n",
    "# Interpolation\n",
    "def interpolate(begin_range, end_range, begin_value, end_value, mode='nearfill'):\n",
    "    # begin_range and end_range are both included in the original series\n",
    "    # the output will be a dataframe including both begin_range and end_range, as well as the filled values in between\n",
    "    times = [begin_range]\n",
    "    voltages = [begin_value]\n",
    "    difference = round((end_range - begin_range).seconds / 60)\n",
    "    if mode == 'nearfill':\n",
    "        for i in range(1, difference):\n",
    "            curr_time = begin_range + timedelta(minutes=i)\n",
    "            times.append(curr_time)\n",
    "            if i <= int(difference / 2):\n",
    "                voltages.append(begin_value)\n",
    "            else:\n",
    "                voltages.append(end_value)\n",
    "        times.append(end_range)\n",
    "        voltages.append(end_value)\n",
    "\n",
    "    output_df = pd.DataFrame({'time': times, 'voltage': voltages})\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can begin looping through all of the uniform data points, saving all of these relevant datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### [Optional]\n",
    "# TAKES A LONG TIME TO RUN\n",
    "station_files = os.listdir(uniform_dir)\n",
    "num_stations = len(station_files)\n",
    "for index, file in enumerate(station_files):\n",
    "    print(f'Station {index + 1}/{num_stations}: {file}')\n",
    "\n",
    "    station_df = pd.read_csv(os.path.join(uniform_dir, file), header=0, index_col=False, parse_dates=['time'])\n",
    "    time_difference = station_df['time'].diff()\n",
    "    print(time_difference.apply(lambda x : x.total_seconds()).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results we find that the shorter gaps in time largely don't exist, so we will be ditching interpolation, as we don't have an effective way to accurately fill the typical gaps of an hour or more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can make the aggregate hourly and daily statistics.\n",
    "\n",
    "The aggregate statistic for hourly is the percentage duration of outage in the hour, and for daily, we will both do gross duration, as well as a measurement of outage frequency (discrete events) and average outage duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_dir = \"/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/01_data/processed/india_hourly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through, getting to the next whole hour.\n",
    "# If the position 60 minutes from it is not the next whole hour, we know that data is incomplete.\n",
    "station_files = os.listdir(uniform_dir)\n",
    "num_stations = len(station_files)\n",
    "for index, file in enumerate(station_files):\n",
    "    print(f'Station {index + 1}/{num_stations}: {file}')\n",
    "    station_df = pd.read_csv(os.path.join(uniform_dir, file), header=0, index_col=False, parse_dates=['time'])\n",
    "\n",
    "    hours = []\n",
    "    pct_blackout = []\n",
    "    index = 0\n",
    "    previous_hour = None\n",
    "    while index + 60 < station_df.shape[0]:\n",
    "        if previous_hour is None:\n",
    "            if station_df['time'][index].minute == 0 and station_df['time'][index].second == 0:\n",
    "                previous_hour = station_df['time'][index]\n",
    "        else:\n",
    "            # If the hour is complete, we take the percentage of that portion of the dataset that is 0 voltage, and save that with the datetime of the\n",
    "            # beginning of the one hour period\n",
    "            next_hour = previous_hour + timedelta(hours = 1, minutes = 0, seconds = 0)\n",
    "            if station_df['time'][index + 60] == next_hour:\n",
    "                hours.append(previous_hour)\n",
    "                pct = station_df['voltage'][index:index+60].value_counts().get(0, 0) / 60\n",
    "                pct_blackout.append(pct)\n",
    "                index = index + 60\n",
    "                previous_hour = next_hour\n",
    "            else:\n",
    "                # We can backtrack until we hit the next whole hour or before, then do the sixty minutes test again\n",
    "                while station_df['time'][index] >= next_hour:\n",
    "                    print(f'{station_df[\"time\"][index]}, {next_hour}')\n",
    "                    if station_df['time'][index] == next_hour:\n",
    "                        previous_hour = next_hour\n",
    "                        break\n",
    "                    else:\n",
    "                        index -= 1 \n",
    "                if station_df['time'][index] < next_hour:\n",
    "                    while index < station_df.shape[0]:\n",
    "                        curr_time = station_df['time'][index]\n",
    "                        if curr_time.minute == 0 and curr_time.second == 0 and curr_time > previous_hour:\n",
    "                            previous_hour = curr_time\n",
    "                            break\n",
    "                        else:\n",
    "                            index += 1\n",
    "\n",
    "    id = file.split('_')[1].split('.')[0]\n",
    "    hourly_df = pd.DataFrame({'hour': hours, 'pct_blackout': pct_blackout})\n",
    "    hourly_df.to_csv(os.path.join(hourly_dir, f'hourly_station_{id}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can acquire the complete days and loop through to get the outages, which will be split along day boundaries to make the calculation of the aggregate measures simpler, as well as the gross duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_dir = \"/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/01_data/processed/india_daily\"\n",
    "outages_dir = \"/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/01_data/processed/india_outages\"\n",
    "agg_outages_dir = \"/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/01_data/processed/india_daily_aggregate_outages\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through, getting to the next whole hour. If the position 60 minutes from it is not the next whole hour, we know that data is incomplete.\n",
    "station_files = os.listdir(uniform_dir)\n",
    "num_stations = len(station_files)\n",
    "for index, file in enumerate(station_files):\n",
    "    print(f'Station {index + 1}/{num_stations}: {file}')\n",
    "    station_df = pd.read_csv(os.path.join(uniform_dir, file), header=0, index_col=False, parse_dates=['time'])\n",
    "\n",
    "    # Lists later converted into daily gross outage duration dataframe\n",
    "    days = []\n",
    "    pct_blackout = []\n",
    "\n",
    "    # Lists later converted into per station outages dataframe\n",
    "    outage_starts = []\n",
    "    outage_ends = []\n",
    "    outage_durations = []\n",
    "\n",
    "    # Lists later converted into per station daily aggregate outage dataframe\n",
    "    agg_outage_days = []\n",
    "    avg_outage_durations = []\n",
    "    outage_frequencies = []\n",
    "    index = 0\n",
    "    previous_day = None\n",
    "    while index + 1439 < station_df.shape[0]:\n",
    "        if previous_day is None:\n",
    "            if station_df['time'][index].hour == 0 and station_df['time'][index].minute == 0 and station_df['time'][index].second == 0:\n",
    "                previous_day = station_df['time'][index]\n",
    "            else:\n",
    "                index += 1\n",
    "        else:\n",
    "            # If the day is complete, we take the percentage of that portion of the dataset that is 0 voltage, and save that with the datetime of the\n",
    "            # beginning of the one hour period\n",
    "            end_of_hour = previous_day + timedelta(hours = 23, minutes = 59)\n",
    "            next_day = end_of_hour + timedelta(minutes = 1)\n",
    "            if station_df['time'][index + 1439] == end_of_hour:\n",
    "                days.append(previous_day)\n",
    "                pct = station_df['voltage'][index:index+1440].value_counts().get(0, 0) / 1440\n",
    "                pct_blackout.append(pct)\n",
    "\n",
    "                # In addition to getting the gross duration, we also loop through to get the outages for the station for this complete day\n",
    "                outage_starts_today = []\n",
    "                outage_ends_today = []\n",
    "                outage_durations_today = []\n",
    "                outage_start = None\n",
    "                outage_continuing = False\n",
    "                for i in range(index, index + 1440):\n",
    "                    if station_df['voltage'][i] == 0:\n",
    "                        if not outage_continuing: # outage start\n",
    "                            outage_start = station_df['time'][i]\n",
    "                            outage_continuing = True\n",
    "                    else:\n",
    "                        if outage_continuing: # outage end\n",
    "                            outage_end = station_df['time'][i]\n",
    "                            duration = outage_end - outage_start\n",
    "                            duration = duration.total_seconds() / 60\n",
    "\n",
    "                            outage_starts_today.append(outage_start)\n",
    "                            outage_ends_today.append(outage_end)\n",
    "                            outage_durations_today.append(duration)\n",
    "\n",
    "                            outage_continuing = False\n",
    "                            outage_start = None\n",
    "\n",
    "                # Append to overall station outages lists\n",
    "                outage_starts.extend(outage_starts_today)\n",
    "                outage_ends.extend(outage_ends_today)\n",
    "                outage_durations.extend(outage_durations_today)\n",
    "                \n",
    "                # Compute daily aggregate outage statistics\n",
    "                if len(outage_durations_today):\n",
    "                    avg_duration = np.mean(outage_durations_today)\n",
    "                else:\n",
    "                    avg_duration = 0\n",
    "                freq = len(outage_starts_today)\n",
    "\n",
    "                agg_outage_days.append(previous_day)\n",
    "                avg_outage_durations.append(avg_duration)\n",
    "                outage_frequencies.append(freq)\n",
    "                \n",
    "                index = index + 1440\n",
    "                previous_day = None # allow the script to reacquire the next day\n",
    "            else:\n",
    "                # We can backtrack until we hit the next whole hour or before, then do the 1439 minutes test again\n",
    "                while station_df['time'][index] >= next_day:\n",
    "                    if station_df['time'][index] == next_day:\n",
    "                        previous_day = next_day\n",
    "                        break\n",
    "                    else:\n",
    "                        index -= 1 \n",
    "                if station_df['time'][index] < next_day:\n",
    "                    while index < station_df.shape[0]:\n",
    "                        curr_time = station_df['time'][index]\n",
    "                        if curr_time.hour == 0 and curr_time.minute == 0 and curr_time.second == 0 and curr_time > previous_day:\n",
    "                            previous_day = curr_time\n",
    "                            break\n",
    "                        else:\n",
    "                            index += 1\n",
    "\n",
    "    id = file.split('_')[1].split('.')[0]\n",
    "    # Daily csv\n",
    "    daily_df = pd.DataFrame({'date': days, 'pct_blackout': pct_blackout})\n",
    "    daily_df.to_csv(os.path.join(daily_dir, f'daily_station_duration_{id}.csv'), index=False)\n",
    "    \n",
    "    # Outages csv\n",
    "    outages_df = pd.DataFrame({'outage_start': outage_starts, 'outage_end': outage_ends, 'duration': outage_durations})\n",
    "    outages_df.to_csv(os.path.join(outages_dir, f'outages_station_{id}.csv'), index=False)\n",
    "\n",
    "    # Daily aggregate outages csv\n",
    "    agg_outages_df = pd.DataFrame({'date': agg_outage_days, 'avg_duration': avg_outage_durations, 'freq': outage_frequencies})\n",
    "    agg_outages_df.to_csv(os.path.join(agg_outages_dir, f'agg_outages_station_{id}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Hourly Weather Data into Daily Aggregate Data\n",
    "\n",
    "The two main operations that we have to do for the extraction of the hourly data is to take the difference between the current and previous total precpitations, as tp is an accumulation variable, meaning it is strictly increasing throughout the day and represents the total precipitation up to that point in the day.\n",
    "\n",
    "For the daily aggregate data, we simply take the average over a day of most of the weather variables, and for the tp accumulation variable, we take the value that is present at 12:00 AM the next day, which is confusingly the total accumulated precipitation for the previous day.\n",
    "\n",
    "In addition, we calculate a cluster of other weather variables, such as relative humidity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
