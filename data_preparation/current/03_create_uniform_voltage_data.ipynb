{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----********************-----\n",
    "\n",
    "# Created Time: 2025/07/03\n",
    "\n",
    "# Author: Yiyi He, Tiger Peng\n",
    "\n",
    "### Use Case\n",
    "\n",
    "# This notebook create uniform voltage data using the scraped minute-wise voltage data\n",
    "# The two main data sources in this project are the current operating ESMI station data,\n",
    "# which is scraped from the Prayas ESMI website using the india_esmi_scraper.py scraper,\n",
    "# as well as the Harvard Dataverse Data\n",
    "\n",
    "\n",
    "# The Harvard Dataverse Data is formatted in a 60 x n grid, with each row being labelled with an hour in the day, from 0-23,\n",
    "# and each of the columns being a minute of the hour from 0-59.\n",
    "\n",
    "# The scraped data is formatted in two columns, with the time column being the datetime down to the minute,\n",
    "# and the voltage column being the voltage from 0-255.\n",
    "# -----********************-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESMI and Harvard Dataverse ID\n",
    "\n",
    "In addition to having different data formats, the two data sources also use different numbering systems.\n",
    "Harvard Dataverse uses the names of the 528 stations covered in the time period of the dataset, between 2014-2018.\n",
    "The ESMI scraper uses the ESMI ids assigned in the dropdown elements used by the web scraper to select stations on the Prayas website.\n",
    "\n",
    "The India station locations were all cross-referenced (still need to document this), with duplicates and stations outside of the ERA5-land dataset removed. Each of the newly-scraped ESMI stations was given an ID number proceeding 528 (the last number assigned to the Dataverse stations), with duplicates in the Dataverse data removed.\n",
    "\n",
    "After this process, we have ids from 1-572 (with some missing because duplicates were removed), and a uniform data format.\n",
    "\n",
    "These files will all be stored in the india_processing/india_uniform directory, and their IDs can be referenced in the file ESMI_India_538_locations.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Dataverse Voltage Data\n",
    "\n",
    "We first need to verify that all of the stations deemed to be in the ERA5 dataset's range is in either the Dataverse dataset or the ESMI dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_name(name, exceptions = None):\n",
    "    formatted = name.split('-')[0].split('[')[0].strip()\n",
    "    \n",
    "    if exceptions:\n",
    "        return exceptions(formatted) # run the name through the entered exceptions function, if provided\n",
    "    else:\n",
    "        return formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the location information table, which we can use to map between station_id, ESMI_ID, and station names.\n",
    "# Target filename: ESMI_India_538_locations.csv\n",
    "locations_path = \"/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/01_data/processed/ESMI_India_538_locations.csv\"\n",
    "locations = pd.read_csv(locations_path, dtype={\"ESMI_ID\" : str, \"station_id\" : int}, usecols=[\"station_id\", \"ESMI_ID\", \"Location name\", \"District\", \"State\", \"Lat\", \"Lon\"])\n",
    "locations.rename(columns={\"Location name\": \"station_name\", \"District\" : \"district\", \"State\": \"state\", \"Lat\": \"lat\", \"Lon\": \"lon\"}, inplace=True)\n",
    "locations[\"station_name\"] = locations[\"station_name\"].apply(strip_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>station_name</th>\n",
       "      <th>ESMI_ID</th>\n",
       "      <th>district</th>\n",
       "      <th>state</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5th phase JP Nagar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bengaluru Urban</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>12.901092</td>\n",
       "      <td>77.589150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>80 feet road</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dhule</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>20.895199</td>\n",
       "      <td>74.775982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Adarsh Nagar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Saharsa</td>\n",
       "      <td>Bihar</td>\n",
       "      <td>25.883507</td>\n",
       "      <td>86.614919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Adgaon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nashik</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>20.037184</td>\n",
       "      <td>73.850136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Agarchitti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Chamoli</td>\n",
       "      <td>Uttarakhand</td>\n",
       "      <td>30.016116</td>\n",
       "      <td>79.308735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   station_id        station_name ESMI_ID         district        state  \\\n",
       "0           1  5th phase JP Nagar     NaN  Bengaluru Urban    Karnataka   \n",
       "1           2        80 feet road     NaN            Dhule  Maharashtra   \n",
       "2           3        Adarsh Nagar     NaN          Saharsa        Bihar   \n",
       "3           4              Adgaon     NaN           Nashik  Maharashtra   \n",
       "4           5          Agarchitti     NaN          Chamoli  Uttarakhand   \n",
       "\n",
       "         lat        lon  \n",
       "0  12.901092  77.589150  \n",
       "1  20.895199  74.775982  \n",
       "2  25.883507  86.614919  \n",
       "3  20.037184  73.850136  \n",
       "4  30.016116  79.308735  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cy/f8598vm53cggn74m01d5s0rc0000gr/T/ipykernel_33155/869827649.py:6: DtypeWarning: Columns (3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(os.path.join(dataverse_dir, file))\n"
     ]
    }
   ],
   "source": [
    "# Get the unique names from all of the Harvard Dataverse spreadsheets\n",
    "dataverse_dir = \"/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/01_data/processed/india_dataverse\"\n",
    "stations_found = set()\n",
    "for file in os.listdir(dataverse_dir):\n",
    "    if file.endswith('.csv'):\n",
    "        data = pd.read_csv(os.path.join(dataverse_dir, file))\n",
    "        stations_found.update(data[\"Location name\"].unique())\n",
    "\n",
    "stations_found = set([strip_name(x) for x in stations_found])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Kothrud', 'Hedayetpur', 'Guwahati Club', 'Ameerpet', 'Dudhimati', 'Nagaon', 'Vidya Nagar', 'Khatorbari', 'Chanho', 'Juri Par Shanti Path', 'Capital Electrical Subdivision', 'Kapoorthla', 'Borbheta', 'Besant Nagar', 'Datalpara', 'Ulubari', 'Kardaitola', 'Mahuadanr', 'ASEB Campus', 'Gohaibari', 'Tezpur', 'Chouparan', 'Saheed Nagar', 'Deopur', 'Bamunimaidan', 'Mihijam', 'Netarhat', 'Kolebira', 'Kairo', 'Lalmatia', 'Perka', 'Satbarwa', 'Banjara Hills', 'Rajdhani Masjid', 'GNB road', 'Nichinta', 'Zalim Khurd', 'Amrit Nagar', 'Sahakar Nagar', 'Tarun Nagar', 'Alipur', 'Haider Nagar', 'Bhurkunda', 'Bilasipara', 'Domadih'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add all of the station ids to a set to be checked against the available names/ids in either dataset\n",
    "station_names = set(locations[\"station_name\"])\n",
    "\n",
    "# Update, checking off all the names found in the dataverse name field\n",
    "stations_remaining = station_names.difference(stations_found)\n",
    "print(stations_remaining)\n",
    "len(stations_remaining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the station_ids of the stations still unaccounted for after the dataverse data\n",
    "station_ids_remaining = set(locations[(locations['station_name'].isin(stations_remaining))]['station_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB road\n",
      "Gohaibari\n",
      "ASEB Campus\n",
      "Guwahati Club\n",
      "Hedayetpur\n",
      "Bamunimaidan\n",
      "Juri Par Shanti Path\n",
      "Tarun Nagar\n",
      "Capital Electrical Subdivision\n",
      "Ulubari\n",
      "Vidya Nagar\n",
      "Datalpara\n",
      "{'Nepal _6417', 'VidyaNagar', 'Santacruz', 'Yadthare', 'GNB road Guwahati', 'Bamunimaidan Guwahati', 'Nepal _4496', 'Sukhbaderi (S)', 'ASEB Campus Guwahati', 'Maruti Vethika Road', 'Ameerpeth', 'MG Road Panjim', 'Ulubari Guwahati', 'Sukhbaderi (N)', 'Miramar Panjim', 'Alwarpet', 'KT_4355', 'Subhash road Ratnagiri', 'Tilak Ali Ratnagiri', 'Palamu_2', 'Tarun Nagar Guwahati', 'Karnataka_7', 'Surakalpeth Cuddalore', 'Nepal _8193', 'Brahmavar', 'Benaulim', 'Uttardah (S)', 'Kapoorthala', 'Carambolim', 'Sancole', 'Capital Electrical Subdivision Guwahati', 'KT_8961', 'Dehradun_5', 'Colva', 'Guwahati Club Guwahati', 'Devpur', 'Saheed Nagar Bhubaneswar', 'Sayapettai Chinnasandhu', 'Nepal _5791', 'Sahakarnagar', 'Masipidi', 'Gimhavne', 'Juri Par Shanti Path Guwahati', 'Siddhapura', 'Nakre', 'Gohaibari Guwahati', 'Nerul', 'Bhubaneswar_5', 'Varanasi_7721', 'Banjara hills Hyderabad', 'Betalbatim', 'Besant Nagar Chennai', 'Manjakuppam Cuddalore', 'Radgaon MIDC Ratnagiri', 'Hedayetpur Guwahati', 'Datalpara Guwahati'}\n"
     ]
    }
   ],
   "source": [
    "# Print all of the station names that we haven't found but expected to, ids less than 529, because those we expect to find in \n",
    "for id in station_ids_remaining:\n",
    "    if int(id) < 529:\n",
    "        print(locations[locations['station_id'] == id]['station_name'].values[0])\n",
    "\n",
    "print(stations_found.difference(station_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to deal with exceptions in name formatting\n",
    "def name_exceptions(name):\n",
    "    if name == \"VidyaNagar\":\n",
    "        return \"Vidya Nagar\"\n",
    "    if \"Guwahati\" in name: # Remove Guwahati from the end\n",
    "        return \" \".join(name.split(\" \")[:-1])\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149    Guwahati Club\n",
      "Name: station_name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "stations_found = set([strip_name(x, name_exceptions) for x in stations_found])\n",
    "stations_remaining = station_names.difference(stations_found)\n",
    "\n",
    "station_ids_remaining = set(locations[(locations['station_name'].isin(stations_remaining))]['station_id'])\n",
    "\n",
    "for id in station_ids_remaining:\n",
    "    if int(id) < 529:\n",
    "        print(locations[locations['station_id'] == id]['station_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "esmi_dir = \"/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/01_data/processed/esmi_scraped/india_esmi\"\n",
    "\n",
    "esmi_found = set() # set for the esmi_ids found\n",
    "for file in os.listdir(esmi_dir):\n",
    "    if file.endswith('.csv'):\n",
    "        esmi_found.add(file.split('+')[0])\n",
    "\n",
    "ids_found = set(locations[locations['ESMI_ID'].isin(esmi_found)]['station_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{161}\n"
     ]
    }
   ],
   "source": [
    "final_remaining = station_ids_remaining.difference(ids_found)\n",
    "print(final_remaining)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to uniform format\n",
    "\n",
    "First, we can simply rename the ESMI stations according to their uniform station id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform_dir = \"/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/01_data/processed/india_uniform\"\n",
    "esmi_dir = \"/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/01_data/processed/esmi_scraped/india_esmi\"\n",
    "\n",
    "# Copy the scraped EMSI data files from \"india_esmi\" to \"india_uniform\" and rename the file to \"station_[uniform_id]\"\n",
    "for file in os.listdir(esmi_dir):\n",
    "    if file.endswith('.csv'):\n",
    "        esmi_id = file.split('+')[0]\n",
    "        uniform_id = locations[locations['ESMI_ID'] == esmi_id]['station_id'].values[0]\n",
    "\n",
    "        shutil.copy(os.path.join(esmi_dir, file), os.path.join(uniform_dir, f'station_{uniform_id}.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>station_name</th>\n",
       "      <th>ESMI_ID</th>\n",
       "      <th>district</th>\n",
       "      <th>state</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>533</td>\n",
       "      <td>Srirampura</td>\n",
       "      <td>212</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>13.068737</td>\n",
       "      <td>77.614819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     station_id station_name  ESMI_ID   district      state        lat  \\\n",
       "498         533   Srirampura      212  Bengaluru  Karnataka  13.068737   \n",
       "\n",
       "           lon  \n",
       "498  77.614819  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [Optional] The code below checks what the \"uniform_id\" is for a given \"ESMI ID\"\n",
    "locations[locations.ESMI_ID == 212]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we convert the Dataverse format into our uniform format.\n",
    "This involves taking each of the individual rows, taking the date and hour columns, along with the index of the minutes columns and transforming that into a vertical 2xn of datetime, voltage values, which can be concatenated together.\n",
    "\n",
    "Then we will use our existing name parsing to section each of the sheets by station.\n",
    "After this has been done for every station, we can save it to our uniform data location, taking care not to override any existing sheets, as the stations which are duplicate between ESMI and Dataverse, ESMI takes priority.\n",
    "\n",
    "Also, as we discovered later in the Cleanup section, the 2019Jan_Jun.csv has data quality problems which have to be address before we can incorporate it into our uniform dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataverse_dir = \"/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Energy_resilience/01_data/processed/india_dataverse\"\n",
    "\n",
    "# Filter out just the locations that aren't actively scraped ESMI locations\n",
    "# so we avoid mapping them to a location number when processing dataverse data.\n",
    "rem_locations = locations[pd.isna(locations['ESMI_ID'])]\n",
    "name_to_station_id = dict(zip(rem_locations['station_name'].to_list(), rem_locations['station_id'].to_list())) # Create dictionary to make id lookup more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(dataverse_dir):\n",
    "    if \"2019\" in file: # Skip 2019 for now\n",
    "        continue\n",
    "    print(file)\n",
    "    hd_df = pd.read_csv(os.path.join(dataverse_dir, file), header=0)\n",
    "\n",
    "    # Remove the weird excel export #VALUE! artifacts\n",
    "    hd_df.replace('#VALUE!', pd.NA, inplace=True)\n",
    "    hd_df.dropna(inplace=True)\n",
    "    hd_df.iloc[:, -60:] = hd_df.iloc[:, -60:].apply(pd.to_numeric)\n",
    "\n",
    "    # Merge the date and hour columns to create a pd datetime\n",
    "    if '/' in hd_df['Date'][0]:\n",
    "        hd_df.insert(loc=1, column='time', value=(pd.to_datetime(hd_df['Date'], format='%m/%d/%Y') + pd.to_timedelta(hd_df['Hour'], unit='h')))\n",
    "    elif '-' in hd_df['Date'][0]:\n",
    "        hd_df.insert(loc=1, column='time', value=(pd.to_datetime(hd_df['Date'], format='%d-%m-%Y') + pd.to_timedelta(hd_df['Hour'], unit='h')))\n",
    "    else:\n",
    "        print('Invalid date format')\n",
    "        break\n",
    "        \n",
    "    hd_df.drop(['Date', 'Hour'], axis=1, inplace=True)\n",
    "    \n",
    "    # Derive the uniform station_id from the station names\n",
    "    hd_df.rename(columns={'Location name': 'station_name'}, inplace=True)\n",
    "    \n",
    "    # Create a new column of uniform station_ids, assigning -1 if it is a station we have already discarded, so we can ignore it\n",
    "    hd_df.insert(loc=0, column='station_id', value=hd_df['station_name'].apply(lambda x : name_to_station_id.get(strip_name(x, exceptions=name_exceptions), -1)))\n",
    "    hd_df.drop('station_name', axis=1, inplace=True)\n",
    "    \n",
    "    unpivoted_rows = []\n",
    "    for index in tqdm(range(hd_df.shape[0])):\n",
    "        if hd_df.iloc[index, 0] == -1: # Skip irrelevant or already covered stations\n",
    "            continue\n",
    "            \n",
    "        unpivoted_row = hd_df.iloc[[index], :].melt(id_vars=['station_id', 'time'], ignore_index=True)\n",
    "        unpivoted_row['time'] = unpivoted_row['time'] + pd.to_timedelta(unpivoted_row['variable'].apply(lambda x : int(x.split(' ')[1])), unit='m')\n",
    "        \n",
    "        unpivoted_rows.append(unpivoted_row)\n",
    "    \n",
    "    unpivoted_df = pd.concat(unpivoted_rows)\n",
    "    unpivoted_df.drop('variable', axis=1, inplace=True)\n",
    "    unpivoted_df.rename(columns={'value': 'voltage'}, inplace=True)\n",
    "\n",
    "    # If the station_data already exists, append the new data on the end, otherwise, write directly to a new csv\n",
    "    for station_id in unpivoted_df['station_id'].unique():\n",
    "        output_path = os.path.join(uniform_dir, f'station_{station_id}.csv')\n",
    "        subset_df = unpivoted_df[unpivoted_df['station_id'] == station_id]\n",
    "        subset_df = subset_df.drop('station_id', axis=1)\n",
    "        \n",
    "        if os.path.exists(output_path):\n",
    "            station_df = pd.read_csv(output_path, header=0, parse_dates=['time'])\n",
    "            if station_df['time'].iloc[-1] < subset_df['time'].iloc[0]:\n",
    "                station_df = pd.concat((station_df, subset_df))\n",
    "                station_df.to_csv(output_path, index=False) # Saving to uniform dir\n",
    "        else:\n",
    "            subset_df.to_csv(output_path, index=False) # Saving to uniform dir\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with 2019Jan-Jun\n",
    "Because there are the presence of duplicate rows, both duplicate in the timestamp and station. I will just remove these duplicate rows.\n",
    "\n",
    "There is also the issue of erroneous voltage values, such as voltages of 535 or 999, which do not appear elsewhere in the dataverse dataset. I choose to simply ignore rows which contain values greater than 400 entirely, because in the way that we aggregate voltage data over hours or days, removal of a single datapoints is equivalent to removal of the entire hour anyways.\n",
    "\n",
    "We also take special care to simply remove the data of Bodireddypally-Prakasam and Kanheri Sarap, as the repeat rate is very high, and what can be salvaged may be questionable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(dataverse_dir):\n",
    "    if not \"2019\" in file: # Only operate on the 2019 dataset\n",
    "        continue\n",
    "    print(file)\n",
    "    hd_df = pd.read_csv(os.path.join(dataverse_dir, file), header=0)\n",
    "\n",
    "    # Remove the weird excel export #VALUE! artifacts\n",
    "    hd_df.replace('#VALUE!', pd.NA, inplace=True)\n",
    "    hd_df.dropna(inplace=True)\n",
    "    hd_df.iloc[:, -60:] = hd_df.iloc[:, -60:].apply(pd.to_numeric)\n",
    "    hd_df.iloc[:, -60:][hd_df.iloc[:, -60:] > 400] = pd.NA\n",
    "    hd_df.dropna(inplace=True) # Ignore rows with values greater than 400 \n",
    "\n",
    "    # Merge the date and hour columns to create a pd datetime\n",
    "    if '/' in hd_df['Date'][0]:\n",
    "        hd_df.insert(loc=1, column='time', value=(pd.to_datetime(hd_df['Date'], format='%m/%d/%Y') + pd.to_timedelta(hd_df['Hour'], unit='h')))\n",
    "    elif '-' in hd_df['Date'][0]:\n",
    "        hd_df.insert(loc=1, column='time', value=(pd.to_datetime(hd_df['Date'], format='%d-%m-%Y') + pd.to_timedelta(hd_df['Hour'], unit='h')))\n",
    "    else:\n",
    "        print('Invalid date format')\n",
    "        break\n",
    "        \n",
    "    hd_df.drop(['Date', 'Hour'], axis=1, inplace=True)\n",
    "    \n",
    "    # Derive the uniform station_id from the station names\n",
    "    hd_df.rename(columns={'Location name': 'station_name'}, inplace=True)\n",
    "    \n",
    "    # Create a new column of uniform station_ids, assigning -1 if it is a station we have already discarded, so we can ignore it\n",
    "    hd_df.insert(loc=0, column='station_id', value=hd_df['station_name'].apply(lambda x : name_to_station_id.get(strip_name(x, exceptions=name_exceptions), -1)))\n",
    "    hd_df.drop('station_name', axis=1, inplace=True)\n",
    "\n",
    "    hd_df = hd_df[hd_df['station_id'] != -1]\n",
    "    hd_df = hd_df[hd_df['station_id'] != 223]\n",
    "    hd_df = hd_df[hd_df['station_id'] != 86] # Remove all the high error rate and irrelevant stations\n",
    "\n",
    "    station_ids = hd_df['station_id'].unique()\n",
    "    \n",
    "    stations_2019 = {}\n",
    "    for station_id in station_ids:\n",
    "        print(station_id)\n",
    "        unpivoted_rows = []\n",
    "        station_df = hd_df[hd_df['station_id'] == station_id]\n",
    "\n",
    "        station_df = station_df[~station_df.duplicated(subset=['time'], keep=False)] # Remove duplicate times\n",
    "\n",
    "        for index in tqdm(range(station_df.shape[0])):\n",
    "            unpivoted_row = station_df.iloc[[index], :].melt(id_vars=['station_id', 'time'], ignore_index=True)\n",
    "            unpivoted_row['time'] = unpivoted_row['time'] + pd.to_timedelta(unpivoted_row['variable'].apply(lambda x : int(x.split(' ')[1])), unit='m')\n",
    "            unpivoted_rows.append(unpivoted_row)\n",
    "\n",
    "        unpivoted_df = pd.concat(unpivoted_rows)\n",
    "        unpivoted_df.drop('variable', axis=1, inplace=True)\n",
    "        unpivoted_df.rename(columns={'value': 'voltage'}, inplace=True)\n",
    "\n",
    "        stations_2019[station_id] = unpivoted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
