{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import urllib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import urllib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "def date2query(date):\n",
    "    return urllib.parse.quote(date.strftime('%d/%m/%Y'), safe='')\n",
    "\n",
    "\n",
    "# Robust request function that automatically retries\n",
    "def retry_request(url, session, total=4, status_forcelist=[429, 500, 502, 503, 504], **kwargs):\n",
    "    # Make number of requests required\n",
    "    for _ in range(total):\n",
    "        try:\n",
    "            response = session.get(url, **kwargs)\n",
    "            if response.status_code in status_forcelist:\n",
    "                # Retry request \n",
    "                continue\n",
    "\n",
    "            return response\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "\n",
    "session = requests.Session()\n",
    "LOGIN_URL = 'http://www.watchyourpower.org/admin/'\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Mobile Safari/537.36'\n",
    "}\n",
    "\n",
    "# Retrieve and display login captcha\n",
    "page_text = retry_request(url=LOGIN_URL, session=session, headers=HEADERS)\n",
    "img_url = 'http://www.watchyourpower.org/captcha.php'\n",
    "img_data = retry_request(url=img_url, session=session, headers=HEADERS).content\n",
    "\n",
    "with open('./captcha.jpg','wb') as fp:\n",
    "    fp.write(img_data)\n",
    "\n",
    "image = mpimg.imread(\"./captcha.jpg\")\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "img_code = input('Enter captchaï¼š')\n",
    "\n",
    "USERNAME ='junrentschler'\n",
    "PASSWORD ='jun@2018'\n",
    "data = {'username': USERNAME,\n",
    "        'password': PASSWORD,\n",
    "        'code': img_code,\n",
    "        'login': 'Login'}\n",
    "\n",
    "# Sign into watch your power session\n",
    "index = session.post(url=LOGIN_URL, headers=HEADERS, data=data)\n",
    "print(index)\n",
    "\n",
    "DETAIL_URL = 'http://www.watchyourpower.org/reports.php?category_id=3&location_id=119&from_date=13%2F01%2F2017&to_date=13%2F02%2F2017'\n",
    "detail_text = retry_request(url=DETAIL_URL, session=session, headers=HEADERS).text\n",
    "\n",
    "if detail_text.find('Welcome') == -1:\n",
    "    exit()\n",
    "\n",
    "# Output directory, india_esmi by default, change as needed\n",
    "INDIA_SCRAPE_DIR = f'india_esmi'\n",
    "\n",
    "# Pull up the dashboard page and retrieve all available monitoring station ids\n",
    "# Because nothing is specified, the rightmost dropdown \"Location\" will have all the monitoring stations available\n",
    "API_URL = f'http://www.watchyourpower.org/reports.php?'\n",
    "print(API_URL)\n",
    "\n",
    "res = retry_request(url=API_URL, session=session, headers=HEADERS)\n",
    "if res.ok:\n",
    "    text = res.text\n",
    "\n",
    "    # Isolate the section of the html that contains the monitoring station dropdown\n",
    "    text_list = text.split('<option value=\"\">-- Select  --</option>')[1].split('</select>')[0]\n",
    "    text_list = re.split('<option class=\"\" | >', text_list)\n",
    "    stationIDs = [elem[6:].replace('\\\"', '').replace(' ', '+') for elem in text_list if elem.startswith(\"value=\")]\n",
    "\n",
    "    # If the output directory already exists, check the log to see the ids and last scraped dates for all of the stations\n",
    "    last_scraped = {}\n",
    "    today_string = datetime.now().strftime('%m/%d/%Y')\n",
    "\n",
    "    # Ensure the output directory exists for the latest run of the scraper and retrieve last scraped dates\n",
    "    # to avoid repeat work\n",
    "    if os.path.exists(INDIA_SCRAPE_DIR):\n",
    "        if os.path.exists(f'{INDIA_SCRAPE_DIR}/log.txt'):\n",
    "            with open(f'{INDIA_SCRAPE_DIR}/log.txt', 'r') as f:\n",
    "                print('Previous scraping log found:')\n",
    "                print(f'Last read: {f.readline().split(\":\")[1]}')\n",
    "\n",
    "                for line in f:\n",
    "                    title, id, date = line.strip().split(',')\n",
    "                    last_scraped[id] = (datetime.strptime(date, '%m/%d/%Y'), title)\n",
    "    else:\n",
    "        os.makedirs(INDIA_SCRAPE_DIR)\n",
    "\n",
    "    try:\n",
    "        for id in stationIDs:\n",
    "            # Query just the location with no dates to pull up the error text which informs us of the available data dates\n",
    "            url = f'http://www.watchyourpower.org/reports.php?location_id={id}'\n",
    "            res = retry_request(url=url, session=session, headers=HEADERS)\n",
    "\n",
    "            range_start = None\n",
    "            range_end = None\n",
    "            title = \"\"\n",
    "\n",
    "            if res:\n",
    "                empty_chart_match = re.search(r'<span class=\"empty_chart\">([\\S\\s]*?)<\\/span>', res.text)\n",
    "                if empty_chart_match:\n",
    "                    empty_chart_content = empty_chart_match.group(1)\n",
    "                    range_start = datetime.strptime(empty_chart_content.split('From ')[1].split(' To ')[0].strip(), '%d/%m/%Y')\n",
    "                    range_end = datetime.strptime(empty_chart_content.split(' To ')[1].strip(), '%d/%m/%Y')\n",
    "                    \n",
    "                    # Ignore data before 2015 due to poor data quality before official program launch\n",
    "                    if range_start < datetime(year=2015, month=1, day=1):\n",
    "                        range_start = datetime(year=2015, month=1, day=1)\n",
    "\n",
    "                title_match = re.search(r'<h4>[\\s]*?Location - (.*?), District - (.*?), State - (.*?) \\..*?<\\/h4>', res.text)\n",
    "                location = title_match.group(1).strip()\n",
    "                district = title_match.group(2).strip()\n",
    "                state = title_match.group(3).strip()\n",
    "                title = re.sub(r'[\\s]*-[\\s]*', '-', f'{location}+{district}+{state}'.replace('/', '_')).replace(' ', '_')\n",
    "                print(title)\n",
    "            else:\n",
    "                print(f'Failed to retrieve data for station {id}')\n",
    "                continue\n",
    "\n",
    "            # If the station has already been scraped, check the last scraped date and start from there\n",
    "            if id in last_scraped:\n",
    "                print('station found')\n",
    "                last_date = last_scraped[id][0]\n",
    "                if last_date >= range_start:\n",
    "                    range_start = last_date + timedelta(days=1)\n",
    "                \n",
    "                if range_end < range_start: # If the last scraped date is after the last available date, skip this station\n",
    "                    print(f'Station {id} already up to date')\n",
    "                    continue\n",
    "\n",
    "            query_start = range_start\n",
    "            query_end = query_start + timedelta(days=31)\n",
    "\n",
    "            # Loop through the available data period in 31 day intervals\n",
    "            dates = []\n",
    "            voltages = []\n",
    "            with tqdm(total=(range_end - range_start).days) as pbar:\n",
    "                while query_end <= range_end and query_start < query_end:\n",
    "                    url = f'http://www.watchyourpower.org/reports.php?location_id={id}&from_date={date2query(query_start)}&to_date={date2query(query_end)}'\n",
    "                    res = retry_request(url=url, session=session, headers=HEADERS)\n",
    "\n",
    "                    if res:\n",
    "                        text = res.text\n",
    "                        pbar.set_description(f'Scraping {query_start.strftime(\"%m/%d/%Y\")} to {query_end.strftime(\"%m/%d/%Y\")} for station {id}')\n",
    "\n",
    "                        # Separate out all of the line chart data points into their \"date\" : date, \"voltage\" : voltage pairs, with the brackets removed\n",
    "                        test_split = text.split('linechartData = [{')\n",
    "                        if len(test_split) < 2:\n",
    "                            pbar.set_description(f'No data found for station {id} between {query_start.strftime(\"%m/%d/%Y\")} and {query_end.strftime(\"%m/%d/%Y\")}')\n",
    "                            # If the data is not empty, which would present the empty_chart span, save the html for manual inspection\n",
    "                            if not re.search(r'<span class=\"empty_chart\">([\\S\\s]*?)<\\/span>', res.text):\n",
    "                                with open(f'{INDIA_SCRAPE_DIR}/error_{id}_{query_start.strftime(\"%m-%d-%Y\")}.html', 'w') as f:\n",
    "                                    f.write(text)\n",
    "                        else:\n",
    "                            datapoints = test_split[1].split('];')[0].split('},{')\n",
    "                            for point in datapoints:\n",
    "                                date = datetime.strptime(point.split('\"date\":\"')[1].split('\",\"')[0], '%a %b %d %Y %H:%M:%S')\n",
    "                                voltage = int(point.split('\"voltage\":\"')[1].split('\"')[0])\n",
    "\n",
    "                                dates.append(date)\n",
    "                                voltages.append(voltage)\n",
    "\n",
    "                    pbar.update((query_end - query_start).days + 1)\n",
    "                    \n",
    "                    query_start = query_end + timedelta(days=1)\n",
    "                    query_end = query_start + timedelta(days=31)\n",
    "\n",
    "                    if query_end > range_end:\n",
    "                        query_end = range_end\n",
    "\n",
    "            # Update last scraped\n",
    "            last_scraped[id] = (range_end, title)\n",
    "\n",
    "            print(f'Total entries: {len(dates)}')\n",
    "            df = pd.DataFrame({'time': dates, 'voltage': voltages})\n",
    "            if id in last_scraped: # Update old file if it exists\n",
    "                df_old = pd.read_csv(f'{INDIA_SCRAPE_DIR}/{id}+{title}.csv')\n",
    "                df = pd.concat([df_old, df], ignore_index=True)\n",
    "            \n",
    "            df.to_csv(f'{INDIA_SCRAPE_DIR}/{id}+{title}.csv', index=False)\n",
    "    \n",
    "    # Update the log file\n",
    "    finally:\n",
    "        with open(f'{INDIA_SCRAPE_DIR}/log.txt', 'w') as f:\n",
    "            f.write(f'Last Scraped:{today_string}\\n')\n",
    "            for id, info in last_scraped.items():\n",
    "                date = info[0]\n",
    "                title = info[1]\n",
    "                f.write(f'{title},{id},{date.strftime(\"%m/%d/%Y\")}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
